{
  "doi": "10.1145/1240624.1240778",
  "title": "Multimodal redundancy across handwriting and speech during computer mediated human-human interactions",
  "published": "2007-04-29",
  "proctitle": "CHI '07: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems",
  "pages": "1009-1018",
  "year": 2007,
  "badges": [],
  "abstract": "Lecturers, presenters and meeting participants often say what they publicly handwrite. In this paper, we report on three empirical explorations of such multimodal redundancy -- during whiteboard presentations, during a spontaneous brainstorming meeting, and during the informal annotation and discussion of photographs. We show that redundantly presented words, compared to other words used during a presentation or meeting, tend to be topic specific and thus are likely to be out-of-vocabulary. We also show that they have significantly higher tf-idf (term frequency-inverse document frequency) weights than other words, which we argue supports the hypothesis that they are dialogue-critical words. We frame the import of these empirical findings by describing SHACER, our recently introduced Speech and HAndwriting reCognizER, which can combine information from instances of redundant handwriting and speech to dynamically learn new vocabulary.",
  "authors": [
    {
      "name": "Edward C. Kaiser",
      "institution": "Adapx, Seattle, WA",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81100460794",
      "orcid": "missing"
    },
    {
      "name": "Paulo Barthelmess",
      "institution": "Adapx, Seattle, WA",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81100242831",
      "orcid": "missing"
    },
    {
      "name": "Candice Erdmann",
      "institution": "Adapx, Seattle, WA",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81328488291",
      "orcid": "missing"
    },
    {
      "name": "Phil Cohen",
      "institution": "Adapx, Seattle, WA",
      "img": "/do/10.1145/contrib-81100149852/rel-imgonly/81100149852.jpg",
      "acmid": "81100149852",
      "orcid": "missing"
    }
  ],
  "references": [
    {
      "text": "A llauzen, A. and J.-L. Gauvain. Open Vocabulary ASR for Audiovisual Document Indexation. ICASSP '05, (2005).",
      "doi": ""
    },
    {
      "text": "Anderson, R., C. Hoyer, C. Prince, J. Su, F. Videon, and S. Wolfman. Speech, Ink and Slides: The Interaction of Content Channels. ACM Multimedia, (2004).  ",
      "doi": "10.1145/1027527.1027713"
    },
    {
      "text": "Anderson, R.J., R. Anderson, C. Hoyer, and S.A. Wolfman. A Study of Digital Ink in Lecture Presentation. CHI '04, (2004).  ",
      "doi": "10.1145/985692.985764"
    },
    {
      "text": "Baeza-Yates, R. and B. Ribeiro-Neto, Modern Information Retrieval: Addison-Wesley, 1999. ",
      "doi": "10.5555/553876"
    },
    {
      "text": "Barthelmess, P., E.C. Kaiser, X. Huang, and D. Demirdjian. Distributed Pointing for Multimodal Collaboration over Sketched Diagrams. ICMI 2005.  ",
      "doi": "10.1145/1088463.1088469"
    },
    {
      "text": "Barthelmess, P., E.C. Kaiser, X. Huang, D. McGee, and P. Cohen. Collaborative Multimodal Photo Annotation over Digital Paper. ICMI'06, (2006).  ",
      "doi": "10.1145/1180995.1181000"
    },
    {
      "text": "Black, A., P. Taylor, and R. Caley, The Festival Speech Synthesis System: System Documentation, in Technical Report HCRC/TR--83. 1998, Human Communication Research Centre.",
      "doi": ""
    },
    {
      "text": "Brennan, S. Lexical Entrainment in Spontaneous Dialogue. International Symposium on Spoken Dialogue, (1996), 41--44.",
      "doi": ""
    },
    {
      "text": "Chai, J.Y., Z. Prasov, J. Blaim, and R. Jin. Linguistic Theories in Efficient Multimodal Reference Resolution: An Empirical Investigation. IUI '05, (2005), 43--50.  ",
      "doi": "10.1145/1040830.1040850"
    },
    {
      "text": "Clark, H.H., Using Language: Cambridge University Press, 1996.",
      "doi": ""
    },
    {
      "text": "Garofolo, J., G. Auzanne, and E. Voorhees. The Trec Spoken Document Retrieval Track: A Success Story. RAIO-2000: Content-Based Multimedia Information Access Conference, (2000), 1--20.",
      "doi": ""
    },
    {
      "text": "Glass, J., T.J. Hazen, L. Hetherington, and C. Wang. Analysis and Processing of Lecture Audio Data: Preliminary Investigations. HLT-NAACL, Workshop on Interdisciplinary Approaches to Speech Indexing and Retrieval, (2004). ",
      "doi": "10.5555/1626307.1626309"
    },
    {
      "text": "Grice, H.P., Logic and Conversation, in Speech Acts, P. Cole and J. Morgan, Eds, Acad. Press: 1975, NY. 41--58.",
      "doi": ""
    },
    {
      "text": "Gupta, A.K. and T. Anastasakos. Dynamic Time Windows for Multimodal Input Fusion. INTERSPEECH-'04, (2004), 1009--1012.",
      "doi": ""
    },
    {
      "text": "Harnad, S., The Symbol Grounding Problem. Physica D 42, (1990), 335--346.  ",
      "doi": "10.1016/0167-2789%2890%2990087-6"
    },
    {
      "text": "Kaiser, E., D. Demirdjian, A. Gruenstein, X. Li, J. Niekrasz, M. Wesson, and S. Kumar. Demo: A Multimodal Learning Interface for Sketch, Speak and Point Creation of a Schedule Chart. ICMI '04, (2004).  ",
      "doi": "10.1145/1027933.1027992"
    },
    {
      "text": "Kaiser, E.C. Multimodal New Vocabulary Recognition through Speech and Handwriting in a Whiteboard Scheduling Application. IUI '05, (2005), 51--58.  ",
      "doi": "10.1145/1040830.1040851"
    },
    {
      "text": "Kaiser, E.C. Shacer: A Speech and Handwriting Recognizer. ICMI '05, Workshop on Multimodal, Multiparty Meeting Processing, (2005).",
      "doi": ""
    },
    {
      "text": "Kaiser, E.C. Using Redundant Speech and Handwriting for Learning New Vocabulary and Understanding Abbreviations. ICMI '06, (2006), 347--356.  ",
      "doi": "10.1145/1180995.1181060"
    },
    {
      "text": "Kaiser, E.C. and P. Barthelmess. Edge-Splitting in a Cumulative Multimodal System, for a No-Wait Temporal Threshold on Information Fusion, Combined with an under-Specified Display. INTERSPEECH 2006.",
      "doi": ""
    },
    {
      "text": "Kaiser, E.C., P. Barthelmess, and A. Arthur. Multimodal Play Back of Collaborative Multiparty Corpora. ICMI '05, Workshop on Multimodal, Multiparty Meeting Processing, (2005).",
      "doi": ""
    },
    {
      "text": "Kurihara, K., M. Goto, J. Ogata, and T. Igarashi. Speech Pen: Predictive Handwriting Based on Ambient Multimodal Recognition. CHI '06, (2006), 851--860.  ",
      "doi": "10.1145/1124772.1124897"
    },
    {
      "text": "Logan, B., P. Moreno, J.-M.V. Thong, and E. Whittaker. An Experimental Study of an Audio Indexing System for the Web. ICSLP, (2000).",
      "doi": ""
    },
    {
      "text": "Mayer, R.E. and R. Moreno, Nine Ways to Reduce Cognitive Load in Multimedia Learning. Educational Psychologist 38, 1, (2003), 43--52.",
      "doi": ""
    },
    {
      "text": "Moreno, R. and R.E. Mayer, Verbal Redundancy in Multimedia Learning: When Reading Helps Listening. Jour. of Educational Psychology 94, 1, (2002), 156--163.",
      "doi": ""
    },
    {
      "text": "Ng, K. and V. Zue, Subword-Based Approaches for Spoken Document Retrieval. Speech Communication 32, 3, (2000), 157--186.  ",
      "doi": "10.1016/S0167-6393%2800%2900008-X"
    },
    {
      "text": "Ohtsuki, K., N. Hiroshima, M. Oku, and A. Imamura. Unsupervised Vocabulary Expansion for Automatic Transcription of Broadcast News. ICASSP '05, (2005).",
      "doi": ""
    },
    {
      "text": "Oviatt, S., Ten Myths of Multimodal Interaction. Communications of the ACM 42, 11, (1999), 74--81.  ",
      "doi": "10.1145/319382.319398"
    },
    {
      "text": "Oviatt, S.L., A. DeAngeli, and K. Kuhn. Integration and Synchronization of Input Modes During Multimodal Human-Computer Interaction. CHI '97, (1997).  ",
      "doi": "10.1145/258549.258821"
    },
    {
      "text": "Salton, G. and C. Buckley, Term-Weighting Approaches in Automatic Text Retrieval. Information Processing & Management 24, 5, (1988), 513--523.  ",
      "doi": "10.1016/0306-4573%2888%2990021-0"
    },
    {
      "text": "Saraclar, M. and R. Sproat. Lattice-Based Search for Spoken Utterance Retrieval. Proc. HLT/NAACL, (2004), 129--136.",
      "doi": ""
    },
    {
      "text": "Seekafile, Http://www.Seekafile.Org/",
      "doi": ""
    },
    {
      "text": "Sethy, A., S. Narayanan, and S. Parthasarthy. A Syllable Based Approach for Improved Recognition of Spoken Names. ISCA Pronunciation Modeling Workshop, (2002).",
      "doi": ""
    },
    {
      "text": "WaveSurfer, Http://www.Speech.Kth.Se/Wavesurfer/, Dep. of Speech, Music and Hearing, KTH.",
      "doi": ""
    },
    {
      "text": "Wickens, C.C., Multiple Resources and Performance Prediction. Theoretical Issues in Ergonomics Science 3, 2, (2002), 159--177.",
      "doi": ""
    },
    {
      "text": "Woodland, P.C., S.E. Johnson, P. Jourlin, and K.S. Jones. Effects of out of Vocabulary Words in Spoken Document Retrieval. Research and Development in Information Retrieval, (2000), 372--374.  ",
      "doi": "10.1145/345508.345661"
    },
    {
      "text": "Yu, H., T. Tomokiyo, Z. Wang, and A. Waibel. New Developments in Automatic Meeting Transcription. ICSLP, (2000).",
      "doi": ""
    },
    {
      "text": "Yu, P., K. Chen, C. Ma, and F. Seide, Vocabulary-Independent Indexing of Spontaneous Speech. IEEE Transactions on Speech and Audio Processing 13, 5, (2005), 635--643.",
      "doi": ""
    },
    {
      "text": "ZDNet, At The Whiteboard, http://news.zdnet.com/2036-2_22-6035716.html",
      "doi": ""
    }
  ]
}