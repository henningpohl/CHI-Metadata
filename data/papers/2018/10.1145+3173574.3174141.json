{
  "doi": "10.1145/3173574.3174141",
  "title": "Identifying Speech Input Errors Through Audio-Only Interaction",
  "published": "2018-04-21",
  "proctitle": "CHI '18: Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems",
  "pages": "1-12",
  "year": 2018,
  "badges": [],
  "abstract": "Speech has become an increasingly common means of text input, from smartphones and smartwatches to voice-based intelligent personal assistants. However, reviewing the recognized text to identify and correct errors is a challenge when no visual feedback is available. In this paper, we first quantify and describe the speech recognition errors that users are prone to miss, and investigate how to better support this error identification task by manipulating pauses between words, speech rate, and speech repetition. To achieve these goals, we conducted a series of four studies. Study 1, an in-lab study, showed that participants missed identifying over 50% of speech recognition errors when listening to audio output of the recognized text. Building on this result, Studies 2 to 4 were conducted using an online crowdsourcing platform and showed that adding a pause between words improves error identification compared to no pause, the ability to identify errors degrades with higher speech rates (300 WPM), and repeating the speech output does not improve error identification. We derive implications for the design of audio-only speech dictation.",
  "tags": [
    "text entry",
    "audio-only interaction",
    "error correction",
    "eyes-free use",
    "speech dictation",
    "synthesized speech"
  ],
  "authors": [
    {
      "name": "Jonggi Hong",
      "institution": "University of Maryland, College Park, MD, USA",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659081211",
      "orcid": "missing"
    },
    {
      "name": "Leah Findlater",
      "institution": "University of Washington, Seattle, WA, USA",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81100170743",
      "orcid": "0000-0002-5619-4452"
    }
  ],
  "references": [
    {
      "text": "Shiri Azenkot and Nicole B Lee. 2013. Exploring the use of speech input by blind people on mobile devices. Proceedings of the ACM SIGACCESS Conference on Computers and Accessibility, ACM, Article No. 11.  ",
      "doi": "10.1145/2513383.2513440"
    },
    {
      "text": "Ann R Bradlow and Jennifer A Alexander. 2007. Semantic and phonetic enhancements for speech-innoise recognition by native and non-native listeners. The Journal of the Acoustical Society of America 121, 4: 2339--2349.",
      "doi": ""
    },
    {
      "text": "Junhwi Choi, Kyungduk Kim, Sungjin Lee, et al. 2012. Seamless error correction interface for voice word processor. Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on, 4973--4976.",
      "doi": ""
    },
    {
      "text": "W Feng. 1994. Using handwriting and gesture recognition to correct speech recognition errors. Urbana 51: 61801.",
      "doi": ""
    },
    {
      "text": "Arnout R H Fischer, Kathleen J Price, and Andrew Sears. 2005. Speech-based text entry for mobile handheld devices: an analysis of efficacy and error correction techniques for server-based solutions. International Journal of Human-Computer Interaction 19, 3: 279--304.",
      "doi": ""
    },
    {
      "text": "Kazuki Fujiwara. 2016. Error Correction of Speech Recognition by Custom Phonetic Alphabet Input for Ultra-Small Devices. Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems, 104--109.  ",
      "doi": "10.1145/2851581.2890380"
    },
    {
      "text": "Beth G Greene. 1986. Perception of synthetic speech by nonnative speakers of English. Proceedings of the Human Factors Society Annual Meeting, 1340--1343.",
      "doi": ""
    },
    {
      "text": "David Huggins-Daines and Alexander I Rudnicky. 2008. Interactive asr error correction for touchscreen devices. Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Demo Session, 17--19. ",
      "doi": "10.5555/1564144.1564149"
    },
    {
      "text": "Esther Janse. 2004. Word perception in fast speech: artificially time-compressed vs. naturally produced fast speech. Speech Communication 42, 2: 155--173.",
      "doi": ""
    },
    {
      "text": "Hui Jiang. 2005. Confidence measures for speech recognition: A survey. Speech communication 45, 4: 455--470.",
      "doi": ""
    },
    {
      "text": "Caroline Jones, Lynn Berry, and Catherine Stevens. 2007. Synthesized speech intelligibility and persuasion: Speech rate and non-native listeners. Computer Speech&Language 21, 4: 641--651.  ",
      "doi": "10.1016/j.csl.2007.03.001"
    },
    {
      "text": "Clare-Marie Karat, Christine Halverson, Daniel Horn, and John Karat. 1999. Patterns of entry and correction in large vocabulary continuous speech recognition systems. Proceedings of the SIGCHI conference on Human Factors in Computing Systems, 568--575.  ",
      "doi": "10.1145/302979.303160"
    },
    {
      "text": "Mary LaLomia. 1994. User Acceptance of Handwritten Recognition Accuracy. Conference Companion on Human Factors in Computing Systems, ACM, 107--108.  ",
      "doi": "10.1145/259963.260086"
    },
    {
      "text": "Yuan Liang, Koji Iwano, and Koichi Shinoda. 2014. Simple gesture-based error correction interface for smartphone speech recognition. INTERSPEECH, 1194--1198.",
      "doi": ""
    },
    {
      "text": "Iain A McCowan, Darren Moore, John Dines, et al. 2004. On the use of information retrieval measures for speech recognition evaluation.",
      "doi": ""
    },
    {
      "text": "Anja Moos and J\u00fcrgen Trouvain. 2007. Comprehension of ultra-fast speech--blind vs.\"normally hearing\" persons. Proceedings of the 16th International Congress of Phonetic Sciences, 677--680.",
      "doi": ""
    },
    {
      "text": "Jun Ogata and Masataka Goto. 2005. Speech repair: quick error correction just by using selection operation for speech input interfaces. INTERSPEECH, 133--136.",
      "doi": ""
    },
    {
      "text": "Antti Oulasvirta, Sakari Tamminen, Virpi Roto, and Jaana Kuorelahti. 2005. Interaction in 4-second bursts: the fragmented nature of attentional resources in mobile HCI. Proceedings of the SIGCHI conference on Human factors in computing systems, 919--928.  ",
      "doi": "10.1145/1054972.1055101"
    },
    {
      "text": "Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015. Librispeech: an ASR corpus based on public domain audio books. Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on, 5206--5210.",
      "doi": ""
    },
    {
      "text": "Konstantinos Papadopoulos and Eleni Koustriava. 2015. Comprehension of Synthetic and Natural Speech: Differences among Sighted and Visually Impaired Young Adults. Enabling Access for Persons with Visual Impairment: 147.",
      "doi": ""
    },
    {
      "text": "Sherry Ruan, Jacob O Wobbrock, Kenny Liou, Andrew Ng, and James Landay. 2016. Speech Is 3x Faster than Typing for English and Mandarin Text Entry on Mobile Devices. arXiv preprint arXiv:1608.07323.",
      "doi": ""
    },
    {
      "text": "Amanda Stent, Ann Syrdal, and Taniya Mishra. 2011. On the intelligibility of fast synthesized speech for individuals with early-onset blindness. The proceedings of the 13th international ACM SIGACCESS conference on Computers and accessibility, 211--218.  ",
      "doi": "10.1145/2049536.2049574"
    },
    {
      "text": "Bernhard Suhm, Brad Myers, and Alex Waibel. 2001. Multimodal error correction for speech user interfaces. ACM transactions on computer-human interaction (TOCHI) 8, 1: 60--98.  ",
      "doi": "10.1145/371127.371166"
    },
    {
      "text": "Brenda Sutton, Julia King, Karen Hux, and David Beukelman. 1995. Younger and older adults' rate performance when listening to synthetic speech. Augmentative and Alternative Communication 11, 3: 147--153.",
      "doi": ""
    },
    {
      "text": "TheMSsoundeffects. City sound effect 1 downtown. Retrieved from https://youtu.be/LZbEIxhiJRM.",
      "doi": ""
    },
    {
      "text": "Simon Tucker and Steve Whittaker. 2005. Novel techniques for time-compressing speech: an exploratory study. Acoustics, Speech, and Signal Processing, 2005. Proceedings.(ICASSP'05). IEEE International Conference on, I--477.",
      "doi": ""
    },
    {
      "text": "Lijuan Wang, Tao Hu, Peng Liu, and Frank K Soong. 2008. Efficient handwriting correction of speech recognition errors with template constrained posterior (TCP). INTERSPEECH, 2659--2662.",
      "doi": ""
    },
    {
      "text": "Zhirong Wang, Tanja Schultz, and Alex Waibel. 2003. Comparison of acoustic model adaptation techniques on non-native speech. Acoustics, Speech, and Signal Processing, 2003. Proceedings.(ICASSP'03). 2003 IEEE International Conference on, I--I.",
      "doi": ""
    },
    {
      "text": "Stephen J Winters and David B Pisoni. 2004. Perception and comprehension of synthetic speech. Progress Report Research on Spoken Language Processing 26: 1--44.",
      "doi": ""
    },
    {
      "text": "Jacob O Wobbrock, Leah Findlater, Darren Gergle, and James J Higgins. 2011. The Aligned Rank Transform for Nonparametric Factorial Analyses Using Only Anova Procedures. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, ACM, 143--146.  ",
      "doi": "10.1145/1978942.1978963"
    },
    {
      "text": "Wayne Xiong, Jasha Droppo, Xuedong Huang, et al. 2017. The Microsoft 2016 conversational speech recognition system. Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE International Conference on, 5255--5259.",
      "doi": ""
    },
    {
      "text": "Hanlu Ye, Meethu Malu, Uran Oh, and Leah Findlater. 2014. Current and future mobile and wearable device use by people with visual impairments. Proceedings of the 32nd annual ACM conference on Human factors in computing systems - CHI '14: 3123--3132.  ",
      "doi": "10.1145/2556288.2557085"
    },
    {
      "text": "Geoffrey Zweig, Chengzhu Yu, Jasha Droppo, and Andreas Stolcke. 2017. Advances in all-neural speech recognition. Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE International Conference on, 4805--4809.",
      "doi": ""
    }
  ]
}