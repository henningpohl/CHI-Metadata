{
  "doi": "10.1145/3173574.3173977",
  "title": "EDITalk: Towards Designing Eyes-free Interactions for Mobile Word Processing",
  "published": "2018-04-21",
  "proctitle": "CHI '18: Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems",
  "pages": "1-10",
  "year": 2018,
  "badges": [],
  "abstract": "We present EDITalk, a novel voice-based, eyes-free word processing interface. We used a Wizard-of-Oz elicitation study to investigate the viability of eyes-free word processing in the mobile context and to elicit user requirements for such scenarios. Results showed that meta-level operations like highlight and comment, and core operations like insert, delete and replace are desired by users. However, users were challenged by the lack of visual feedback and the cognitive load of remembering text while editing it. We then studied a commercial-grade dictation application and discovered serious limitations that preclude comfortable speak-to-edit interactions. We address these limitations through EDITalk's closed-loop interaction design, enabling eyes-free operation of both meta-level and core word processing operations in the mobile context. Finally, we discuss implications for the design of future mobile, voice-based, eyes-free word processing interface.",
  "authors": [
    {
      "name": "Debjyoti Ghosh",
      "institution": "National University of Singapore, Singapore, Singapore",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659260512",
      "orcid": "missing"
    },
    {
      "name": "Pin Sym Foong",
      "institution": "National University of Singapore, Singapore, Singapore",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81414606008",
      "orcid": "missing"
    },
    {
      "name": "Shengdong Zhao",
      "institution": "National University of Singapore, Singapore, Singapore",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81416592901",
      "orcid": "0000-0001-7971-3107"
    },
    {
      "name": "Di Chen",
      "institution": "National Univeristy of Singapore, Singapore, Singapore",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659260979",
      "orcid": "missing"
    },
    {
      "name": "Morten Fjeld",
      "institution": "Chalmers University of Technology, Gothenburg, Sweden",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81100571519",
      "orcid": "missing"
    }
  ],
  "references": [
    {
      "text": "Shiri Azenkot and Nicole B. Lee. 2013. Exploring the use of speech input by blind people on mobile devices. In Proceedings of the 15th International ACM SIGACCESS Conference on Computers and Accessibility - ASSETS '13, 1--8.  ",
      "doi": "10.1145/2513383.2513440"
    },
    {
      "text": "Keith Brown, J. Lai, and N. Yankelovich. 2006. Speech Interface Design. In Encyclopedia of Language & Linguistics. 764--770.",
      "doi": ""
    },
    {
      "text": "N. Dahlb\u00e4ck, A. J\u00f6nsson, and L. Ahrenberg. 1993. Wizard of Oz studies - why and how. KnowledgeBased Systems 6, 4: 258--266.",
      "doi": "10.1016/0950-7051%2893%2990017-N"
    },
    {
      "text": "CA Halverson, DB Horn, CM Karat, and J Karat. 1999. The Beauty of Errors: Patterns of Error Correction in Desktop Speech Systems. INTERACT. Retrieved September 9, 2017 from https://books.google.com.sg/books?hl=en&lr=&id= yXehjiOd_kkC&oi=fnd&pg=PA133&dq=The+bea uty+of+errors:+patterns+of+error+correction+in+d esktop+speech+system&ots=NWfXoV0yCp&sig=k YQMxijl92ja_Pzg-igBmRz6tVs",
      "doi": ""
    },
    {
      "text": "Sk Kane, Jo Wobbrock, and Re Ladner. 2011. Usable gestures for blind people: understanding preference and performance. Proceedings of the 2011 annual conference on Human factors in computing systems - CHI '11: 413--422.  ",
      "doi": "10.1145/1978942.1979001"
    },
    {
      "text": "TB Martin. 1980. Practical speech recognizers and some performance effectiveness parameters. Trends in speech recognition. Retrieved September 9, 2017 from http://ci.nii.ac.jp/naid/10020895638/",
      "doi": ""
    },
    {
      "text": "Donald A Norman. 2004. Emotional Design: Why We Love (or Hate) Everyday Things.",
      "doi": "10.1145/985600.966013"
    },
    {
      "text": "Sharon Oviatt. 2000. Taming recognition Errors with a Multimodal Interface. COMMUNICATIONS OF THE ACM 43, 9: 45--51.  ",
      "doi": "10.1145/348941.348979"
    },
    {
      "text": "Ian J. Pitt and Alistair D N Edwards. 1996. Improving the usability of speech-based interfaces for blind users. Annual ACM Conference on Assistive Technologies, Proceedings: 124--130.  ",
      "doi": "10.1145/228347.228367"
    },
    {
      "text": "TL Roberts and TP Moran. 1983. The evaluation of text editors: methodology and empirical results. Communications of the ACM 26, April: 265--283.  ",
      "doi": "10.1145/2163.2164"
    },
    {
      "text": "J Schalkwyk, D Beeferman, F Beaufays, and B Byrne. 2010. \"Your word is my command\": Google search by voice: a case study. Advances in Speech. Retrieved September 1, 2017 from http://link.springer.com/chapter/10.1007/978--14419--5951--5_4",
      "doi": ""
    },
    {
      "text": "A Sears, CM Karat, K Oseitutu, and A Karimullah. 2001. Productivity, satisfaction, and interaction strategies of individuals with spinal cord injuries and traditional users interacting with speech recognition software. Universal Access in the. Retrieved September 9, 2017 from http://www.springerlink.com/index/XT2JV1975PQ A7D2H.pdf",
      "doi": ""
    },
    {
      "text": "A Stent, A Syrdal, and T Mishra. 2011. On the intelligibility of fast synthesized speech for individuals with early-onset blindness. The proceedings of the 13th international. Retrieved September 9, 2017 from http://dl.acm.org/citation.cfm?id=2049574  ",
      "doi": "10.1145/2049536.2049574"
    },
    {
      "text": "Bernhard Suhm, Brad Myers, and Alex Waibel. 2001. Multimodal error correction for speech user interfaces. ACM Transactions on Computer-Human Interaction 8, 1: 60--98.  ",
      "doi": "10.1145/371127.371166"
    },
    {
      "text": "M Turunen, J Hakulinen, and N Rajput. 2012. Evaluation of mobile and pervasive speech applications. Speech in Mobile and. Retrieved September 1, 2017 from",
      "doi": ""
    },
    {
      "text": "Keith Vertanen and Per Ola Kristensson. 2009. Automatic selection of recognition errors by respeaking the intended text. In Proceedings of the 2009 IEEE Workshop on Automatic Speech Recognition and Understanding, ASRU 2009, 130-- 135.",
      "doi": ""
    },
    {
      "text": "J. R. Williams. 1998. Guidelines for the Use of Multimedia in Instruction. Proceedings of the Human Factors and Ergonomics Society Annual Meeting 42, 20: 1447--1451.",
      "doi": ""
    },
    {
      "text": "Nicole Yankelovich and Jennifer Lai. 1999. Designing speech user interfaces. In CHI '99 extended abstracts on Human factors in computing systems - CHI '99, 124.  ",
      "doi": "10.1145/632716.632793"
    },
    {
      "text": "Bo Yi, Xiang Cao, Morten Fjeld, and Shengdong Zhao. 2012. Exploring user motivations for eyesfree interaction on mobile devices. Proceedings of the 2012 ACM annual conference on Human Factors in Computing Systems CHI 12 c: 2789.  ",
      "doi": "10.1145/2207676.2208678"
    },
    {
      "text": "The Best Voice Recognition Software of 2017 | Top Ten Reviews. Retrieved September 20, 2017 from http://www.toptenreviews.com/business/software/b est-voice-recognition-software/",
      "doi": ""
    },
    {
      "text": "The best voice recognition software of 2017 | TechRadar. Retrieved September 20, 2017 from http://www.techradar.com/news/the-best-voicerecognition-software-of-2017",
      "doi": ""
    }
  ]
}