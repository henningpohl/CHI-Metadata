{
  "doi": "10.1145/3491102.3501977",
  "title": "Lattice Menu: A Low-Error Gaze-Based Marking Menu Utilizing Target-Assisted Gaze Gestures on a Lattice of Visual Anchors",
  "published": "2022-04-29",
  "proctitle": "CHI '22: Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems",
  "pages": "1-12",
  "year": 2022,
  "badges": [],
  "abstract": "We present Lattice Menu, a gaze-based marking menu utilizing a lattice of visual anchors that helps perform accurate gaze pointing for menu item selection. Users who know the location of the desired item can leverage target-assisted gaze gestures for multilevel item selection by looking at visual anchors over the gaze trajectories. Our evaluation showed that Lattice Menu exhibits a considerably low error rate (~1%) and a quick menu selection time (1.3-1.6 s) for expert usage across various menu structures (4 \u00d7 4 \u00d7 4 and 6 \u00d7 6 \u00d7 6) and sizes (8, 10 and 12\u00b0). In comparison with a traditional gaze-based marking menu that does not utilize visual targets, Lattice Menu showed remarkably (~5 times) fewer menu selection errors for expert usage. In a post-interview, all 12 subjects preferred Lattice Menu, and most subjects (8 out of 12) commented that the provisioning of visual targets facilitated more stable menu selections with reduced eye fatigue.",
  "tags": [
    "Eye Tracking",
    "Gaze-Based Interaction",
    "AR/VR",
    "Marking Menu"
  ],
  "authors": [
    {
      "name": "Taejun Kim",
      "institution": "HCI Lab, School of Computing, Korea Advanced Institute of Science and Technology, Korea, Republic of",
      "img": "/do/10.1145/contrib-99659701560/rel-imgonly/_____.jpg",
      "acmid": "99659701560",
      "orcid": "missing"
    },
    {
      "name": "Auejin Ham",
      "institution": "HCI Lab, School of Computing, Korea Advanced Institute of Science and Technology, Korea, Republic of",
      "img": "/do/10.1145/contrib-99659701004/rel-imgonly/kakaotalk_20220216_114909300.jpg",
      "acmid": "99659701004",
      "orcid": "missing"
    },
    {
      "name": "Sunggeun Ahn",
      "institution": "HCI Lab, School of Computing, Korea Advanced Institute of Science and Technology, Korea, Republic of",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99658638580",
      "orcid": "missing"
    },
    {
      "name": "Geehyuk Lee",
      "institution": "HCI Lab, School of Computing, Korea Advanced Institute of Science and Technology, Korea, Republic of",
      "img": "/do/10.1145/contrib-81100384871/rel-imgonly/imgp3335.jpg",
      "acmid": "81100384871",
      "orcid": "missing"
    }
  ],
  "references": [
    {
      "text": "Sunggeun Ahn, Stephanie Santosa, Mark Parent, Daniel Wigdor, Tovi Grossman, and Marcello Giordano. 2021. StickyPie: A Gaze-Based, Scale-Invariant Marking Menu Optimized for AR/VR. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 1\u201316.",
      "doi": "10.1145/3411764.3445297"
    },
    {
      "text": "AH Bell, MA Meredith, AJ Van\u00a0Opstal, and DougP Munoz. 2006. Stimulus intensity modifies saccadic reaction time and visual response latency in the superior colliculus. Experimental Brain Research 174, 1 (2006), 53\u201359.",
      "doi": ""
    },
    {
      "text": "Dipesh Bhattarai, Marwan Suheimat, Andrew\u00a0J Lambert, and David\u00a0A Atchison. 2019. Fixation stability with Bessel beams. Optometry and Vision Science 96, 2 (2019), 95\u2013102.",
      "doi": ""
    },
    {
      "text": "Andy Cockburn, Carl Gutwin, Joey Scarr, and Sylvain Malacria. 2014. Supporting novice to expert transitions in user interfaces. ACM Computing Surveys (CSUR) 47, 2 (2014), 1\u201336.",
      "doi": "10.1145/2659796"
    },
    {
      "text": "H Deuble, W Wolf, and G Hauske. 1984. The evaluation of the oculomotor error signal. In Advances in Psychology. Vol.\u00a022. Elsevier, 55\u201362.",
      "doi": ""
    },
    {
      "text": "Heiko Drewes and Albrecht Schmidt. 2007. Interacting with the computer using gaze gestures. In IFIP Conference on Human-Computer Interaction. Springer, 475\u2013488.",
      "doi": ""
    },
    {
      "text": "Jay Henderson, Sylvain Malacria, Mathieu Nancel, and Edward Lank. 2020. Investigating the necessity of delay in marking menu invocation. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. 1\u201313.",
      "doi": "10.1145/3313831.3376296"
    },
    {
      "text": "Anthony Hornof, Anna Cavender, and Rob Hoselton. 2003. Eyedraw: a system for drawing pictures with eye movements. ACM SIGACCESS Accessibility and Computing77-78 (2003), 86\u201393.",
      "doi": "10.1145/1029014.1028647"
    },
    {
      "text": "Anke Huckauf and Mario\u00a0H Urbina. 2008. Gazing with pEYEs: towards a universal input for various applications. In Proceedings of the 2008 symposium on Eye tracking research & applications. 51\u201354.",
      "doi": "10.1145/1344471.1344483"
    },
    {
      "text": "Aulikki Hyrskykari, Howell Istance, and Stephen Vickers. 2012. Gaze gestures or dwell-based interaction?. In Proceedings of the Symposium on Eye Tracking Research and Applications. 229\u2013232.",
      "doi": "10.1145/2168556.2168602"
    },
    {
      "text": "FOVE inc.2021. FOVE0 Headset Specification. https://fove-inc.com/product/, last visited Dec. 2021.",
      "doi": ""
    },
    {
      "text": "HTC inc.2021. HTC Vive Pro Eye Headset Specification. https://www.vive.com/kr/product/vive-pro-eye/specs/, last visited Dec. 2021.",
      "doi": ""
    },
    {
      "text": "Toshiya Isomoto, Shota Yamanaka, and Buntarou Shizuki. 2020. Gaze-based Command Activation Technique Robust Against Unintentional Activation using Dwell-then-Gesture. (2020).",
      "doi": ""
    },
    {
      "text": "Laurent Itti and Christof Koch. 2001. Computational modelling of visual attention. Nature reviews neuroscience 2, 3 (2001), 194\u2013203.",
      "doi": ""
    },
    {
      "text": "RP Kalesnykas and PE Hallett. 1994. Retinal eccentricity and the latency of eye saccades. Vision research 34, 4 (1994), 517\u2013531.",
      "doi": ""
    },
    {
      "text": "Yvonne Kammerer, Katharina Scheiter, and Wolfgang Beinhauer. 2008. Looking my way through the menu: the impact of menu design and multimodal input on gaze-based menu selection. In Proceedings of the 2008 Symposium on Eye Tracking Research & Applications. 213\u2013220.",
      "doi": "10.1145/1344471.1344522"
    },
    {
      "text": "Gordon Kurtenbach and William Buxton. 1991. Issues in combining marking and direct manipulation techniques. In Proceedings of the 4th annual ACM symposium on User interface software and technology. 137\u2013144.",
      "doi": "10.1145/120782.120797"
    },
    {
      "text": "Gordon Kurtenbach and William Buxton. 1994. User learning and performance with marking menus. In Proceedings of the SIGCHI conference on Human factors in computing systems. 258\u2013264.",
      "doi": "10.1145/191666.191759"
    },
    {
      "text": "Gordon\u00a0Paul Kurtenbach. 1993. The design and evaluation of marking menus.University of Toronto Toronto.",
      "doi": ""
    },
    {
      "text": "Olivier Le\u00a0Meur and Zhi Liu. 2015. Saccadic model of eye movements for free-viewing condition. Vision research 116(2015), 152\u2013164.",
      "doi": ""
    },
    {
      "text": "P\u00e4ivi Majaranta, Ulla-Kaija Ahola, and Oleg \u0160pakov. 2009. Fast gaze typing with an adjustable dwell time. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 357\u2013360.",
      "doi": "10.1145/1518701.1518758"
    },
    {
      "text": "P\u00e4ivi Majaranta, Jari Laitinen, Jari Kangas, and Poika Isokoski. 2019. Inducing gaze gestures by static illustrations. In Proceedings of the 11th ACM Symposium on Eye Tracking Research & Applications. 1\u20135.",
      "doi": "10.1145/3317956.3318151"
    },
    {
      "text": "P\u00e4ivi Majaranta, Kari-Jouko R\u00e4ih\u00e4, Aulikki Hyrskykari, and Oleg \u0160pakov. 2019. Eye movements and human-computer interaction. In Eye Movement Research. Springer, 971\u20131015.",
      "doi": ""
    },
    {
      "text": "M\u00a0v Menozzi, A\u00a0v Buol, H Krueger, and Ch Mi\u00e8ge. 1994. Direction of gaze and comfort: discovering the relation for the ergonomic optimization of visual tasks. Ophthalmic and Physiological Optics 14, 4 (1994), 393\u2013399.",
      "doi": ""
    },
    {
      "text": "Marco Porta and Matteo Turina. 2008. Eye-S: a full-screen input modality for pure eye-based communication. In Proceedings of the 2008 symposium on Eye tracking research & applications. 27\u201334.",
      "doi": "10.1145/1344471.1344477"
    },
    {
      "text": "Vijay Rajanna and John\u00a0Paulin Hansen. 2018. Gaze typing in virtual reality: impact of keyboard design, selection method, and motion. In Proceedings of the 2018 ACM Symposium on Eye Tracking Research & Applications. 1\u201310.",
      "doi": "10.1145/3204493.3204541"
    },
    {
      "text": "Joey Scarr, Andy Cockburn, Carl Gutwin, and Philip Quinn. 2011. Dips and ceilings: understanding and supporting transitions to expertise in user interfaces. In Proceedings of the sigchi conference on human factors in computing systems. 2741\u20132750.",
      "doi": "10.1145/1978942.1979348"
    },
    {
      "text": "Immo Schuetz, T\u00a0Scott Murdison, Kevin\u00a0J MacKenzie, and Marina Zannoli. 2019. An Explanation of Fitts\u2019 Law-like Performance in Gaze-Based Selection Tasks Using a Psychophysics Approach. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. 1\u201313.",
      "doi": "10.1145/3290605.3300765"
    },
    {
      "text": "Ludwig Sidenmark and Hans Gellersen. 2019. Eye, head and torso coordination during gaze shifts in virtual reality. ACM Transactions on Computer-Human Interaction (TOCHI) 27, 1(2019), 1\u201340.",
      "doi": "10.1145/3361218"
    },
    {
      "text": "Ludwig Sidenmark and Hans Gellersen. 2019. Eye&head: Synergetic eye and head movement for gaze pointing and selection. In Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology. 1161\u20131174.",
      "doi": "10.1145/3332165.3347921"
    },
    {
      "text": "Lore Thaler, Alexander\u00a0C Sch\u00fctz, Melvyn\u00a0A Goodale, and Karl\u00a0R Gegenfurtner. 2013. What is the best fixation target? The effect of target shape on stability of fixational eye movements. Vision research 76(2013), 31\u201342.",
      "doi": ""
    },
    {
      "text": "Jan Theeuwes, Arthur\u00a0F Kramer, Sowon Hahn, and David\u00a0E Irwin. 1998. Our eyes do not always go where we want them to go: Capture of the eyes by new objects. Psychological Science 9, 5 (1998), 379\u2013385.",
      "doi": ""
    },
    {
      "text": "Jan Theeuwes, Sebastiaan Math\u00f4t, and Alan Kingstone. 2010. Object-based eye movements: The eyes prefer to stay within the same object. Attention, Perception, & Psychophysics 72, 3 (2010), 597\u2013601.",
      "doi": ""
    },
    {
      "text": "Geoffrey Tien and M\u00a0Stella Atkins. 2008. Improving hands-free menu selection using eyegaze glances and fixations. In Proceedings of the 2008 symposium on Eye tracking research & applications. 47\u201350.",
      "doi": "10.1145/1344471.1344482"
    },
    {
      "text": "Mario\u00a0H Urbina, Maike Lorenz, and Anke Huckauf. 2010. Pies with EYEs: the limits of hierarchical pie menus in gaze control. In Proceedings of the 2010 Symposium on Eye-Tracking Research & Applications. 93\u201396.",
      "doi": "10.1145/1743666.1743689"
    },
    {
      "text": "Jacob\u00a0O Wobbrock, Leah Findlater, Darren Gergle, and James\u00a0J Higgins. 2011. The aligned rank transform for nonparametric factorial analyses using only anova procedures. In Proceedings of the SIGCHI conference on human factors in computing systems. 143\u2013146.",
      "doi": "10.1145/1978942.1978963"
    },
    {
      "text": "Jacob\u00a0O Wobbrock, James Rubinstein, Michael\u00a0W Sawyer, and Andrew\u00a0T Duchowski. 2008. Longitudinal evaluation of discrete consecutive gaze gestures for text entry. In Proceedings of the 2008 symposium on Eye tracking research & applications. 11\u201318.",
      "doi": "10.1145/1344471.1344475"
    },
    {
      "text": "Steven Yantis and Anne\u00a0P Hillstrom. 1994. Stimulus-driven attentional capture: evidence from equiluminant visual objects.Journal of experimental psychology: Human perception and performance 20, 1(1994), 95.",
      "doi": ""
    }
  ]
}