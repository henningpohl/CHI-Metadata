{
  "doi": "10.1145/3491102.3517441",
  "title": "Toward User-Driven Algorithm Auditing: Investigating users\u2019 strategies for uncovering harmful algorithmic behavior",
  "published": "2022-04-29",
  "proctitle": "CHI '22: Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems",
  "pages": "1-19",
  "year": 2022,
  "badges": [],
  "abstract": "Recent work in HCI suggests that users can be powerful in surfacing harmful algorithmic behaviors that formal auditing approaches fail to detect. However, it is not well understood how users are often able to be so effective, nor how we might support more effective user-driven auditing. To investigate, we conducted a series of think-aloud interviews, diary studies, and workshops, exploring how users find and make sense of harmful behaviors in algorithmic systems, both individually and collectively. Based on our findings, we present a process model capturing the dynamics of and influences on users\u2019 search and sensemaking behaviors. We find that 1) users\u2019 search strategies and interpretations are heavily guided by their personal experiences with and exposures to societal bias; and 2) collective sensemaking amongst multiple users is invaluable in user-driven algorithm audits. We offer directions for the design of future methods and tools that can better support user-driven auditing.",
  "authors": [
    {
      "name": "Alicia DeVos",
      "institution": "Human-Computer Interaction Institute, Carnegie Mellon University, United States",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659898043",
      "orcid": "missing"
    },
    {
      "name": "Aditi Dhabalia",
      "institution": "Carnegie Mellon University, United States",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99660236782",
      "orcid": "missing"
    },
    {
      "name": "Hong Shen",
      "institution": "Human-Computer Interaction Institute, Carnegie Mellon University, United States",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659589829",
      "orcid": "0000-0002-5364-3718"
    },
    {
      "name": "Kenneth Holstein",
      "institution": "Human-Computer Interaction Institute, Carnegie Mellon University, United States",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659133171",
      "orcid": "0000-0001-6730-922X"
    },
    {
      "name": "Motahhare Eslami",
      "institution": "School of Computer Science, Carnegie Mellon University, United States",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "87959132657",
      "orcid": "0000-0002-1499-3045"
    }
  ],
  "references": [
    {
      "text": "[n.d.]. Google image search cements national stereotypes of \u2019racy\u2019 women. ([n. d.]). https://www.dw.com/en/google-image-search-cements-national-stereotypes-of-racy-women/a-56767605",
      "doi": ""
    },
    {
      "text": "2016. Do Google\u2019s \u2019unprofessional hair\u2019 results show it is racist?(2016). https://www.theguardian.com/technology/2016/apr/08/does-google-unprofessional-hair-results-prove-algorithms-racist-",
      "doi": ""
    },
    {
      "text": "2016. I saw a tweet saying \u201dGoogle unprofessional hairstyles for work\u201d. I did. Then I checked the \u2019professional\u2019 ones. (2016). https://twitter.com/HereroRocher/status/717457819864272896",
      "doi": ""
    },
    {
      "text": "2018. Gender and Jobs in Online Image Searches. (2018). https://www.pewresearch.org/social-trends/2018/12/17/gender-and-jobs-in-online-image-searches/",
      "doi": ""
    },
    {
      "text": "2019. Holy $%#@: Religion & Conscious Language. panel at ACES Annual Conference. https://aceseditors.org/conference/past-conferences/2019/sessions",
      "doi": ""
    },
    {
      "text": "2019. Search Engine Bias. (May 2019). https://therepproject.org/search-engine-bias/",
      "doi": ""
    },
    {
      "text": "[7] 2020. (2020). https://www2.census.gov/programs-surveys/decennial/2020/technical-documentation/questionnaires-and-instructions/questionnaires/2020-informational-questionnaire.pdf",
      "doi": ""
    },
    {
      "text": "Ali Alkhatib. 2021. To Live in Their Utopia: Why Algorithmic Systems Create Absurd Outcomes. Association for Computing Machinery, New York, NY, USA. https://doi.org/10.1145/3411764.3445740",
      "doi": ""
    },
    {
      "text": "Joshua Asplund, Motahhare Eslami, Hari Sundaram, Christian Sandvig, and Karrie Karahalios. 2020. Auditing Race and Gender Discrimination in Online Housing Markets. Proceedings of the International AAAI Conference on Web and Social Media 14, 1 (May 2020), 24\u201335. https://ojs.aaai.org/index.php/ICWSM/article/view/7276",
      "doi": ""
    },
    {
      "text": "Joshua Attenberg, Panos Ipeirotis, and Foster Provost. 2015. Beat the Machine: Challenging Humans to Find a Predictive Model\u2019s \u201cUnknown Unknowns\u201d. J. Data and Information Quality 6, 1, Article 1 (mar 2015), 17\u00a0pages. https://doi.org/10.1145/2700832",
      "doi": "10.1145/2700832"
    },
    {
      "text": "Eric\u00a0P.S. Baumer and M.\u00a0Six Silberman. 2011. When the Implication is Not to Design (Technology). In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (Vancouver, BC, Canada) (CHI \u201911). Association for Computing Machinery, New York, NY, USA, 2271\u20132274. https://doi.org/10.1145/1978942.1979275",
      "doi": "10.1145/1978942.1979275"
    },
    {
      "text": "Reuben Binns, Max\u00a0Van Kleek, Michael Veale, Ulrik Lyngs, Jun Zhao, and Nigel Shadbolt. 2018. \u2019It\u2019s Reducing a Human Being to a Percentage\u2019; Perceptions of Justice in Algorithmic Decisions. CoRR abs/1801.10408(2018). arxiv:1801.10408http://arxiv.org/abs/1801.10408",
      "doi": ""
    },
    {
      "text": "Su\u00a0Lin Blodgett, Solon Barocas, Hal Daum\u00e9\u00a0III, and Hanna Wallach. 2020. Language (technology) is power: A critical survey of\u201d bias\u201d in nlp. arXiv preprint arXiv:2005.14050(2020).",
      "doi": ""
    },
    {
      "text": "Khristopher\u00a0J Brooks. 2020. Twitter users say the platform crops out Black faces. CBS News (Sep 2020). https://www.cbsnews.com/news/twitter-image-cropping-algorithm-racial-profiling/",
      "doi": ""
    },
    {
      "text": "Anna Brown, Alexandra Chouldechova, Emily Putnam-Hornstein, Andrew Tobin, and Rhema Vaithianathan. 2019. Toward Algorithmic Accountability in Public Services: A Qualitative Study of Affected Community Perspectives on Algorithmic Decision-Making in Child Welfare Services. (2019), 1\u201312. https://doi.org/10.1145/3290605.3300271",
      "doi": ""
    },
    {
      "text": "Andrew\u00a0D Brown, Patrick Stacey, and Joe Nandhakumar. 2008. Making sense of sensemaking narratives. Human relations 61, 8 (2008), 1035\u20131062.",
      "doi": ""
    },
    {
      "text": "Joy Buolamwini and Timnit Gebru. 2018. Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification. 81 (23\u201324 Feb 2018), 77\u201391. http://proceedings.mlr.press/v81/buolamwini18a.html",
      "doi": ""
    },
    {
      "text": "\u00c1ngel\u00a0Alexander Cabrera, Abraham Druck, Jason\u00a0I. Hong, and Adam Perer. 2021. Discovering and Validating AI Errors With Crowdsourced Failure Reports. 5, CSCW2 (2021).",
      "doi": "10.1145/3479569"
    },
    {
      "text": "Metz Cade. [n.d.]. \u2018Nerd,\u2019 \u2018Nonsmoker,\u2019 \u2018Wrongdoer\u2019: How Might A.I. Label You? ImageNet Roulette, a digital art project and viral selfie app, exposes how biases have crept into the artificial-intelligence technologies changing our lives. ([n. d.]). https://www.nytimes.com/2019/09/20/arts/design/imagenet-trevor-paglen-ai-facial-recognition.html",
      "doi": ""
    },
    {
      "text": "DJ\u00a0PATIL CECILIA\u00a0MU\u00d1OZ, MEGAN\u00a0SMITH. 2016. Big Data: A Report on Algorithmic Systems, Opportunity, and Civil Rights. (2016). https://obamawhitehouse.archives.gov/sites/default/files/microsites/ostp/2016_0504_data_discrimination.pdf",
      "doi": ""
    },
    {
      "text": "Le Chen, Ruijun Ma, Anik\u00f3 Hann\u00e1k, and Christo Wilson. 2018. Investigating the Impact of Gender on Rank in Resume Search Engines. Association for Computing Machinery, New York, NY, USA, 1\u201314. https://doi.org/10.1145/3173574.3174225",
      "doi": ""
    },
    {
      "text": "Henriette Cramer, Jean Garcia-Gathright, Aaron Springer, and Sravana Reddy. 2018. Assessing and Addressing Algorithmic Bias in Practice. Interactions 25, 6 (Oct. 2018), 58\u201363. https://doi.org/10.1145/3278156",
      "doi": "10.1145/3278156"
    },
    {
      "text": "Kate Crawford and Trevor Paglen. 2021. Excavating AI: The politics of images in machine learning training sets. AI & Society (2021), 1\u201312.",
      "doi": ""
    },
    {
      "text": "Jeffrey Dastin. 2018. Amazon scraps secret AI recruiting tool that showed bias against women. (2018). https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G",
      "doi": ""
    },
    {
      "text": "MICHAEL\u00a0ANN DEVITO. 2021. Adaptive Folk Theorization as a Path to Algorithmic Literacy on Changing Platforms. (2021). https://doi.org/10.1145/3476080",
      "doi": ""
    },
    {
      "text": "Michael\u00a0A. DeVito, Jeremy Birnholtz, Jeffery\u00a0T. Hancock, Megan French, and Sunny Liu. 2018. How People Form Folk Theories of Social Media Feeds and What It Means for How We Study Self-Presentation. Association for Computing Machinery, New York, NY, USA, 1\u201312. https://doi.org/10.1145/3173574.3173694",
      "doi": ""
    },
    {
      "text": "Michael\u00a0A. DeVito, Darren Gergle, and Jeremy Birnholtz. 2017. \u201dAlgorithms Ruin Everything\u201d: #RIPTwitter, Folk Theories, and Resistance to Algorithmic Change in Social Media. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (Denver, Colorado, USA) (CHI \u201917). Association for Computing Machinery, New York, NY, USA, 3163\u20133174. https://doi.org/10.1145/3025453.3025659",
      "doi": "10.1145/3025453.3025659"
    },
    {
      "text": "Motahhare Eslami, Karrie Karahalios, Christian Sandvig, Kristen Vaccaro, Aimee Rickman, Kevin Hamilton, and Alex Kirlik. 2016. First I \u201dlike\u201d It, Then I Hide It: Folk Theories of Social Feeds. Association for Computing Machinery, New York, NY, USA, 2371\u20132382. https://doi.org/10.1145/2858036.2858494",
      "doi": "10.1145/2858036.2858494"
    },
    {
      "text": "Motahhare Eslami, Sneha\u00a0R. Krishna\u00a0Kumaran, Christian Sandvig, and Karrie Karahalios. 2018. Communicating Algorithmic Process in Online Behavioral Advertising. Association for Computing Machinery, New York, NY, USA, 1\u201313. https://doi.org/10.1145/3173574.3174006",
      "doi": ""
    },
    {
      "text": "Motahhare Eslami, Aimee Rickman, Kristen Vaccaro, Amirhossein Aleyasen, Andy Vuong, Karrie Karahalios, Kevin Hamilton, and Christian Sandvig. 2015. \u201dI Always Assumed That I Wasn\u2019t Really That Close to [Her]\u201d: Reasoning about Invisible Algorithms in News Feeds. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (Seoul, Republic of Korea) (CHI \u201915). Association for Computing Machinery, New York, NY, USA, 153\u2013162. https://doi.org/10.1145/2702123.2702556",
      "doi": "10.1145/2702123.2702556"
    },
    {
      "text": "Motahhare Eslami, Kristen Vaccaro, Karrie Karahalios, and Kevin Hamilton. 2017. Be careful; Things can be worse than they appear - Understanding biased algorithms and users\u2019 behavior around them in rating platforms\u201d. (2017), 62\u201371. Funding Information: This work was funded by NSF grant CHS-1564041. Publisher Copyright: \u00a9 Copyright 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.; 11th International Conference on Web and Social Media, ICWSM 2017 ; Conference date: 15-05-2017 Through 18-05-2017.",
      "doi": ""
    },
    {
      "text": "Motahhare Eslami, Kristen Vaccaro, Min\u00a0Kyung Lee, Amit Elazari Bar\u00a0On, Eric Gilbert, and Karrie Karahalios. 2019. User Attitudes towards Algorithmic Opacity and Transparency in Online Reviewing Platforms. (2019), 1\u201314. https://doi.org/10.1145/3290605.3300724",
      "doi": "10.1145/3290605.3300724"
    },
    {
      "text": "Motahhare Eslami, Kristen Vaccaro, Min\u00a0Kyung Lee, Amit Elazari Bar\u00a0On, Eric Gilbert, and Karrie Karahalios. 2019. User attitudes towards algorithmic opacity and transparency in online reviewing platforms. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. 1\u201314.",
      "doi": "10.1145/3290605.3300724"
    },
    {
      "text": "Nancy Fraser. 1990. Rethinking the public sphere: A contribution to the critique of actually existing democracy. Social text25/26(1990), 56\u201380.",
      "doi": ""
    },
    {
      "text": "Batya Friedman and Helen Nissenbaum. 1996. Bias in Computer Systems. ACM Trans. Inf. Syst. 14, 3 (July 1996), 330\u2013347. https://doi.org/10.1145/230538.230561",
      "doi": "10.1145/230538.230561"
    },
    {
      "text": "Day\u00a0S. Goldstone, R.\u00a0L. and J.\u00a0Y. Son. 2010. Comparison. In Towards a theory of thinking, Goel Vinod von M\u00fcller\u00a0Albrecht Glatzeder, Britt (Ed.). Springer, Berlin, Heidelberg, 103\u2013121.",
      "doi": ""
    },
    {
      "text": "Mitchell\u00a0L. Gordon, Kaitlyn Zhou, Kayur Patel, Tatsunori Hashimoto, and Michael\u00a0S. Bernstein. 2021. The Disagreement Deconvolution: Bringing Machine Learning Performance Metrics In Line With Reality. Association for Computing Machinery, New York, NY, USA. https://doi.org/10.1145/3411764.3445423",
      "doi": "10.1145/3411764.3445423"
    },
    {
      "text": "Nina Grgic-Hlaca, Elissa\u00a0M. Redmiles, Krishna\u00a0P. Gummadi, and Adrian Weller. 2018. Human Perceptions of Fairness in Algorithmic Decision Making: A Case Study of Criminal Risk Prediction. (2018), 903\u2013912. https://doi.org/10.1145/3178876.3186138",
      "doi": ""
    },
    {
      "text": "Ben Guarino. [n.d.]. Google faulted for racial bias in image search results for black teenagers. ([n. d.]). https://www.dw.com/en/google-image-search-cements-national-stereotypes-of-racy-women/a-56767605",
      "doi": ""
    },
    {
      "text": "Jessica Guynn. 2015. Google photos labeled black people \u2018gorillas\u2019. USA Today (June 2015). https://www.usatoday.com/story/tech/2015/07/01/google-apologizes-after-photos-identify-black-people-as-gorillas/29567465",
      "doi": ""
    },
    {
      "text": "Anik\u00f3 Hann\u00e1k, Piotr Sapiezynski, Arash\u00a0Molavi Khaki, David Lazer, Alan Mislove, and Christo Wilson. 2017. Measuring Personalization of Web Search. CoRR abs/1706.05011(2017). arXiv:1706.05011http://arxiv.org/abs/1706.05011",
      "doi": ""
    },
    {
      "text": "Aniko Hannak, Gary Soeller, David Lazer, Alan Mislove, and Christo Wilson. 2014. Measuring Price Discrimination and Steering on E-Commerce Web Sites. (2014), 305\u2013318. https://doi.org/10.1145/2663716.2663744",
      "doi": ""
    },
    {
      "text": "Aniko Hannak, Gary Soeller, David Lazer, Alan Mislove, and Christo Wilson. 2014. Measuring Price Discrimination and Steering on E-Commerce Web Sites. (2014), 305\u2013318. https://doi.org/10.1145/2663716.2663744",
      "doi": ""
    },
    {
      "text": "Alex Hern. 2020. Twitter apologises for \u2019racist\u2019 image-cropping algorithm. The Guardian (Sept. 2020). https://www.theguardian.com/technology/2020/sep/21/twitter-apologises-for-racist-image-cropping-algorithm",
      "doi": ""
    },
    {
      "text": "Kenneth Holstein, Erik Harpstead, Rebecca Gulotta, and Jodi Forlizzi. 2020. Replay enactments: Exploring possible futures through historical data. In Proceedings of the 2020 ACM Designing Interactive Systems Conference. 1607\u20131618.",
      "doi": "10.1145/3357236.3395427"
    },
    {
      "text": "Kenneth Holstein, Jennifer Wortman\u00a0Vaughan, Hal Daum\u00e9, Miro Dudik, and Hanna Wallach. 2019. Improving Fairness in Machine Learning Systems. Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (May 2019). https://doi.org/10.1145/3290605.3300830",
      "doi": "10.1145/3290605.3300830"
    },
    {
      "text": "Samuel\u00a0GB Johnson, Greeshma Rajeev-Kumar, and Frank\u00a0C Keil. 2016. Sense-making under ignorance. Cognitive psychology 89(2016), 39\u201370.",
      "doi": ""
    },
    {
      "text": "Nadia Karizat, Dan Delmonaco, Motahhare Eslami, and Nazanin Andalibi. 2021. Algorithmic Folk Theories and Identity: How TikTok Users Co-Produce Knowledge of Identity and Engage in Algorithmic Resistance. Proc. ACM Hum.-Comput. Interact. 5, CSCW2, Article 305 (oct 2021), 44\u00a0pages. https://doi.org/10.1145/3476046",
      "doi": "10.1145/3476046"
    },
    {
      "text": "Matthew Kay, Cynthia Matuszek, and Sean\u00a0A. Munson. 2015. Unequal Representation and Gender Stereotypes in Image Search Results for Occupations. (2015), 3819\u20133828. https://doi.org/10.1145/2702123.2702520",
      "doi": ""
    },
    {
      "text": "Anna Kramer. 2021. Twitter\u2019s image cropping was biased, so it dumped the algorithm. https://www.protocol.com/twitter-image-cropping-algorithm-biased",
      "doi": ""
    },
    {
      "text": "Michael\u00a0W Kramer. 2017. Sensemaking. The international encyclopedia of organizational communication (2017), 1\u201310.",
      "doi": ""
    },
    {
      "text": "Juhi Kulshrestha, Motahhare Eslami, Johnnatan Messias, Muhammad\u00a0Bilal Zafar, Saptarshi Ghosh, Krishna\u00a0P. Gummadi, and Karrie Karahalios. 2017. Quantifying Search Bias: Investigating Sources of Bias for Political Searches in Social Media. CoRR abs/1704.01347(2017). arxiv:1704.01347http://arxiv.org/abs/1704.01347",
      "doi": ""
    },
    {
      "text": "Michelle Seng\u00a0Ah Lee and Jatinder Singh. [n.d.]. The Landscape and Gaps in Open Source Fairness Toolkits (September 18, 2020), biburl = https://doi.org/10.1145/3411764.3445261. ([n. d.]).",
      "doi": ""
    },
    {
      "text": "Rosina Lippi-Green. [n.d.]. English with an Accent: Language, Ideology and Discrimination in the United States. ([n. d.]).",
      "doi": ""
    },
    {
      "text": "Michael\u00a0A. Madaio, Luke Stark, Jennifer Wortman\u00a0Vaughan, and Hanna Wallach. 2020. Co-Designing Checklists to Understand Organizational Challenges and Opportunities around Fairness in AI. (2020), 1\u201314. https://doi.org/10.1145/3313831.3376445",
      "doi": ""
    },
    {
      "text": "Bugcrowd\u00a0Product Marketing. 2020. Reducing Noise in Crowdsourced Security. https://www.bugcrowd.com/blog/reducing-noise-in-crowdsourced-security/",
      "doi": ""
    },
    {
      "text": "Aarian Marshall. 2021. Gig Workers Gather Their Own Data to Check the Algorithm\u2019s Math. https://www.wired.com/story/gig-workers-gather-data-check-algorithm-math/",
      "doi": ""
    },
    {
      "text": "Milagros Miceli, Julian Posada, and Tianling Yang. 2021. Studying Up Machine Learning Data: Why Talk About Bias When We Mean Power?arxiv:2109.08131\u00a0[cs.HC]",
      "doi": ""
    },
    {
      "text": "Safiya\u00a0Umoja Noble. 2018. Algorithms of Oppression: How Search Engines Reinforce Racism. (2018). https://www.jstor.org/stable/j.ctt1pwt9w5",
      "doi": ""
    },
    {
      "text": "Rodrigo Ochigame and Katherine Ye. 2021. Search Atlas: Visualizing Divergent Search Results Across Geopolitical Borders. (2021), 1970\u20131983. https://doi.org/10.1145/3461778.3462032",
      "doi": ""
    },
    {
      "text": "Peter Pirolli and Stuart Card. 2005. The sensemaking process and leverage points for analyst technology as identified through cognitive task analysis. In Proceedings of international conference on intelligence analysis, Vol.\u00a05. McLean, VA, USA, 2\u20134.",
      "doi": ""
    },
    {
      "text": "Inioluwa\u00a0Deborah Raji, Andrew Smart, Rebecca\u00a0N. White, Margaret Mitchell, Timnit Gebru, Ben Hutchinson, Jamila Smith-Loud, Daniel Theron, and Parker Barnes. 2020. Closing the AI Accountability Gap: Defining an End-to-End Framework for Internal Algorithmic Auditing. CoRR abs/2001.00973(2020). arxiv:2001.00973http://arxiv.org/abs/2001.00973",
      "doi": ""
    },
    {
      "text": "Ronald\u00a0E. Robertson, Shan Jiang, Kenneth Joseph, Lisa Friedland, David Lazer, and Christo Wilson. 2018. Auditing Partisan Audience Bias within Google Search. Proc. ACM Hum.-Comput. Interact. 2, CSCW, Article 148 (Nov. 2018), 22\u00a0pages. https://doi.org/10.1145/3274417",
      "doi": "10.1145/3274417"
    },
    {
      "text": "Aja Romano. 2019. A group of YouTubers is trying to prove the site systematically demonetizes queer content. (2019). https://www.vox.com/culture/2019/10/10/20893258/youtube-lgbtq-censorship-demonetization-nerd-city-algorithm-report",
      "doi": ""
    },
    {
      "text": "Daniel\u00a0M Russell, Mark\u00a0J Stefik, Peter Pirolli, and Stuart\u00a0K Card. 1993. The cost structure of sensemaking. (1993), 269\u2013276.",
      "doi": ""
    },
    {
      "text": "Christian Sandvig, Kevin Hamilton, Karrie Karahalios, and Cedric Langbort. 2014. Auditing algorithms: Research methods for detecting discrimination on internet platforms. Data and Discrimination: Converting Critical Concerns into Productive Inquiry (2014).",
      "doi": ""
    },
    {
      "text": "Mike Schaekermann, Joslin Goh, Kate Larson, and Edith Law. 2018. Resolvable vs. Irresolvable Disagreement: A Study on Worker Deliberation in Crowd Work. Proc. ACM Hum.-Comput. Interact. 2, CSCW, Article 154 (nov 2018), 19\u00a0pages. https://doi.org/10.1145/3274423",
      "doi": "10.1145/3274423"
    },
    {
      "text": "[68] Sarita Schoenebeck.[n.d.]. ([n. d.]). https://twitter.com/syardi/status/1276587095767887874?s=20",
      "doi": ""
    },
    {
      "text": "Nick Seaver. 2017. Algorithms as culture: Some tactics for the ethnography of algorithmic systems. Big Data & Society 4, 2 (2017), 2053951717738104. https://doi.org/10.1177/2053951717738104 arXiv:https://doi.org/10.1177/2053951717738104",
      "doi": ""
    },
    {
      "text": "Andrew\u00a0D. Selbst, Danah Boyd, Sorelle\u00a0A. Friedler, Suresh Venkatasubramanian, and Janet Vertesi. 2019. Fairness and Abstraction in Sociotechnical Systems. (2019), 59\u201368. https://doi.org/10.1145/3287560.3287598",
      "doi": ""
    },
    {
      "text": "Hong Shen, Alicia DeVos, Motahhare Eslami, and Kenneth Holstein. 2021. Everyday Algorithm Auditing: Understanding the Power of Everyday Users in Surfacing Harmful Algorithmic Behaviors. Proc. ACM Hum.-Comput. Interact. 5, CSCW2, Article 433 (oct 2021), 29\u00a0pages. https://doi.org/10.1145/3479577",
      "doi": "10.1145/3479577"
    },
    {
      "text": "Ignacio Siles, Andr\u00e9s Segura-Castillo, Ricardo Sol\u00eds, and M\u00f3nica Sancho. 2020. Folk theories of algorithmic recommendations on Spotify: Enacting data assemblages in the global South. Big Data & Society 7, 1 (2020), 2053951720923377. https://doi.org/10.1177/2053951720923377 arXiv:https://doi.org/10.1177/2053951720923377",
      "doi": ""
    },
    {
      "text": "Jina Suh, Soroush Ghorashi, Gonzalo Ramos, Nan-Chen Chen, Steven Drucker, Johan Verwey, and Patrice Simard. 2019. AnchorViz: Facilitating Semantic Data Exploration and Concept Discovery for Interactive Machine Learning. ACM Trans. Interact. Intell. Syst. 10, 1, Article 7 (Aug. 2019), 38\u00a0pages. https://doi.org/10.1145/3241379",
      "doi": "10.1145/3241379"
    },
    {
      "text": "Latanya Sweeney. 2013. Discrimination in Online Ad Delivery. (2013). arxiv:1301.6822\u00a0[cs.IR]",
      "doi": ""
    },
    {
      "text": "Niels van Berkel, Jorge Goncalves, Danula Hettiachchi, Senuri Wijenayake, Ryan\u00a0M. Kelly, and Vassilis Kostakos. 2019. Crowdsourcing Perceptions of Fair Predictors for Machine Learning: A Recidivism Case Study. Proc. ACM Hum.-Comput. Interact. 3, CSCW, Article 28 (Nov. 2019), 21\u00a0pages. https://doi.org/10.1145/3359130",
      "doi": "10.1145/3359130"
    },
    {
      "text": "Julia Velkova and Anne Kaun. 2019. Algorithmic resistance: Media practices and the politics of repair. Information, Communication & Society(2019), 1\u201318.",
      "doi": ""
    },
    {
      "text": "Neil Vigdor. 2019. Apple card investigated after gender discrimination complaints. The New York Times. (2019). https://www.nytimes.com/2019/11/10/business/Apple-credit-card-investigation.html",
      "doi": ""
    },
    {
      "text": "James Vincent. 2016. Twitter taught Microsoft\u2019s AI chatbot to be a racist asshole in less than a day. The Verge (Mar 2016). https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist",
      "doi": ""
    },
    {
      "text": "Cedric\u00a0Deslandes Whitney, Teresa Naval, Elizabeth Quepons, Simrandeep Singh, Steven\u00a0R Rick, and Lilly Irani. 2021. HCI Tactics for Politics from Below: Meeting the Challenges of Smart Cities. Association for Computing Machinery, New York, NY, USA. https://doi.org/10.1145/3411764.3445314",
      "doi": ""
    },
    {
      "text": "Allison Woodruff, Sarah\u00a0E. Fox, Steven Rousso-Schindler, and Jeffrey Warshaw. 2018. A Qualitative Exploration of Perceptions of Algorithmic Fairness. (2018), 1\u201314. https://doi.org/10.1145/3173574.3174230",
      "doi": ""
    },
    {
      "text": "Meg Young, Lassana Magassa, and Batya Friedman. [n.d.]. Toward inclusive tech policy design: a method for underrepresented voices to strengthen tech policy documents. Ethics and Information Technology([n. d.]).",
      "doi": ""
    },
    {
      "text": "James Zou and Londa Schiebinger. 2018. AI can be sexist and racist\u2014it\u2019s time to make it fair. Nature 559(2018), 324\u2013326.",
      "doi": ""
    }
  ]
}