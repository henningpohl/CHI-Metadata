{
  "doi": "10.1145/3491102.3502015",
  "title": "SilentSpeller: Towards mobile, hands-free, silent speech text entry using electropalatography",
  "published": "2022-04-29",
  "proctitle": "CHI '22: Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems",
  "pages": "1-19",
  "year": 2022,
  "badges": [],
  "abstract": "Speech is inappropriate in many situations, limiting when voice control can be used. Most unvoiced speech text entry systems can not be used while on-the-go due to movement artifacts. Using a dental retainer with capacitive touch sensors, SilentSpeller tracks tongue movement, enabling users to type by spelling words without voicing. SilentSpeller achieves an average 97% character accuracy in offline isolated word testing on a 1164-word dictionary. Walking has little effect on accuracy; average offline character accuracy was roughly equivalent on 107 phrases entered while walking (97.5%) or seated (96.5%). To demonstrate extensibility, the system was tested on 100 unseen words, leading to an average 94% accuracy. Live text entry speeds for seven participants averaged 37 words per minute at 87% accuracy. Comparing silent spelling to current practice suggests that SilentSpeller may be a viable alternative for silent mobile text entry.",
  "tags": [
    "wearable computing",
    "text entry",
    "silent speech interface"
  ],
  "authors": [
    {
      "name": "Naoki Kimura",
      "institution": "The University of Tokyo, The University of Tokyo, Japan",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81545212556",
      "orcid": "missing"
    },
    {
      "name": "Tan Gemicioglu",
      "institution": "Georgia Institute of Technology, United States",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659700445",
      "orcid": "missing"
    },
    {
      "name": "Jonathan Womack",
      "institution": "Georgia Institute of Technology, United States",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659680244",
      "orcid": "missing"
    },
    {
      "name": "Richard Li",
      "institution": "Paul G. Allen School of Computer Science & Engineering, University of Washington, United States",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659462855",
      "orcid": "0000-0002-0192-3371"
    },
    {
      "name": "Yuhui Zhao",
      "institution": "School of Interactive Computing, Georgia Institute of Technology, United States",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659512671",
      "orcid": "0000-0001-8976-2041"
    },
    {
      "name": "Abdelkareem Bedri",
      "institution": "HCII, Carnegie Mellon University, United States",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99658635707",
      "orcid": "missing"
    },
    {
      "name": "Zixiong Su",
      "institution": "GSII, The University of Tokyo, Japan",
      "img": "/do/10.1145/contrib-99659729502/rel-imgonly/profile.jpg",
      "acmid": "99659729502",
      "orcid": "0000-0001-6048-3268"
    },
    {
      "name": "Alex Olwal",
      "institution": "Google Inc., United States",
      "img": "/do/10.1145/contrib-81309497296/rel-imgonly/alex.png",
      "acmid": "81309497296",
      "orcid": "0000-0001-7772-0530"
    },
    {
      "name": "Jun Rekimoto",
      "institution": "The University of Tokyo, Japan and Sony CSL Kyoto, Japan",
      "img": "/do/10.1145/contrib-81100008564/rel-imgonly/p2.png",
      "acmid": "81100008564",
      "orcid": "0000-0002-3629-2514"
    },
    {
      "name": "Thad Starner",
      "institution": "School of Interactive Computing, Georgia Institute of Technology, United States",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81100175439",
      "orcid": "missing"
    }
  ],
  "references": [
    {
      "text": "S.\u00a0T. Ahi, H. Kambara, and Y. Koike. 2011. A Dictionary-Driven P300 Speller With a Modified Interface. IEEE Transactions on Neural Systems and Rehabilitation Engineering 19, 1(2011), 6\u201314.",
      "doi": ""
    },
    {
      "text": "Abdelkareem Bedri, Himanshu Sahni, Pavleen Thukral, Thad Starner, David Byrd, Peter Presti, Gabriel Reyes, Maysam Ghovanloo, and Zehua Guo. 2015. Toward silent-speech control of consumer wearables. Computer 48, 10 (2015), 54\u201362.",
      "doi": "10.1109/MC.2015.310"
    },
    {
      "text": "Florent Bocquelet, Thomas Hueber, Laurent Girin, Christophe Savariaux, and Blaise Yvert. 2016. Real-Time Control of an Articulatory-Based Speech Synthesizer for Brain Computer Interfaces. PLOS Computational Biology 12, 11 (11 2016), 1\u201328. https://doi.org/10.1371/journal.pcbi.1005119",
      "doi": ""
    },
    {
      "text": "H\u00e9ctor\u00a0A Caltenco, Bj\u00f6rn Breidegard, and Lotte\u00a0NS Andreasen\u00a0Struijk. 2014. On the tip of the tongue: Learning typing and pointing with an intra-oral computer interface. Disability and Rehabilitation: Assistive Technology 9, 4 (2014), 307\u2013317.",
      "doi": ""
    },
    {
      "text": "Steven\u00a0J Castellucci and I\u00a0Scott MacKenzie. 2008. Graffiti vs. unistrokes: an empirical comparison. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 305\u2013308.",
      "doi": "10.1145/1357054.1357106"
    },
    {
      "text": "Xiaogang Chen, Yijun Wang, Masaki Nakanishi, Xiaorong Gao, Tzyy-Ping Jung, and Shangkai Gao. 2015. High-speed spelling with a noninvasive brain\u2013computer interface. Proceedings of the national academy of sciences 112, 44(2015), E6058\u2013E6067.",
      "doi": ""
    },
    {
      "text": "Edward Clarkson, James Clawson, Kent Lyons, and Thad Starner. 2005. An Empirical Study of Typing Rates on Mini-QWERTY Keyboards. In CHI \u201905 Extended Abstracts on Human Factors in Computing Systems (Portland, OR, USA) (CHI EA \u201905). Association for Computing Machinery, New York, NY, USA, 1288\u20131291. https://doi.org/10.1145/1056808.1056898",
      "doi": "10.1145/1056808.1056898"
    },
    {
      "text": "J. Clawson, K. Lyons, T. Starner, and E. Clarkson. 2005. The impacts of limited visual feedback on mobile text entry for the Twiddler and mini-QWERTY keyboards. In Ninth IEEE International Symposium on Wearable Computers (ISWC\u201905). 170\u2013177.",
      "doi": ""
    },
    {
      "text": "James Clawson, Thad Starner, Daniel Kohlsdorf, David\u00a0P. Quigley, and Scott Gilliland. 2014. Texting While Walking: An Evaluation of Mini-Qwerty Text Input While on-the-Go. In Proceedings of the 16th International Conference on Human-Computer Interaction with Mobile Devices and Services (Toronto, ON, Canada) (MobileHCI \u201914). Association for Computing Machinery, New York, NY, USA, 339\u2013348. https://doi.org/10.1145/2628363.2628408",
      "doi": "10.1145/2628363.2628408"
    },
    {
      "text": "Tam\u00e1s\u00a0G\u00e1bor Csap\u00f3, Tam\u00e1s Gr\u00f3sz, G\u00e1bor Gosztolya, L\u00e1szl\u00f3 T\u00f3th, and Alexandra Mark\u00f3. 2017. DNN-Based Ultrasound-to-Speech Conversion for a Silent Speech Interface. In INTERSPEECH.",
      "doi": ""
    },
    {
      "text": "Bruce Denby, Thomas Schultz, Kiyoshi Honda, Thomas Hueber, Jim\u00a0M Gilbert, and Jonathan\u00a0S Brumberg. 2010. Silent speech interfaces. Speech Communication 52, 4 (2010), 270\u2013287.",
      "doi": "10.1016/j.specom.2009.08.002"
    },
    {
      "text": "M.J. Fagan, S.R. Ell, J.M. Gilbert, E. Sarrazin, and P.M. Chapman. 2008. Development of a (silent) speech recognition system for patients following laryngectomy. Medical Engineering & Physics 30, 4 (2008), 419 \u2013 425. https://doi.org/10.1016/j.medengphy.2007.05.003",
      "doi": ""
    },
    {
      "text": "Torsten Felzer, I\u00a0Scott MacKenzie, and Stephan Rinderknecht. 2014. Applying small-keyboard computer control to the real world. In International Conference on Computers for Handicapped Persons. Springer, 180\u2013187.",
      "doi": ""
    },
    {
      "text": "Jo\u00e3o Freitas, Ant\u00f3nio Teixeira, Miguel\u00a0Sales Dias, and Samuel Silva. 2017. An Introduction to Silent Speech Interfaces. Springer.",
      "doi": ""
    },
    {
      "text": "Masaaki Fukumoto. 2018. SilentVoice: Unnoticeable Voice Input by Ingressive Speech. In The 31st Annual ACM Symposium on User Interface Software and Technology. ACM, 237\u2013246.",
      "doi": "10.1145/3242587.3242603"
    },
    {
      "text": "Yang Gao, Yincheng Jin, Jiyang Li, Seokmin Choi, and Zhanpeng Jin. 2020. EchoWhisper: Exploring an Acoustic-Based Silent Speech Interface for Smartphone Users. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 4, 3, Article 80 (Sept. 2020), 27\u00a0pages. https://doi.org/10.1145/3411830",
      "doi": "10.1145/3411830"
    },
    {
      "text": "Mayank Goel, Leah Findlater, and Jacob Wobbrock. 2012. WalkType: using accelerometer data to accomodate situational impairments in mobile touch screen text entry. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 2687\u20132696.",
      "doi": "10.1145/2207676.2208662"
    },
    {
      "text": "Mayank Goel, Jacob Wobbrock, and Shwetak Patel. 2012. GripSense: using built-in sensors to detect hand posture and pressure on commodity mobile phones. In Proceedings of the 25th annual ACM symposium on User interface software and technology. 545\u2013554.",
      "doi": "10.1145/2380116.2380184"
    },
    {
      "text": "Jose\u00a0A. Gonzalez, Lam\u00a0A. Cheah, James\u00a0M. Gilbert, Jie Bai, Stephen\u00a0R. Ell, Phil\u00a0D. Green, and Roger\u00a0K. Moore. 2016. A Silent Speech System Based on Permanent Magnet Articulography and Direct Synthesis. Comput. Speech Lang. 39, C (Sept. 2016), 67\u201387. https://doi.org/10.1016/j.csl.2016.02.002",
      "doi": ""
    },
    {
      "text": "Tam\u00e1s Gr\u00f3sz, G\u00e1bor Gosztolya, L\u00e1szl\u00f3 T\u00f3th, Tam\u00e1s Csap\u00f3, and Alexandra Mark\u00f3. 2018. F0 Estimation for DNN-Based Ultrasound Silent Speech Interfaces. https://doi.org/10.1109/ICASSP.2018.8461732",
      "doi": ""
    },
    {
      "text": "W. Hardcastle, W. Jones, C. Knight, A. Trudgeon, and G. Calder. 1989. New developments in electropalatography: A state-of-the-art report. Clinical Linguistics & Phonetics 3, 1 (1989), 1\u201338. https://doi.org/10.3109/02699208908985268 arXiv:https://doi.org/10.3109/02699208908985268",
      "doi": ""
    },
    {
      "text": "Tatsuya Hirahara, Makoto Otani, Shota Shimizu, Tomoki Toda, Keigo Nakamura, Yoshitaka Nakajima, and Kiyohiro Shikano. 2010. Silent-speech Enhancement Using Body-conducted Vocal-tract Resonance Signals. Speech Commun. 52, 4 (April 2010), 301\u2013313. https://doi.org/10.1016/j.specom.2009.12.001",
      "doi": "10.1016/j.specom.2009.12.001"
    },
    {
      "text": "T. Hueber, G. Aversano, G. Cholle, B. Denby, G. Dreyfus, Y. Oussar, P. Roussel, and M. Stone. 2007. Eigentongue Feature Extraction for an Ultrasound-Based Silent Speech Interface. In 2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP \u201907, Vol.\u00a01. I\u20131245\u2013I\u20131248. https://doi.org/10.1109/ICASSP.2007.366140",
      "doi": ""
    },
    {
      "text": "Thomas Hueber, Elie-Laurent Benaroya, G\u00e9rard Chollet, Bruce Denby, G\u00e9rard Dreyfus, and Maureen Stone. 2010. Development of a silent speech interface driven by ultrasound and optical images of the tongue and lips. Speech Communication 52, 4 (2010), 288 \u2013 300. https://doi.org/10.1016/j.specom.2009.11.004 Silent Speech Interfaces.",
      "doi": "10.1016/j.specom.2009.11.004"
    },
    {
      "text": "Yan Ji, Licheng Liu, Hongcui Wang, Zhilei Liu, Zhibin Niu, and Bruce Denby. 2018. Updating the Silent Speech Challenge benchmark with deep learning. Speech Communication 98(2018), 42 \u2013 50. https://doi.org/10.1016/j.specom.2018.02.002",
      "doi": "10.1016/j.specom.2018.02.002"
    },
    {
      "text": "Biing-Hwang Juang and Lawrence\u00a0R Rabiner. 2005. Automatic speech recognition\u2013a brief history of the technology development. Georgia Institute of Technology. Atlanta Rutgers University and the University of California. Santa Barbara 1 (2005), 67.",
      "doi": ""
    },
    {
      "text": "Arnav Kapur, Shreyas Kapur, and Pattie Maes. 2018. AlterEgo: A Personalized Wearable Silent Speech Interface. In 23rd International Conference on Intelligent User Interfaces. ACM, 43\u201353.",
      "doi": "10.1145/3172944.3172977"
    },
    {
      "text": "Arnav Kapur, Utkarsh Sarawgi, Eric Wadkins, Matthew Wu, Nora Hollenstein, and Pattie Maes. 2020. Non-Invasive Silent Speech Recognition in Multiple Sclerosis with Dysphonia. In Machine Learning for Health Workshop. 25\u201338.",
      "doi": ""
    },
    {
      "text": "Naoki Kimura, Tan Gemicioglu, Jonathan Womack, Richard Li, Yuhui Zhao, Abdelkareem Bedri, Alex Olwal, Jun Rekimoto, and Thad Starner. 2021. Mobile, Hands-Free, Silent Speech Texting Using SilentSpeller. Association for Computing Machinery, New York, NY, USA. https://doi.org/10.1145/3411763.3451552",
      "doi": "10.1145/3411763.3451552"
    },
    {
      "text": "Naoki Kimura, Michinari Kono, and Jun Rekimoto. 2019. SottoVoce: An Ultrasound Imaging-Based Silent Speech Interaction Using Deep Neural Networks. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (Glasgow, Scotland Uk) (CHI \u201919). Association for Computing Machinery, New York, NY, USA, 1\u201311. https://doi.org/10.1145/3290605.3300376",
      "doi": "10.1145/3290605.3300376"
    },
    {
      "text": "Yongkuk Lee, Connor Howe, Saswat Mishra, Dong\u00a0Sup Lee, Musa Mahmood, Matthew Piper, Youngbin Kim, Katie Tieu, Hun-Soo Byun, James\u00a0P. Coffey, Mahdis Shayan, Youngjae Chun, Richard\u00a0M. Costanzo, and Woon-Hong Yeo. 2018. Wireless, intraoral hybrid electronics for real-time quantification of sodium intake toward hypertension management. Proceedings of the National Academy of Sciences 115, 21(2018), 5377\u20135382. https://doi.org/10.1073/pnas.1719573115 arXiv:https://www.pnas.org/content/115/21/5377.full.pdf",
      "doi": ""
    },
    {
      "text": "Richard Li, Jason Wu, and Thad Starner. 2019. TongueBoard: An Oral Interface for Subtle Input. In Proceedings of the 10th Augmented Human International Conference 2019. 1\u20139.",
      "doi": "10.1145/3311823.3311831"
    },
    {
      "text": "Kent Lyons, Thad Starner, Daniel Plaisted, James Fusia, Amanda Lyons, Aaron Drew, and EW Looney. 2004. Twiddler typing: one-handed chording text entry for mobile phones. In Proceedings of the SIGCHI conference on Human factors in computing systems. 671\u2013678.",
      "doi": "10.1145/985692.985777"
    },
    {
      "text": "I.\u00a0Scott MacKenzie and R.\u00a0William Soukoreff. 2003. Phrase Sets for Evaluating Text Entry Techniques. In CHI \u201903 Extended Abstracts on Human Factors in Computing Systems (Ft. Lauderdale, Florida, USA) (CHI EA \u201903). Association for Computing Machinery, New York, NY, USA, 754\u2013755. https://doi.org/10.1145/765891.765971",
      "doi": "10.1145/765891.765971"
    },
    {
      "text": "I\u00a0Scott MacKenzie and Kumiko Tanaka-Ishii. 2010. Text entry systems: Mobility, accessibility, universality. Elsevier.",
      "doi": "10.5555/1296062"
    },
    {
      "text": "L. Maier-Hein, F. Metze, T. Schultz, and A. Waibel. 2005. Session independent non-audible speech recognition using surface electromyography. In IEEE Workshop on Automatic Speech Recognition and Understanding, 2005.331\u2013336. https://doi.org/10.1109/ASRU.2005.1566521",
      "doi": ""
    },
    {
      "text": "P\u00e4ivi Majaranta, Ulla-Kaija Ahola, and Oleg \u0160pakov. 2009. Fast gaze typing with an adjustable dwell time. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 357\u2013360.",
      "doi": "10.1145/1518701.1518758"
    },
    {
      "text": "Geoffrey\u00a0S Meltzner, James\u00a0T Heaton, Yunbin Deng, Gianluca De\u00a0Luca, Serge\u00a0H Roy, and Joshua\u00a0C Kline. 2018. Development of sEMG sensors and algorithms for silent speech recognition. Journal of neural engineering(2018).",
      "doi": ""
    },
    {
      "text": "Carlos\u00a0H Morimoto and Arnon Amir. 2010. Context switching for fast key selection in text entry applications. In Proceedings of the 2010 symposium on eye-tracking research & applications. 271\u2013274.",
      "doi": "10.1145/1743666.1743730"
    },
    {
      "text": "Y Nakajima, Hideki Kashioka, Kiyohiro Shikano, and Nick Campbell. 2003. Non-audible murmur recognition input interface using stethoscopic microphone attached to the skin. ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings 5, V \u2013 708. https://doi.org/10.1109/ICASSP.2003.1200069",
      "doi": ""
    },
    {
      "text": "Shuo Niu, Li Liu, and D\u00a0Scott McCrickard. 2014. Tongue-able interfaces: Evaluating techniques for a camera based tongue gesture input system. In Proceedings of the 16th international ACM SIGACCESS conference on Computers & accessibility. 277\u2013278.",
      "doi": "10.1145/2661334.2661395"
    },
    {
      "text": "Lotte NS\u00a0Andreasen\u00a0Struijk, Eugen\u00a0R Lontis, Michael Gaihede, Hector\u00a0A Caltenco, Morten\u00a0Enemark Lund, Henrik Schioeler, and Bo Bentsen. 2017. Development and functional demonstration of a wireless intraoral inductive tongue computer interface for severely disabled persons. Disability and Rehabilitation: Assistive Technology 12, 6(2017), 631\u2013640.",
      "doi": ""
    },
    {
      "text": "Kseniia Palin, Anna\u00a0Maria Feit, Sunjun Kim, Per\u00a0Ola Kristensson, and Antti Oulasvirta. 2019. How do people type on mobile devices? Observations from a study with 37,000 volunteers. In Proceedings of the 21st International Conference on Human-Computer Interaction with Mobile Devices and Services. 1\u201312.",
      "doi": "10.1145/3338286.3340120"
    },
    {
      "text": "Laxmi Pandey and Ahmed\u00a0Sabbir Arif. 2021. LipType: A Silent Speech Recognizer Augmented with an Independent Repair Model. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 1\u201319.",
      "doi": "10.1145/3411764.3445565"
    },
    {
      "text": "Daniel\u00a0S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin\u00a0D Cubuk, and Quoc\u00a0V Le. 2019. Specaugment: A simple data augmentation method for automatic speech recognition. arXiv preprint arXiv:1904.08779(2019).",
      "doi": ""
    },
    {
      "text": "J. Picone. 1990. Continuous speech recognition using hidden Markov models. IEEE ASSP Magazine 7, 3 (1990), 26\u201341.",
      "doi": ""
    },
    {
      "text": "F. Robineau, F. Boy, J. Orliaguet, J. Demongeot, and Y. Payan. 2007. Guiding the Surgical Gesture Using an Electro-Tactile Stimulus Array on the Tongue: A Feasibility Study. IEEE Transactions on Biomedical Engineering 54, 4 (2007), 711\u2013717. https://doi.org/10.1109/TBME.2006.889180",
      "doi": ""
    },
    {
      "text": "Anne Roudaut, Andreas Rau, Christoph Sterz, Max Plauth, Pedro Lopes, and Patrick Baudisch. 2013. Gesture Output: Eyes-Free Output Using a Force Feedback Touch Surface. Association for Computing Machinery, New York, NY, USA, 2547\u20132556. https://doi.org/10.1145/2470654.2481352",
      "doi": "10.1145/2470654.2481352"
    },
    {
      "text": "Sherry Ruan, Jacob\u00a0O. Wobbrock, Kenny Liou, Andrew Ng, and James\u00a0A. Landay. 2018. Comparing Speech and Keyboard Text Entry for Short Messages in Two Languages on Touchscreen Phones. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 1, 4, Article 159 (Jan. 2018), 23\u00a0pages. https://doi.org/10.1145/3161187",
      "doi": "10.1145/3161187"
    },
    {
      "text": "Sherry Ruan, Jacob\u00a0O Wobbrock, Kenny Liou, Andrew Ng, and James\u00a0A Landay. 2018. Comparing speech and keyboard text entry for short messages in two languages on touchscreen phones. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 1, 4 (2018), 1\u201323.",
      "doi": "10.1145/3161187"
    },
    {
      "text": "Hasim Sak, Andrew\u00a0W. Senior, and Fran\u00e7oise Beaufays. 2014. Long short-term memory recurrent neural network architectures for large scale acoustic modeling. In INTERSPEECH 2014, 15th Annual Conference of the International Speech Communication Association, Singapore, September 14-18, 2014. ISCA, 338\u2013342.",
      "doi": ""
    },
    {
      "text": "Javier San\u00a0Agustin, Henrik Skovsgaard, Emilie Mollenbach, Maria Barret, Martin Tall, Dan\u00a0Witzner Hansen, and John\u00a0Paulin Hansen. 2010. Evaluation of a low-cost open-source gaze tracker. In Proceedings of the 2010 Symposium on Eye-Tracking Research & Applications. 77\u201380.",
      "doi": "10.1145/1743666.1743685"
    },
    {
      "text": "Tanja Schultz. 2010. ICCHP Keynote: Recognizing Silent and Weak Speech Based on Electromyography. 595\u2013604. https://doi.org/10.1007/978-3-642-14097-6_96",
      "doi": ""
    },
    {
      "text": "R. Soukoreff and I. MacKenzie. 2003. Metrics for text entry research: An evaluation of MSD and KSPC, and a new unified error metric. Conference on Human Factors in Computing Systems - Proceedings, 113\u2013120. https://doi.org/10.1145/642611.642632",
      "doi": ""
    },
    {
      "text": "Simon Stone and Peter Birkholz. 2020. Cross-Speaker Silent-Speech Command Word Recognition Using Electro-Optical Stomatography. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 7849\u20137853. https://doi.org/10.1109/ICASSP40776.2020.9053447",
      "doi": ""
    },
    {
      "text": "Ke Sun, Chun Yu, Weinan Shi, Lan Liu, and Yuanchun Shi. 2018. Lip-Interact: Improving Mobile Device Interaction with Silent Speech Commands. In Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology(Berlin, Germany) (UIST \u201918). ACM, New York, NY, USA, 581\u2013593. https://doi.org/10.1145/3242587.3242599",
      "doi": "10.1145/3242587.3242599"
    },
    {
      "text": "Ke Sun, Chun Yu, Weinan Shi, Lan Liu, and Yuanchun Shi. 2018. Lip-Interact: Improving Mobile Device Interaction with Silent Speech Commands. In Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology(Berlin, Germany) (UIST \u201918). ACM, New York, NY, USA, 581\u2013593. https://doi.org/10.1145/3242587.3242599",
      "doi": "10.1145/3242587.3242599"
    },
    {
      "text": "L\u00e1szl\u00f3 T\u00f3th, G\u00e1bor Gosztolya, Tam\u00e1s Gr\u00f3sz, Alexandra Mark\u00f3, and Tam\u00e1s\u00a0G\u00e1bor Csap\u00f3. 2018. Multi-Task Learning of Speech Recognition and Speech Synthesis Parameters for Ultrasound-based Silent Speech Interfaces.. In INTERSPEECH. 3172\u20133176.",
      "doi": ""
    },
    {
      "text": "Jason Tu, Angeline\u00a0Vidhula Jeyachandra, Deepthi Nagesh, Naresh Prabhu, and Thad Starner. 2021. Typing on Tap: Estimating a Finger-Worn One-Handed Chording Keyboard\u2019s Text Entry Rate. In 2021 International Symposium on Wearable Computers. 156\u2013158.",
      "doi": "10.1145/3460421.3480428"
    },
    {
      "text": "Outi Tuisku, P\u00e4ivi Majaranta, Poika Isokoski, and Kari-Jouko R\u00e4ih\u00e4. 2008. Now Dasher! Dash away! Longitudinal study of fast text entry by eye gaze. In Proceedings of the 2008 symposium on Eye tracking research & applications. 19\u201326.",
      "doi": "10.1145/1344471.1344476"
    },
    {
      "text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan\u00a0N. Gomez, undefinedukasz Kaiser, and Illia Polosukhin. 2017. Attention is All You Need. In Proceedings of the 31st International Conference on Neural Information Processing Systems (Long Beach, California, USA) (NIPS\u201917). Curran Associates Inc., Red Hook, NY, USA, 6000\u20136010.",
      "doi": "10.5555/3295222.3295349"
    },
    {
      "text": "Jingxian Wang, Chengfeng Pan, Haojian Jin, Vaibhav Singh, Yash Jain, Jason\u00a0I Hong, Carmel Majidi, and Swarun Kumar. 2019. RFID Tattoo: A Wireless Platform for Speech Recognition. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 3, 4 (2019), 1\u201324.",
      "doi": "10.1145/3359194"
    },
    {
      "text": "Tracy Westeyn, Helene Brashear, Amin Atrash, and Thad Starner. 2003. Georgia Tech Gesture Toolkit: Supporting Experiments in Gesture Recognition. In Proceedings of the 5th International Conference on Multimodal Interfaces (Vancouver, British Columbia, Canada) (ICMI \u201903). Association for Computing Machinery, New York, NY, USA, 85\u201392. https://doi.org/10.1145/958432.958452",
      "doi": "10.1145/958432.958452"
    },
    {
      "text": "Jacob\u00a0O Wobbrock. 2006. EdgeWrite: A versatile design for text entry and control.",
      "doi": ""
    },
    {
      "text": "Jacob\u00a0O Wobbrock. 2007. Measures of text entry performance. Text entry systems: Mobility, accessibility, universality (2007), 47\u201374.",
      "doi": ""
    },
    {
      "text": "Jacob\u00a0O Wobbrock. 2019. Situationally-induced impairments and disabilities. In Web Accessibility. Springer, 59\u201392.",
      "doi": ""
    },
    {
      "text": "Jacob\u00a0O Wobbrock, Shaun\u00a0K Kane, Krzysztof\u00a0Z Gajos, Susumu Harada, and Jon Froehlich. 2011. Ability-based design: Concept, principles and examples. ACM Transactions on Accessible Computing (TACCESS) 3, 3 (2011), 1\u201327.",
      "doi": "10.1145/1952383.1952384"
    },
    {
      "text": "Jacob\u00a0O Wobbrock, Brad\u00a0A Myers, and John\u00a0A Kembel. 2003. EdgeWrite: a stylus-based text entry method designed for high accuracy and stability of motion. In Proceedings of the 16th annual ACM symposium on User interface software and technology. 61\u201370.",
      "doi": "10.1145/964696.964703"
    },
    {
      "text": "Jacob\u00a0O Wobbrock, James Rubinstein, Michael\u00a0W Sawyer, and Andrew\u00a0T Duchowski. 2008. Longitudinal evaluation of discrete consecutive gaze gestures for text entry. In Proceedings of the 2008 symposium on Eye tracking research & applications. 11\u201318.",
      "doi": "10.1145/1344471.1344475"
    },
    {
      "text": "Pei Yin, Thad Starner, Harley Hamilton, Irfan Essa, and James\u00a0M Rehg. 2009. Learning the basic units in american sign language using discriminative segmental feature selection. In 2009 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 4757\u20134760.",
      "doi": "10.1109/ICASSP.2009.4960694"
    },
    {
      "text": "Steve Young, G Evermann, M.J.F. Gales, Thomas Hain, D Kershaw, Xunying Liu, G Moore, James Odell, D Ollason, Daniel Povey, V Valtchev, and Philip Woodland. 2002. The HTK book.",
      "doi": ""
    },
    {
      "text": "Shumin Zhai and Per\u00a0Ola Kristensson. 2012. The word-gesture keyboard: reimagining keyboard interaction. Commun. ACM 55, 9 (2012), 91\u2013101.",
      "doi": "10.1145/2330667.2330689"
    },
    {
      "text": "Mingrui\u00a0Ray Zhang and Jacob\u00a0O. Wobbrock. 2019. Beyond the Input Stream: Making Text Entry Evaluations More Flexible with Transcription Sequences. In Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology (New Orleans, LA, USA) (UIST \u201919). Association for Computing Machinery, New York, NY, USA, 831\u2013842. https://doi.org/10.1145/3332165.3347922",
      "doi": "10.1145/3332165.3347922"
    },
    {
      "text": "Qian Zhang, Dong Wang, Run Zhao, and Yinggang Yu. 2021. SoundLip: Enabling Word and Sentence-level Lip Interaction for Smart Devices. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 5, 1 (2021), 1\u201328.",
      "doi": "10.1145/3494987"
    }
  ]
}