{
  "doi": "10.1145/3491102.3517650",
  "title": "Improving understandability of feature contributions in model-agnostic explainable AI tools",
  "published": "2022-04-29",
  "proctitle": "CHI '22: Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems",
  "pages": "1-9",
  "year": 2022,
  "badges": [],
  "abstract": "Model-agnostic explainable AI tools explain their predictions by means of \u2019local\u2019 feature contributions. We empirically investigate two potential improvements over current approaches. The first one is to always present feature contributions in terms of the contribution to the outcome that is perceived as positive by the user (\u201cpositive framing\u201d). The second one is to add \u201csemantic labeling\u201d, that explains the directionality of each feature contribution (\u201cthis feature leads to +5% eligibility\u201d), reducing additional cognitive processing steps. In a user study, participants evaluated the understandability of explanations for different framing and labeling conditions for loan applications and music recommendations. We found that positive framing improves understandability even when the prediction is negative. Additionally, adding semantic labels eliminates any framing effects on understandability, with positive labels outperforming negative labels. We implemented our suggestions in a package ArgueView[11].",
  "authors": [
    {
      "name": "Sophia Hadash",
      "institution": "Jheronimus Academy of Data Science, Netherlands and Human-Technology Interaction, Eindhoven University of Technology, Netherlands",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99660239573",
      "orcid": "missing"
    },
    {
      "name": "Martijn C. Willemsen",
      "institution": "Jheronimus Academy of Data Science, Netherlands and Human-Technology Interaction, Eindhoven University of Technology, Netherlands",
      "img": "/do/10.1145/contrib-81444608032/rel-imgonly/martijn2011.jpg",
      "acmid": "81444608032",
      "orcid": "missing"
    },
    {
      "name": "Chris Snijders",
      "institution": "Human Technology Interaction, Eindhoven University of Technology, Netherlands",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81100560517",
      "orcid": "missing"
    },
    {
      "name": "Wijnand A. IJsselsteijn",
      "institution": "Human Technology Interaction, Eindhoven University of Technology, Netherlands",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81100032008",
      "orcid": "0000-0001-6856-9269"
    }
  ],
  "references": [
    {
      "text": "Namita Agarwal and Saikat Das. 2020. Interpretable Machine Learning Tools: A Survey. 2020 IEEE Symposium Series on Computational Intelligence, SSCI 2020 (2020), 1528\u20131534. https://doi.org/10.1109/SSCI47803.2020.9308260",
      "doi": ""
    },
    {
      "text": "H Ahrens. 1988. Stigler, Stephen M.: The History of Statistics. The Measurement of Uncertainty before 1900. The Belknap Press of Harvard University, Cambridge, Mass., & London 1986; XVI, 410 S. Biometrical Journal 30, 5 (1988), 631\u2013632. https://doi.org/10.1002/bimj.4710300527",
      "doi": ""
    },
    {
      "text": "Vaishak Belle and Ioannis Papantonis. 2020. Principles and practice of explainable machine learning. arXiv (2020). arxiv:2009.11698",
      "doi": ""
    },
    {
      "text": "Adrian Brasoveanu, Megan Moodie, and Rakshit Agrawal. 2020. Textual evidence for the perfunctoriness of independent medical reviews. CEUR Workshop Proceedings 2657 (2020), 1\u20139. https://doi.org/10.1145/nnnnnnn.nnnnnnn",
      "doi": ""
    },
    {
      "text": "Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie Elhadad. 2015. Intelligible Models for HealthCare: Predicting Pneumonia Risk and Hospital 30-day Readmission. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM New York, NY, USA, Sydney, NSW, Australia, 1721\u20131730. https://doi.org/10.1145/2783258.2788613",
      "doi": "10.1145/2783258.2788613"
    },
    {
      "text": "Roberto Confalonieri, Tillman Weyde, Tarek\u00a0R. Besold, and Ferm\u00edn Moscoso del Prado Mart\u00edn. 2021. Using ontologies to enhance human understandability of global post-hoc explanations of black-box models. Artificial Intelligence 296 (2021), 103471. https://doi.org/10.1016/j.artint.2021.103471",
      "doi": ""
    },
    {
      "text": "Jan Forberg, Annett Mitschick, Martin Voigt, and Raimund Dachselt. 2019. Interactive Exploration of Large Decision Tree Ensembles. (2019), 0\u20135. https://doi.org/10.1145/1122445.1122456",
      "doi": ""
    },
    {
      "text": "John Fox. 2017. Cognitive systems at the point of care : The CREDO program. Journal of Biomedical Informatics 68 (2017), 83\u201395. https://doi.org/10.1016/j.jbi.2017.02.008",
      "doi": "10.1016/j.jbi.2017.02.008"
    },
    {
      "text": "Daniel\u00a0T Gilbert, Douglas\u00a0S Krull, and Patrick\u00a0S Malone. 1990. Unbelieving the unbelievable: Some problems in the rejection of false information.Journal of personality and social psychology 59, 4(1990), 601.",
      "doi": ""
    },
    {
      "text": "B. Goodman and Seth Flaxman. 2017. European Union regulations on algorithmic decision-making and a \u2018\u2018right to explanation\u201d. AI Magazine 38, 3 (2017), 50\u201357. https://doi.org/10.1609/aimag.v38i3.2741 arxiv:1606.08813",
      "doi": ""
    },
    {
      "text": "[11] Sophia Hadash.2021. https://pypi.org/project/argueview/",
      "doi": ""
    },
    {
      "text": "Satoshi Hara and Kohei Hayashi. 2016. Making Tree Ensembles Interpretable. arxiv:1606.05390\u00a0[stat.ML]",
      "doi": ""
    },
    {
      "text": "Andrew\u00a0F Hayes, Carroll\u00a0J Glynn, and Michael\u00a0E Huge. 2012. Cautions Regarding the Interpretation of Regression Coefficients and Hypothesis Tests in Linear Models with Interactions. Communication Methods and Measures 6, 1 (2012), 1\u201311. https://doi.org/10.1080/19312458.2012.651415",
      "doi": ""
    },
    {
      "text": "Eliot Hearst. 1991. Psychology and Nothing. American Scientist 79, 5 (1991), 432\u2013443. http://www.jstor.org/stable/29774477",
      "doi": ""
    },
    {
      "text": "Peter\u00a0K Koo, Antonio Majdandzic, Matthew Ploenzke, Praveen Anand, and Steffan\u00a0B Paul. 2021. Global importance analysis: An interpretability method to quantify importance of genomic features in deep neural networks. PLOS Computational Biology 17, 5 (2021), 1\u201321. https://doi.org/10.1371/journal.pcbi.1008925",
      "doi": ""
    },
    {
      "text": "Joshua\u00a0A Kroll, Joanna Huey, Solon Barocas, Edward\u00a0W Felten, Joel\u00a0R Reidenberg, David\u00a0G Robinson, and Harlan Yu. 2017. Accountable Algorithms. Pennsylvania law review 165, 3 (2017), 633\u2013705. https://scholarship.law.upenn.edu/penn_law_review/vol165/iss3/3",
      "doi": ""
    },
    {
      "text": "Xuan Liu, Xiaoguang Wang, and Stan Matwin. 2018. Improving the Interpretability of Deep Neural Networks with Knowledge Distillation. In 2018 IEEE International Conference on Data Mining Workshops (ICDMW). 905\u2013912. https://doi.org/10.1109/ICDMW.2018.00132",
      "doi": ""
    },
    {
      "text": "Scott\u00a0M Lundberg and Su-in Lee. 2017. A Unified Approach to Interpreting Model Predictions. In Advances in Neural Information Processing Systems 30, I.\u00a0Guyon, U.V. Luxburg, S.\u00a0Bengio, H.\u00a0Wallach, R.\u00a0Fergus, S.\u00a0Vishwanathan, and R.\u00a0Garnett. (Eds.). Curran Associates, Inc., Long Beach, CA, USA, 4765\u20134774. https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions",
      "doi": ""
    },
    {
      "text": "M.W. Matlin. 2008. Cognition. Wiley. https://books.google.nl/books?id=BUEdAQAAMAAJ",
      "doi": ""
    },
    {
      "text": "Rory Mitchell, Eibe Frank, and Geoffrey Holmes. 2021. GPUTreeShap: Massively Parallel Exact Calculation of SHAP Scores for Tree Ensembles. arxiv:2010.13972\u00a0[cs.LG]",
      "doi": ""
    },
    {
      "text": "Christoph Molnar. 2018. iml: An R package for Interpretable Machine Learning. Journal of Open Source Software 3, 26 (2018), 786. https://doi.org/10.21105/joss.00786",
      "doi": ""
    },
    {
      "text": "Christoph Molnar, Giuseppe Casalicchio, and Bernd Bischl. 2020. Interpretable Machine Learning \u2013 A Brief History, State-of-the-Art and Challenges. Communications in Computer and Information Science 1323, 01(2020), 417\u2013431. https://doi.org/10.1007/978-3-030-65965-3_28 arxiv:2010.09337",
      "doi": ""
    },
    {
      "text": "Harsha Nori, Samuel Jenkins, Paul Koch, and Rich Caruana. 2019. InterpretML: A Unified Framework for Machine Learning Interpretability. (2019), 1\u20138. arxiv:1909.09223http://arxiv.org/abs/1909.09223",
      "doi": ""
    },
    {
      "text": "Marco\u00a0Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. Model-Agnostic Interpretability of Machine Learning. In ICML Workshop on Human Interpretability in Machine Learning (WHI 2016), Been Kim, Dmitry\u00a0M. Malioutov, and Kush\u00a0R. Varshney (Eds.). ArXiv, New York, NY, USA, 91\u201395. arxiv:1606.05386http://arxiv.org/abs/1606.05386",
      "doi": ""
    },
    {
      "text": "Marco\u00a0Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. \u201dWhy Should I Trust You?\u201d: Explaining the Predictions of Any Classifier. (2016). https://doi.org/10.18653/v1/N16-3020 arxiv:1602.04938",
      "doi": ""
    },
    {
      "text": "Marco\u00a0Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2018. Anchors : High-Precision Model-Agnostic Explanations. (2018).",
      "doi": ""
    },
    {
      "text": "Jaspreet Singh and Avishek Anand. 2020. Model agnostic interpretability of rankers via intent modelling. FAT* 2020 - Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency(2020), 618\u2013628. https://doi.org/10.1145/3351095.3375234",
      "doi": ""
    },
    {
      "text": "Sarah Tan, Matvey Soloviev, Giles Hooker, and Martin\u00a0T. Wells. 2020. Tree Space Prototypes: Another Look at Making Tree Ensembles Interpretable. In Proceedings of the 2020 ACM-IMS on Foundations of Data Science Conference (Virtual Event, USA) (FODS \u201920). Association for Computing Machinery, New York, NY, USA, 23\u201334. https://doi.org/10.1145/3412815.3416893",
      "doi": "10.1145/3412815.3416893"
    },
    {
      "text": "Amos Tversky and Daniel Kahneman. 1974. Judgment under uncertainty: Heuristics and biases. science 185, 4157 (1974), 1124\u20131131.",
      "doi": ""
    },
    {
      "text": "Adrian Weller. 2017. Challenges for Transparency. (2017). https://doi.org/10.1063/1.523063 arxiv:1708.01870",
      "doi": ""
    },
    {
      "text": "Brett Williams, Andrys Onsman, and Ted Brown. 1996. Exploratory factor analysis: A five-step guide for novices. Journal of Emergency Primary Health Care 19, May (1996), 42\u201350. https://doi.org/10.1080/09585190701763982 arxiv:1512.00567",
      "doi": ""
    },
    {
      "text": "Jiaming Zeng, Berk Ustun, and Cynthia Rudin. 2017. Interpretable classification models for recidivism prediction. Journal of the Royal Statistical Society. Series A: Statistics in Society 180, 3(2017), 689\u2013722. https://doi.org/10.1111/rssa.12227 arxiv:1503.07810",
      "doi": ""
    },
    {
      "text": "Xinyang Zhang, Ren Pang, Shouling Ji, Fenglong Ma, and Ting Wang. 2021. i-Algebra: Towards Interactive Interpretability of Deep Neural Networks. arxiv:2101.09301\u00a0[cs.LG]",
      "doi": ""
    }
  ]
}