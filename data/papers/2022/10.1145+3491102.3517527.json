{
  "doi": "10.1145/3491102.3517527",
  "title": "\u201cLook! It\u2019s a Computer Program! It\u2019s an Algorithm! It\u2019s AI!\u201d: Does Terminology Affect Human Perceptions and Evaluations of Algorithmic Decision-Making Systems?",
  "published": "2022-04-29",
  "proctitle": "CHI '22: Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems",
  "pages": "1-28",
  "year": 2022,
  "badges": [],
  "abstract": "In the media, in policy-making, but also in research articles, algorithmic decision-making (ADM) systems are referred to as algorithms, artificial intelligence, and computer programs, amongst other terms. We hypothesize that such terminological differences can affect people\u2019s perceptions of properties of ADM systems, people\u2019s evaluations of systems in application contexts, and the replicability of research as findings may be influenced by terminological differences. In two studies (N = 397, N = 622), we show that terminology does indeed affect laypeople\u2019s perceptions of system properties (e.g., perceived complexity) and evaluations of systems (e.g., trust). Our findings highlight the need to be mindful when choosing terms to describe ADM systems, because terminology can have unintended consequences, and may impact the robustness and replicability of HCI research. Additionally, our findings indicate that terminology can be used strategically (e.g., in communication about ADM systems) to influence people\u2019s perceptions and evaluations of these systems.",
  "tags": [
    "replicability",
    "research methodology",
    "terminology",
    "human-centered AI"
  ],
  "authors": [
    {
      "name": "Markus Langer",
      "institution": "Universit\u00e4t des Saarlandes, Germany",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659883612",
      "orcid": "missing"
    },
    {
      "name": "Tim Hunsicker",
      "institution": "Universit\u00e4t des Saarlandes, Germany",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99660234606",
      "orcid": "missing"
    },
    {
      "name": "Tina Feldkamp",
      "institution": "Universit\u00e4t des Saarlandes, Germany",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99660235149",
      "orcid": "missing"
    },
    {
      "name": "Cornelius J. K\u00f6nig",
      "institution": "Universit\u00e4t des Saarlandes, Germany",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "86159091057",
      "orcid": "missing"
    },
    {
      "name": "Nina Grgi\u0107-Hla\u010da",
      "institution": "Max Planck Institute for Software Systems, Germany and Max Planck Institute for Research on Collective Goods, Germany",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659260207",
      "orcid": "0000-0003-3397-2984"
    }
  ],
  "references": [
    {
      "text": "Herman Aguinis and Kyle\u00a0J Bradley. 2014. Best practice recommendations for designing and implementing experimental vignette methodology studies. Organizational Research Methods 17, 4 (2014), 351\u2013371. https://doi.org/10.1177/1094428114547952",
      "doi": ""
    },
    {
      "text": "Kirk Allen, Teri Reed-Rhoads, Robert\u00a0A Terry, Teri\u00a0J Murphy, and Andrea\u00a0D Stone. 2008. Coefficient alpha: An engineer\u2019s interpretation of test reliability. Journal of Engineering Education 97, 1 (2008), 87\u201394. https://doi.org/10.1002/j.2168-9830.2008.tb00956.x",
      "doi": ""
    },
    {
      "text": "Theo Araujo. 2018. Living up to the chatbot hype: The influence of anthropomorphic design cues and communicative agency framing on conversational agent and company perceptions. Computers in Human Behavior 85 (2018), 183\u2013189. https://doi.org/10.1016/j.chb.2018.03.051",
      "doi": ""
    },
    {
      "text": "Christiane Atzm\u00fcller and Peter\u00a0M Steiner. 2010. Experimental vignette studies in survey research. Methodology 6(2010), 128\u2013138. https://doi.org/10.1027/1614-2241/a000014",
      "doi": ""
    },
    {
      "text": "Christoph Bartneck, Dana Kuli\u0107, Elizabeth Croft, and Susana Zoghbi. 2009. Measurement instruments for the anthropomorphism, animacy, likeability, perceived intelligence, and perceived safety of robots. International Journal of Social Robotics 1, 1 (2009), 71\u201381. https://doi.org/10.1007/s12369-008-0001-3",
      "doi": "10.1007/s12369-008-0001-3"
    },
    {
      "text": "Reuben Binns, Max Van\u00a0Kleek, Michael Veale, Ulrik Lyngs, Jun Zhao, and Nigel Shadbolt. 2018. \u2019It\u2019s reducing a human being to a percentage\u2019 perceptions of justice in algorithmic decisions. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems. ACM, 1\u201314. https://doi.org/10.31235/osf.io/9wqxr",
      "doi": "10.1145/3173574.3173951"
    },
    {
      "text": "Samuel Bowman, Gabor Angeli, Christopher Potts, and Christopher\u00a0D Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. 632\u2013642. https://doi.org/10.18653/v1/d15-1075",
      "doi": ""
    },
    {
      "text": "Noah Castelo, Maarten\u00a0W Bos, and Donald\u00a0R Lehmann. 2019. Task-dependent algorithm aversion. Journal of Marketing Research 56, 5 (2019), 809\u2013825. https://doi.org/10.1177/0022243719851788",
      "doi": ""
    },
    {
      "text": "Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. 2017. Semantic textual similarity-multilingual and cross-lingual focused evaluation. Proceedings of the 2017 SEMVAL International Workshop on Semantic Evaluation (2017). https://doi.org/10.18653/v1/s17-2001",
      "doi": ""
    },
    {
      "text": "Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni\u00a0St John, Noah Constant, Mario Guajardo-C\u00e9spedes, Steve Yuan, Chris Tar, 2018. Universal sentence encoder. arXiv:1803.11175 (2018).",
      "doi": ""
    },
    {
      "text": "Jason\u00a0A Colquitt. 2001. On the dimensionality of organizational justice: A construct validation of a measure. Journal of Applied Psychology 86, 3 (2001), 386\u2013400. https://doi.org/10.1037/0021-9010.86.3.386",
      "doi": ""
    },
    {
      "text": "Kimberly\u00a0E Culley and Poornima Madhavan. 2013. A note of caution regarding anthropomorphism in HCI agents. Computers in Human Behavior 29, 3 (2013), 577\u2013579. https://doi.org/10.1016/j.chb.2012.11.023",
      "doi": "10.1016/j.chb.2012.11.023"
    },
    {
      "text": "Oren Danieli, Andrew Hillis, and Michael Luca. 2016. How to hire with algorithms. Retrieved July, 17, 2021 from https://hbr.org/2016/10/how-to-hire-with-algorithms",
      "doi": ""
    },
    {
      "text": "Maartje\u00a0Ma De\u00a0Graaf, Somaya\u00a0Ben Allouch, and Tineke Klamer. 2015. Sharing a life with Harvey: Exploring the acceptance of and relationship-building with a social robot. Computers in Human Behavior 43 (2015), 1\u201314. https://doi.org/10.1016/j.chb.2014.10.030",
      "doi": "10.1016/j.chb.2014.10.030"
    },
    {
      "text": "Ewart\u00a0J De\u00a0Visser, Samuel\u00a0S Monfort, Ryan McKendrick, Melissa\u00a0AB Smith, Patrick\u00a0E McKnight, Frank Krueger, and Raja Parasuraman. 2016. Almost human: Anthropomorphism increases trust resilience in cognitive agents. Journal of Experimental Psychology: Applied 22, 3 (2016), 331\u2013349. https://doi.org/10.1037/xap0000092",
      "doi": ""
    },
    {
      "text": "Berkeley\u00a0J Dietvorst and Soaham Bharti. 2020. People reject algorithms in uncertain decision domains because they have diminishing sensitivity to forecasting error. Psychological Science 31, 10 (2020), 1302\u20131314. https://doi.org/10.1177/0956797620948841",
      "doi": ""
    },
    {
      "text": "Berkeley\u00a0J Dietvorst, Joseph\u00a0P Simmons, and Cade Massey. 2015. Algorithm aversion: People erroneously avoid algorithms after seeing them err. Journal of Experimental Psychology: General 144, 1 (2015), 114\u2013126. https://doi.org/10.2139/ssrn.2466040",
      "doi": ""
    },
    {
      "text": "Rob Eisinga, Manfred Te\u00a0Grotenhuis, and Ben Pelzer. 2013. The reliability of a two-item scale: Pearson, Cronbach, or Spearman-Brown?International Journal of Public Health 58, 4 (2013), 637\u2013642. https://doi.org/10.1007/s00038-012-0416-3",
      "doi": ""
    },
    {
      "text": "Melissa\u00a0V Eitzel, Jessica\u00a0L Cappadonna, Chris Santos-Lang, Ruth\u00a0Ellen Duerr, Arika Virapongse, Sarah\u00a0Elizabeth West, Christopher Kyba, Anne Bowser, Caren\u00a0Beth Cooper, Andrea Sforzi, 2017. Citizen science terminology matters: Exploring key terms. Citizen Science: Theory and Practice 2, 1 (2017), 1\u201320. https://doi.org/10.5334/cstp.96",
      "doi": ""
    },
    {
      "text": "Kimberly\u00a0D Elsbach and Ileana Stigliani. 2019. New information technology and implicit bias. Academy of Management Perspectives 33, 2 (2019), 185\u2013206. https://doi.org/10.5465/amp.2017.0079",
      "doi": ""
    },
    {
      "text": "Nicholas Epley, Adam Waytz, and John\u00a0T Cacioppo. 2007. On seeing human: A three-factor theory of anthropomorphism. Psychological Review 114, 4 (2007), 864\u2013886. https://doi.org/10.1037/0033-295x.114.4.864",
      "doi": "10.1037/0033-"
    },
    {
      "text": "Doug Ertz. 2021. Eight in ten leaders want intelligent systems success in five years but the time to start blueprinting is now. Retrieved July, 17, 2021 from https://www.forbes.com/sites/windriver/2021/05/01/eight-in-ten-leaders-want-intelligent-systems-success-in-five-years-but-the-time-to-start-blueprinting-is-now/",
      "doi": ""
    },
    {
      "text": "Thomas Franke, Christiane Attig, and Daniel Wessel. 2019. A personal resource for technology interaction: Development and validation of the affinity for technology interaction (ATI) scale. International Journal of Human\u2013Computer Interaction 35, 6(2019), 456\u2013467. https://doi.org/10.1080/10447318.2018.1456150",
      "doi": ""
    },
    {
      "text": "Murray Gell-Mann. 2002. What is complexity?In Complexity and industrial clusters, Alberto\u00a0Quadrio Curzio and Marco Fortis (Eds.). Springer, Heidelberg, Germany, 13\u201324. https://doi.org/10.1007/978-3-642-50007-7_2",
      "doi": ""
    },
    {
      "text": "Ella Glikson and Anita\u00a0Williams Woolley. 2020. Human trust in artificial intelligence: Review of empirical research. Academy of Management Annals 14, 2 (2020), 627\u2013660. https://doi.org/10.5465/annals.2018.0057",
      "doi": ""
    },
    {
      "text": "Manuel\u00a0F Gonzalez, John\u00a0F Capman, Frederick\u00a0L Oswald, Evan\u00a0R Theys, and David\u00a0L Tomczak. 2019. \u201cWhere\u2019s the IO?\u201d Artificial intelligence and machine learning in talent management systems. Personnel Assessment and Decisions 5, 3 (2019), 5. https://doi.org/10.25035/pad.2019.03.005",
      "doi": ""
    },
    {
      "text": "Nina Grgi\u0107-Hla\u010da, Christoph Engel, and Krishna\u00a0P Gummadi. 2019. Human decision making with machine assistance: An experiment on bailing and jailing. Proceedings of the ACM on Human-Computer Interaction 3, CSCW(2019), 1\u201325. https://doi.org/10.2139/ssrn.3465622",
      "doi": ""
    },
    {
      "text": "Nina Grgi\u0107-Hla\u010da, Elissa\u00a0M Redmiles, Krishna\u00a0P Gummadi, and Adrian Weller. 2018. Human perceptions of fairness in algorithmic decision making: A case study of criminal risk prediction. In Proceedings of the 2018 WWW World Wide Web Conference. ACM, 903\u2013912. https://doi.org/10.1145/3178876.3186138",
      "doi": "10.1145/3178876.3186138"
    },
    {
      "text": "Kevin\u00a0Anthony Hoff and Masooda Bashir. 2015. Trust in automation: Integrating empirical evidence on factors that influence trust. Human Factors 57, 3 (2015), 407\u2013434. https://doi.org/10.1177/0018720814547570",
      "doi": ""
    },
    {
      "text": "Yoyo Tsung-Yu Hou and Malte\u00a0F Jung. 2021. Who is the expert? Reconciling algorithm aversion and algorithm appreciation in AI-supported decision making. Proceedings of the ACM on Human-Computer Interaction 5, CSCW2(2021), 1\u201325. https://doi.org/10.1145/3479864",
      "doi": ""
    },
    {
      "text": "Frederick\u00a0M Howard, Catherine\u00a0A Gao, and Christopher Sankey. 2020. Implementation of an automated scheduling tool improves schedule quality and resident satisfaction. PloS One 15, 8 (2020), e0236952. https://doi.org/10.1371/journal.pone.0236952",
      "doi": ""
    },
    {
      "text": "Alon Jacovi, Ana Marasovi\u0107, Tim Miller, and Yoav Goldberg. 2021. Formalizing trust in artificial intelligence: Prerequisites, causes and goals of human trust in ai. In Proceedings of the 2021 FAccT Conference on Fairness, Accountability, and Transparency. ACM, 624\u2013635. https://doi.org/10.1145/3442188.3445923",
      "doi": "10.1145/3442188.3445923"
    },
    {
      "text": "Stuart Keel, Pei\u00a0Ying Lee, Jane Scheetz, Zhixi Li, Mark\u00a0A Kotowicz, Richard\u00a0J MacIsaac, and Mingguang He. 2018. Feasibility and patient acceptability of a novel artificial intelligence-based screening model for diabetic retinopathy at endocrinology outpatient services: A pilot study. Scientific Reports 8, 1 (2018). https://doi.org/10.1038/s41598-018-22612-2",
      "doi": ""
    },
    {
      "text": "Markus Langer, Cornelius\u00a0J K\u00f6nig, and Victoria Hemsing. 2020. Is anybody listening? The impact of automatically evaluated job interviews on impression management and applicant reactions. Journal of Managerial Psychology 35, 4 (2020), 271\u2013284. https://doi.org/10.1108/jmp-03-2019-0156",
      "doi": ""
    },
    {
      "text": "Markus Langer, Cornelius\u00a0J K\u00f6nig, and Maria Papathanasiou. 2019. Highly automated job interviews: Acceptance under the influence of stakes. International Journal of Selection and Assessment 27, 3(2019), 217\u2013234. https://doi.org/10.1111/ijsa.12246",
      "doi": ""
    },
    {
      "text": "Markus Langer and Richard\u00a0N Landers. 2021. The future of artificial intelligence at work: A review on effects of decision automation and augmentation on workers targeted by algorithms and third-party observers. Computers in Human Behavior(2021), 106878. https://doi.org/10.1016/j.chb.2021.106878",
      "doi": ""
    },
    {
      "text": "John\u00a0D Lee and Katrina\u00a0A See. 2004. Trust in automation: Designing for appropriate reliance. Human Factors 46, 1 (2004), 50\u201380. https://doi.org/10.1518/hfes.46.1.50.30392",
      "doi": "10.1518/hfes.46.1.50_30392"
    },
    {
      "text": "Kwan\u00a0Min Lee, Younbo Jung, Jaywoo Kim, and Sang\u00a0Ryong Kim. 2006. Are physically embodied social agents better than disembodied social agents?: The effects of physical embodiment, tactile interaction, and people\u2019s loneliness in human\u2013robot interaction. International Journal of Human-Computer Studies 64, 10 (2006), 962\u2013973. https://doi.org/10.1016/j.ijhcs.2006.05.002",
      "doi": "10.1016/j.ijhcs.2006.05.002"
    },
    {
      "text": "Min\u00a0Kyung Lee. 2018. Understanding perception of algorithmic decisions: Fairness, trust, and emotion in response to algorithmic management. Big Data & Society 5, 1 (2018), 2053951718756684. https://doi.org/10.1177/2053951718756684",
      "doi": ""
    },
    {
      "text": "Min\u00a0Kyung Lee and Katherine Rich. 2021. Who Is Included in human perceptions of AI?: Trust and perceived fairness around healthcare AI and cultural mistrust. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. ACM, 1\u201314. https://doi.org/10.1145/3411764.3445570",
      "doi": "10.1145/3411764.3445570"
    },
    {
      "text": "Jamy Li. 2015. The benefit of being physically present: A survey of experimental works comparing copresent robots, telepresent robots and virtual agents. International Journal of Human-Computer Studies 77 (2015), 23\u201337. https://doi.org/10.1016/j.ijhcs.2015.01.001",
      "doi": "10.1016/j.ijhcs.2015.01.001"
    },
    {
      "text": "LiveCareer. 2018. 7 Things to Know about the role of robots in recruitment. Retrieved July, 17, 2021 from https://careerenlightenment.com/7-things-to-know-about-the-role-of-robots-in-recruitment",
      "doi": ""
    },
    {
      "text": "Chiara Longoni, Andrea Bonezzi, and Carey\u00a0K Morewedge. 2019. Resistance to medical artificial intelligence. Journal of Consumer Research 46, 4 (2019), 629\u2013650. https://doi.org/10.1093/jcr/ucz013",
      "doi": ""
    },
    {
      "text": "Niklas Luhmann. 2000. Familiarity, confidence, trust: Problems and alternatives. In Trust: Making and breaking cooperative relations, Diego Gambetta (Ed.). Department of Sociology, University of Oxford, Oxford, UK, 94\u2013107.",
      "doi": ""
    },
    {
      "text": "Frank Marcinkowski, Kimon Kieslich, Christopher Starke, and Marco L\u00fcnich. 2020. Implications of AI (un-) fairness in higher education admissions: The effects of perceived AI (un-) fairness on exit, voice and organizational reputation. In Proceedings of the 2020 FAT* Conference on Fairness, Accountability, and Transparency. ACM, 122\u2013130. https://doi.org/10.1145/3351095.3372867",
      "doi": "10.1145/3351095.3372867"
    },
    {
      "text": "Enid\u00a0NH Montague, Brian\u00a0M Kleiner, and Woodrow\u00a0W Winchester\u00a0III. 2009. Empirically understanding trust in medical technology. International Journal of Industrial Ergonomics 39, 4(2009), 628\u2013634. https://doi.org/10.1016/j.ergon.2009.01.004",
      "doi": ""
    },
    {
      "text": "Rosanna Nagtegaal. 2021. The impact of using algorithms for managerial decisions on public employees\u2019 procedural justice. Government Information Quarterly 38, 1 (2021), 101536. https://doi.org/10.1016/j.giq.2020.101536",
      "doi": ""
    },
    {
      "text": "David\u00a0T Newman, Nathanael\u00a0J Fast, and Derek\u00a0J Harmon. 2020. When eliminating bias isn\u2019t fair: Algorithmic reductionism and procedural justice in human resource decisions. Organizational Behavior and Human Decision Processes 160 (2020), 149\u2013167. https://doi.org/10.1016/j.obhdp.2020.03.008",
      "doi": ""
    },
    {
      "text": "Tatsuya Nomura, Tomohiro Suzuki, Takayuki Kanda, and Kensuke Kato. 2006. Altered attitudes of people toward robots: Investigation through the Negative Attitudes toward Robots Scale. In Proceedings of the 2006 AAAI workshop on Human Implications of Human-Robot Interaction. 29\u201335.",
      "doi": ""
    },
    {
      "text": "High-Level Expert\u00a0Group on Artificial\u00a0Intelligence. 2021. Ethics guidelines for trustworthy AI. Retrieved July, 17, 2021 from https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai",
      "doi": ""
    },
    {
      "text": "Sonja\u00a0K \u00d6tting and G\u00fcnter\u00a0W Maier. 2018. The importance of procedural justice in human\u2013machine interactions: Intelligent systems as new decision agents in organizations. Computers in Human Behavior 89 (2018), 27\u201339. https://doi.org/10.1016/j.chb.2018.07.022",
      "doi": "10.1016/j.chb.2018.07.022"
    },
    {
      "text": "Thomas O\u2019Neill, Nathan McNeese, Amy Barron, and Beau Schelble. 2020. Human\u2013Autonomy teaming: A review and analysis of the empirical literature. Human Factors (2020), 0018720820960865. https://doi.org/10.1177/0018720820960865",
      "doi": ""
    },
    {
      "text": "Robert\u00a0A Peterson. 1994. A meta-analysis of Cronbach\u2019s coefficient alpha. Journal of Consumer Research 21, 2 (1994), 381\u2013391.",
      "doi": ""
    },
    {
      "text": "R Puhl, JL Peterson, and J Luedicke. 2013. Motivating or stigmatizing? Public perceptions of weight-related language used by health providers. International Journal of Obesity 37, 4 (2013), 612\u2013619. https://doi.org/10.1038/ijo.2012.110",
      "doi": ""
    },
    {
      "text": "Rebecca\u00a0M Puhl. 2020. What words should we use to talk about weight? A systematic review of quantitative and qualitative studies examining preferences for weight-related terminology. Obesity Reviews 21, 6 (2020), e13008. https://doi.org/10.1111/obr.13008",
      "doi": ""
    },
    {
      "text": "Victoria\u00a0A Shaffer, C\u00a0Adam Probst, Edgar\u00a0C Merkle, Hal\u00a0R Arkes, and Mitchell\u00a0A Medow. 2013. Why do patients derogate physicians who use a computer-based diagnostic support system?Medical Decision Making 33, 1 (2013), 108\u2013118. https://doi.org/10.1177/0272989x12453501",
      "doi": ""
    },
    {
      "text": "Daniel\u00a0B Shank, Madison Bowen, Alexander Burns, and Matthew Dew. 2021. Humans are perceived as better, but weaker, than artificial intelligence: A comparison of affective impressions of humans, AIs, and computer systems in roles on teams. Computers in Human Behavior Reports 3 (2021), 100092. https://doi.org/10.1016/j.chbr.2021.100092",
      "doi": ""
    },
    {
      "text": "Daniel\u00a0B Shank, Alexander Burns, Sophia Rodriguez, and Madison Bowen. 2020. Software program, bot, or artificial intelligence? Affective sentiments across general technology labels.Current Research in Social Psychology(2020). https://crisp.org.uiowa.edu/sites/crisp.org.uiowa.edu/files/2020-06/crisp_28_4_shank.pdf",
      "doi": ""
    },
    {
      "text": "Daniel\u00a0B Shank and Alyssa DeSanti. 2018. Attributions of morality and mind to artificial intelligence after real-world moral violations. Computers in Human Behavior 86 (2018), 401\u2013411. https://doi.org/10.1016/j.chb.2018.05.014",
      "doi": ""
    },
    {
      "text": "Rania Shibl, Meredith Lawley, and Justin Debuse. 2013. Factors influencing decision support system acceptance. Decision Support Systems 54, 2 (2013), 953\u2013961. https://doi.org/10.1016/j.dss.2012.09.018",
      "doi": "10.1016/j.dss.2012.09.018"
    },
    {
      "text": "Stephen\u00a0M Smith and Richard\u00a0E Petty. 1996. Message framing and persuasion: A message processing analysis. Personality and Social Psychology Bulletin 22, 3 (1996), 257\u2013268. https://doi.org/10.1177/0146167296223004",
      "doi": ""
    },
    {
      "text": "Alarith Uhde, Nadine Schlicker, Dieter\u00a0P Wallach, and Marc Hassenzahl. 2020. Fairness and decision-making in collaborative shift scheduling systems. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. ACM, 1\u201313. https://doi.org/10.1145/3313831.3376656",
      "doi": "10.1145/3313831.3376656"
    },
    {
      "text": "Viswanath Venkatesh, Michael\u00a0G Morris, Gordon\u00a0B Davis, and Fred\u00a0D Davis. 2003. User acceptance of information technology: Toward a unified view. MIS Quarterly 27, 3 (2003), 425\u2013478. https://doi.org/10.2307/30036540",
      "doi": "10.5555/2017197.2017202"
    },
    {
      "text": "Katrien Verbert, Denis Parra, Peter Brusilovsky, and Erik Duval. 2013. Visualizing recommendations to support exploration, transparency and controllability. In Proceedings of the 2013 International Conference on Intelligent User Interfaces. ACM, 351\u2013362. https://doi.org/10.1145/2449396.2449442",
      "doi": "10.1145/2449396.2449442"
    },
    {
      "text": "James Vincent. 2019. Forty percent of \u2018AI startups\u2019 in Europe don\u2019t actually use AI, claims report. Retrieved July, 23, 2021 from https://www.theverge.com/2019/3/5/18251326/ai-startups-europe-fake-40-percent-mmc-report",
      "doi": ""
    },
    {
      "text": "Pauli Virtanen, Ralf Gommers, Travis\u00a0E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, St\u00e9fan\u00a0J. van der Walt, Matthew Brett, Joshua Wilson, K.\u00a0Jarrod Millman, Nikolay Mayorov, Andrew R.\u00a0J. Nelson, Eric Jones, Robert Kern, Eric Larson, C\u00a0J Carey, \u0130lhan Polat, Yu Feng, Eric\u00a0W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E.\u00a0A. Quintero, Charles\u00a0R. Harris, Anne\u00a0M. Archibald, Ant\u00f4nio\u00a0H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. 2020. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods 17(2020), 261\u2013272. https://doi.org/10.1038/s41592-019-0686-2",
      "doi": ""
    },
    {
      "text": "Ruotong Wang, F\u00a0Maxwell Harper, and Haiyi Zhu. 2020. Factors influencing perceived fairness in algorithmic decision-making: Algorithm outcomes, development procedures, and individual differences. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. ACM, 1\u201314. https://doi.org/10.1145/3313831.3376813",
      "doi": "10.1145/3313831.3376813"
    },
    {
      "text": "David Windley. 2021. Is AI the answer to recruiting effectiveness?Retrieved July, 23, 2021 from https://www.forbes.com/sites/forbeshumanresourcescouncil/2021/06/16/is-ai-the-answer-to-recruiting-effectiveness/",
      "doi": ""
    }
  ]
}