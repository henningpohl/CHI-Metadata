{
  "doi": "10.1145/3491102.3517474",
  "title": "How can Explainability Methods be Used to Support Bug Identification in Computer Vision Models?",
  "published": "2022-04-29",
  "proctitle": "CHI '22: Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems",
  "pages": "1-16",
  "year": 2022,
  "badges": [],
  "abstract": "Deep learning models for image classification suffer from dangerous issues often discovered after deployment. The process of identifying bugs that cause these issues remains limited and understudied. Especially, explainability methods are often presented as obvious tools for bug identification. Yet, the current practice lacks an understanding of what kind of explanations can best support the different steps of the bug identification process, and how practitioners could interact with those explanations. Through a formative study and an iterative co-creation process, we build an interactive design probe providing various potentially relevant explainability functionalities, integrated into interfaces that allow for flexible workflows. Using the probe, we perform 18 user-studies with a diverse set of machine learning practitioners. Two-thirds of the practitioners engage in successful bug identification. They use multiple types of explanations, e.g. visual and textual ones, through non-standardized sequences of interactions including queries and exploration. Our results highlight the need for interactive, guiding, interfaces with diverse explanations, shedding light on future research directions.",
  "authors": [
    {
      "name": "Agathe Balayn",
      "institution": "Software Technology, Delft University of Technology, Netherlands",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659730652",
      "orcid": "0000-0003-2725-5305"
    },
    {
      "name": "Natasa Rikalo",
      "institution": "Delft University of Technology, Netherlands",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99660239516",
      "orcid": "0000-0001-8261-4652"
    },
    {
      "name": "Christoph Lofi",
      "institution": "Delft University of Technology, Netherlands",
      "img": "/do/10.1145/contrib-81458657843/rel-imgonly/lofi408.jpg",
      "acmid": "81458657843",
      "orcid": "missing"
    },
    {
      "name": "Jie Yang",
      "institution": "Delft University of Technology, Netherlands",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659594294",
      "orcid": "0000-0002-0350-0313"
    },
    {
      "name": "Alessandro Bozzon",
      "institution": "Web Information Systems, Delft University of Technology, Netherlands",
      "img": "/do/10.1145/contrib-81314480825/rel-imgonly/me_medium.jpg",
      "acmid": "81314480825",
      "orcid": "0000-0002-3300-2913"
    }
  ],
  "references": [
    {
      "text": "Naveed Akhtar and Ajmal Mian. 2018. Threat of adversarial attacks on deep learning in computer vision: A survey. Ieee Access 6(2018), 14410\u201314430.",
      "doi": ""
    },
    {
      "text": "Ahmed Alqaraawi 2020. Evaluating saliency map explanations for convolutional neural networks: a user study. In IUI. 275\u2013285.",
      "doi": ""
    },
    {
      "text": "Saleema Amershi, Max Chickering, Steven\u00a0M Drucker, Bongshin Lee, Patrice Simard, and Jina Suh. 2015. Modeltracker: Redesigning performance analysis tools for machine learning. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems. 337\u2013346.",
      "doi": "10.1145/2702123.2702509"
    },
    {
      "text": "Ariful\u00a0Islam Anik and Andrea Bunt. 2021. Data-Centric Explanations: Explaining Training Data of Machine Learning Systems to Promote Transparency. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 1\u201313.",
      "doi": "10.1145/3411764.3445736"
    },
    {
      "text": "Keijiro Araki, Zengo Furukawa, and Jingde Cheng. 1991. A general framework for debugging. IEEE software 8, 3 (1991), 14\u201320.",
      "doi": "10.1109/52.88939"
    },
    {
      "text": "Vijay Arya, Rachel\u00a0KE Bellamy, 2019. One explanation does not fit all: A toolkit and taxonomy of ai explainability techniques. (2019).",
      "doi": ""
    },
    {
      "text": "Joshua Attenberg, Panos Ipeirotis, and Foster Provost. 2015. Beat the machine: Challenging humans to find a predictive model\u2019s \u201cunknown unknowns\u201d. Journal of Data and Information Quality (JDIQ) 6, 1 (2015), 1\u201317.",
      "doi": "10.1145/2700832"
    },
    {
      "text": "Agathe Balayn, Panagiotis Soilis, Christoph Lofi, Jie Yang, and Alessandro Bozzon. 2021. What do You Mean? Interpreting Image Classification with Crowdsourced Concept Extraction and Analysis. In Proceedings of the Web Conference 2021. 1937\u20131948.",
      "doi": "10.1145/3442381.3450069"
    },
    {
      "text": "Gagan Bansal. 2018. Explanatory Dialogs: Towards Actionable, Interactive Explanations. In AIES \u201918. 356\u2013357.",
      "doi": "10.1145/3278721.3278795"
    },
    {
      "text": "Umang Bhatt, Alice Xiang, 2020. Explainable machine learning in deployment. In FAT*. 648\u2013657.",
      "doi": ""
    },
    {
      "text": "Shanqing Cai, Eric Breck, Eric Nielsen, M Salib, and D Sculley. 2016. Tensorflow debugger: Debugging dataflow graphs for machine learning. (2016).",
      "doi": ""
    },
    {
      "text": "Hao-Fei Cheng 2019. Explaining decision-making algorithms through UI: Strategies to help non-expert stakeholders. In CHI. 1\u201312.",
      "doi": ""
    },
    {
      "text": "Michael Chromik 2021. I Think I Get Your Point, AI! The Illusion of Explanatory Depth in Explainable AI. In IUI. 307\u2013317.",
      "doi": ""
    },
    {
      "text": "A Ghorbani and al. 2019. Towards automatic concept-based explanations. In NeurIPS.",
      "doi": ""
    },
    {
      "text": "Yash Goyal, Ziyan Wu, Jan Ernst, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019. Counterfactual visual explanations. In ICML. PMLR, 2376\u20132384.",
      "doi": ""
    },
    {
      "text": "Brent Hailpern and Padmanabhan Santhanam. 2002. Software debugging, testing, and verification. IBM Systems Journal 41, 1 (2002), 4\u201312.",
      "doi": "10.1147/sj.411.0004"
    },
    {
      "text": "Lisa\u00a0Anne Hendricks, Kaylee Burns, 2018. Women also snowboard: Overcoming bias in captioning models. In ECCV. 771\u2013787.",
      "doi": ""
    },
    {
      "text": "Dan Hendrycks, Steven Basart, 2021. The many faces of robustness: A critical analysis of out-of-distribution generalization. In ICCV. 8340\u20138349.",
      "doi": ""
    },
    {
      "text": "Dan Hendrycks and Thomas Dietterich. 2018. Benchmarking Neural Network Robustness to Common Corruptions and Perturbations. In ICLR.",
      "doi": ""
    },
    {
      "text": "Fred Hohman 2019. Gamut: A design probe to understand how data scientists understand machine learning models. In CHI. 1\u201313.",
      "doi": ""
    },
    {
      "text": "Fred Hohman, Haekyu Park, Caleb Robinson, and Duen Horng\u00a0Polo Chau. 2019. Summit: Scaling deep learning interpretability by visualizing activation and attribution summarizations. IEEE transactions on visualization and computer graphics 26, 1(2019), 1096\u20131106.",
      "doi": "10.1109/TVCG.2019.2934659"
    },
    {
      "text": "Hilary Hutchinson, Wendy Mackay, 2003. Technology probes: inspiring design for and with families. In SIGCHI. 17\u201324.",
      "doi": ""
    },
    {
      "text": "S\u00e9rgio Jesus 2021. How can I choose an explainer? An Application-grounded Evaluation of Post-hoc Explanations. In FAccT. 805\u2013815.",
      "doi": ""
    },
    {
      "text": "Daniel Kang, Deepti Raghavan, Peter Bailis, and Matei Zaharia. 2018. Model assertions for debugging machine learning. In NeurIPS MLSys Workshop.",
      "doi": ""
    },
    {
      "text": "Harmanpreet Kaur 2020. Interpreting Interpretability: Understanding Data Scientists\u2019 Use of Interpretability Tools for Machine Learning. In CHI. 1\u201314.",
      "doi": ""
    },
    {
      "text": "Christopher\u00a0J Kelly, Alan Karthikesalingam, Mustafa Suleyman, Greg Corrado, and Dominic King. 2019. Key challenges for delivering clinical impact with artificial intelligence. BMC medicine 17, 1 (2019), 1\u20139.",
      "doi": ""
    },
    {
      "text": "B Kim, M Wattenberg, and al. 2018. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors. In ICML.",
      "doi": ""
    },
    {
      "text": "Lucas Layman, Madeline Diep, 2013. Debugging revisited: Toward understanding the debugging needs of contemporary software developers. In 2013 ACM/IEEE international symposium on empirical software engineering and measurement. IEEE, 383\u2013392.",
      "doi": ""
    },
    {
      "text": "Yi Li and Nuno Vasconcelos. 2019. Repair: Removing representation bias by dataset resampling. In ICCV. 9572\u20139581.",
      "doi": ""
    },
    {
      "text": "Q\u00a0Vera Liao 2020. Questioning the AI: informing design practices for explainable AI user experiences. In CHI. 1\u201315.",
      "doi": ""
    },
    {
      "text": "Brian\u00a0Y Lim 2019. Why these Explanations? Selecting Intelligibility Types for Explanation Goals.. In IUI Workshops.",
      "doi": ""
    },
    {
      "text": "Anthony Liu, Santiago Guerra, 2020. Towards hybrid human-ai workflows for unknown unknown detection. In WWW. 2432\u20132442.",
      "doi": ""
    },
    {
      "text": "Raoni Louren\u00e7o, Juliana Freire, and Dennis Shasha. 2019. Debugging machine learning pipelines. In Proceedings of the 3rd International Workshop on Data Management for End-to-End Machine Learning. 1\u201310.",
      "doi": "10.1145/3329486.3329489"
    },
    {
      "text": "Shiqing Ma, Yingqi Liu, Wen-Chuan Lee, Xiangyu Zhang, and Ananth Grama. 2018. MODE: automated neural network model debugging via state differential analysis and input selection. In 2018 ACM Joint Meeting on European Software Engineering Conference. 175\u2013186.",
      "doi": "10.1145/3236024.3236082"
    },
    {
      "text": "Shweta Narkar 2021. Model LineUpper: Supporting Interactive Model Comparison at Multiple Levels for AutoML. In IUI. 170\u2013174.",
      "doi": ""
    },
    {
      "text": "Chris Olah, Alexander Mordvintsev, and Ludwig Schubert. 2017. Feature visualization. Distill 2, 11 (2017), e7.",
      "doi": ""
    },
    {
      "text": "Heather\u00a0L O\u2019Brien, Paul Cairns, and Mark Hall. 2018. A practical approach to measuring user engagement with the refined user engagement scale (UES) and new UES short form. International Journal of Human-Computer Studies 112 (2018), 28\u201339.",
      "doi": ""
    },
    {
      "text": "Nathalie Rauschmayr, Vikas Kumar, Rahul Huilgol, Andrea Olgiati, Satadal Bhattacharjee, 2021. Amazon SageMaker Debugger: A System for Real-Time Insights into Machine Learning Model Training. Proceedings of Machine Learning and Systems 3 (2021).",
      "doi": ""
    },
    {
      "text": "Donghao Ren, Saleema Amershi, Bongshin Lee, Jina Suh, and Jason\u00a0D Williams. 2016. Squares: Supporting interactive performance analysis for multiclass classifiers. IEEE transactions on visualization and computer graphics 23, 1(2016), 61\u201370.",
      "doi": "10.1109/TVCG.2016.2598828"
    },
    {
      "text": "Marco\u00a0Tulio Ribeiro 2016. Why should i trust you? Explaining the predictions of any classifier. In SIGKDD. 1135\u20131144.",
      "doi": ""
    },
    {
      "text": "Frank Schneider, Felix Dangel, and Philipp Hennig. 2021. Cockpit: A Practical Debugging Tool for Training Deep Neural Networks. (2021).",
      "doi": ""
    },
    {
      "text": "E Schoop, F Huang, and B Hartmann. 2021. UMLAUT: Debugging Deep Learning Programs using Program Structure and Model Behavior. (2021).",
      "doi": ""
    },
    {
      "text": "K Simonyan, A Vedaldi, and A Zisserman. 2014. Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps. In ICLR.",
      "doi": ""
    },
    {
      "text": "Sahil Singla, Besmira Nushi, S Shah, E Kamar, and E Horvitz. 2020. Understanding Failures of Deep Networks via Robust Feature Extraction. (2020).",
      "doi": ""
    },
    {
      "text": "D Smilkov and al. 2017. SmoothGrad: removing noise by adding noise. (2017).",
      "doi": ""
    },
    {
      "text": "Kacper Sokol and Peter Flach. 2020. Explainability fact sheets: a framework for systematic assessment of explainable approaches. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. 56\u201367.",
      "doi": "10.1145/3351095.3372870"
    },
    {
      "text": "Harini Suresh, Steven\u00a0R Gomez, K\u00a0K Nam, and A Satyanarayan. 2021. Beyond Expertise and Roles: A Framework to Characterize the Stakeholders of Interpretable Machine Learning and their Needs. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 1\u201316.",
      "doi": "10.1145/3411764.3445088"
    },
    {
      "text": "Tatiana Tommasi, Novi Patricia, Barbara Caputo, and Tinne Tuytelaars. 2017. A deeper look at dataset bias. In Domain adaptation in computer vision applications. Springer, 37\u201355.",
      "doi": ""
    },
    {
      "text": "Antonio Torralba and Alexei\u00a0A Efros. 2011. Unbiased look at dataset bias. In CVPR 2011. IEEE, 1521\u20131528.",
      "doi": "10.1109/CVPR.2011.5995347"
    },
    {
      "text": "Anneliese von Mayrhauser and A\u00a0Marie Vans. 1997. Program understanding behavior during debugging of large scale software. In Papers presented at the seventh workshop on Empirical studies of programmers. 157\u2013179.",
      "doi": ""
    },
    {
      "text": "James Wexler, Mahima Pushkarna, Tolga Bolukbasi, Martin Wattenberg, Fernanda Vi\u00e9gas, and Jimbo Wilson. 2019. The what-if tool: Interactive probing of machine learning models. IEEE transactions on visualization and computer graphics 26, 1(2019), 56\u201365.",
      "doi": ""
    },
    {
      "text": "Yao Xie 2020. CheXplain: Enabling Physicians to Explore and Understand Data-Driven, AI-Enabled Medical Imaging Analysis. In CHI. 1\u201313.",
      "doi": ""
    },
    {
      "text": "Jiawei Zhang, Yang Wang, Piero Molino, Lezhi Li, and David\u00a0S Ebert. 2018. Manifold: A model-agnostic framework for interpretation and diagnosis of machine learning models. IEEE transactions on visualization and computer graphics 25, 1(2018), 364\u2013373.",
      "doi": ""
    },
    {
      "text": "J\u00a0M Zhang, M Harman, and al. 2020. Machine learning testing: Survey, landscapes and horizons. IEEE Transactions on Software Engineering(2020).",
      "doi": ""
    },
    {
      "text": "Peng Zhao, Yu-Jie Zhang, and Zhi-Hua Zhou. 2021. Exploratory machine learning with unknown unknowns. In AAAI, Vol.\u00a035. 10999\u201311006.",
      "doi": ""
    }
  ]
}