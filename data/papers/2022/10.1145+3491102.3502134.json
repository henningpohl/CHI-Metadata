{
  "doi": "10.1145/3491102.3502134",
  "title": "Integrating Gaze and Speech for Enabling Implicit Interactions",
  "published": "2022-04-29",
  "proctitle": "CHI '22: Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems",
  "pages": "1-14",
  "year": 2022,
  "badges": [],
  "abstract": "Gaze and speech are rich contextual sources of information that, when combined, can result in effective and rich multimodal interactions. This paper proposes a machine learning-based pipeline that leverages and combines users\u2019 natural gaze activity, the semantic knowledge from their vocal utterances and the synchronicity between gaze and speech data to facilitate users\u2019 interaction. We evaluated our proposed approach on an existing dataset, which involved 32 participants recording voice notes while reading an academic paper. Using a Logistic Regression classifier, we demonstrate that our proposed multimodal approach maps voice notes with accurate text passages with an average F1-Score of 0.90. Our proposed pipeline motivates the design of multimodal interfaces that combines natural gaze and speech patterns to enable robust interactions.",
  "tags": [
    "voice interfaces",
    "implicit annotation",
    "natural language processing",
    "semantic similarity",
    "natural gaze"
  ],
  "authors": [
    {
      "name": "Anam Ahmad Khan",
      "institution": "The University of Melbourne, Australia",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659463081",
      "orcid": "missing"
    },
    {
      "name": "Joshua Newn",
      "institution": "Computing and Information Systems, The University of Melbourne, Australia",
      "img": "/do/10.1145/contrib-99658770761/rel-imgonly/joshua-newn.jpg",
      "acmid": "99658770761",
      "orcid": "0000-0001-5769-6297"
    },
    {
      "name": "James Bailey",
      "institution": "Department of Computing and Information Systems, The University of Melbourne, Australia",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81100616144",
      "orcid": "missing"
    },
    {
      "name": "Eduardo Velloso",
      "institution": "School of Computing and Information Systems, University of Melbourne, Australia",
      "img": "/do/10.1145/contrib-81488669445/rel-imgonly/profilepic.jpg",
      "acmid": "81488669445",
      "orcid": "0000-0003-4414-2249"
    }
  ],
  "references": [
    {
      "text": "2021. Hypothesis. https://web.hypothes.is/",
      "doi": ""
    },
    {
      "text": "2021. reflect-in-seesaw. https://chrome.google.com/webstore/detail/reflect-in-seesaw-extensi/lhgiigkiddoalobhmmcpdhddlccindjj",
      "doi": ""
    },
    {
      "text": "Sharifa Alghowinem, Roland Goecke, Michael Wagner, Julien Epps, Matthew Hyett, Gordon Parker, and Michael Breakspear. 2016. Multimodal depression detection: fusion analysis of paralinguistic, head pose and eye gaze behaviors. IEEE Transactions on Affective Computing 9, 4 (2016), 478\u2013490.",
      "doi": "10.1109/TAFFC.2016.2634527"
    },
    {
      "text": "Thomas Bader, Matthias Vogelgesang, and Edmund Klaus. 2009. Multimodal Integration of Natural Gaze Behavior for Intention Recognition during Object Manipulation. In Proceedings of the 2009 International Conference on Multimodal Interfaces (Cambridge, Massachusetts, USA) (ICMI-MLMI \u201909). Association for Computing Machinery, New York, NY, USA, 199\u2013206. https://doi.org/10.1145/1647314.1647350",
      "doi": "10.1145/1647314.1647350"
    },
    {
      "text": "Abhijit Balaji, Thuvaarakkesh Ramanathan, and Venkateshwarlu Sonathi. 2018. Chart-text: A fully automated chart image descriptor. arXiv preprint arXiv:1812.10636(2018).",
      "doi": ""
    },
    {
      "text": "TR Beelders and PJ Blignaut. 2011. The usability of speech and eye gaze as a multimodal interface for a word processor. Speech Technologies (2011), 386\u2013404.",
      "doi": ""
    },
    {
      "text": "Richard\u00a0A. Bolt. 1980. \u201cPut-That-There\u201d: Voice and Gesture at the Graphics Interface. In Proceedings of the 7th Annual Conference on Computer Graphics and Interactive Techniques (Seattle, Washington, USA) (SIGGRAPH \u201980). Association for Computing Machinery, New York, NY, USA, 262\u2013270. https://doi.org/10.1145/800250.807503",
      "doi": "10.1145/800250.807503"
    },
    {
      "text": "Matteo Casarini, Marco Porta, and Piercarlo Dondi. 2020. A Gaze-Based Web Browser with Multiple Methods for Link Selection. In ACM Symposium on Eye Tracking Research and Applications (Stuttgart, Germany) (ETRA \u201920 Adjunct). Association for Computing Machinery, New York, NY, USA, Article 17, 8\u00a0pages. https://doi.org/10.1145/3379157.3388929",
      "doi": "10.1145/3379157.3388929"
    },
    {
      "text": "Asli Celikyilmaz, Zhaleh Feizollahi, Dilek Hakkani-Tur, and Ruhi Sarikaya. 2014. Resolving referring expressions in conversational dialogs for natural user interfaces. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2094\u20132104.",
      "doi": ""
    },
    {
      "text": "Siyuan Chen, Julien Epps, and Fang Chen. 2013. Automatic and Continuous User Task Analysis via Eye Activity. In Proceedings of the 2013 International Conference on Intelligent User Interfaces (Santa Monica, California, USA) (IUI \u201913). Association for Computing Machinery, New York, NY, USA, 57\u201366. https://doi.org/10.1145/2449396.2449406",
      "doi": "10.1145/2449396.2449406"
    },
    {
      "text": "Andrew\u00a0M Dai, Christopher Olah, and Quoc\u00a0V Le. 2015. Document embedding with paragraph vectors. arXiv preprint arXiv:1507.07998(2015).",
      "doi": ""
    },
    {
      "text": "Xin Fu, Tom Ciszek, Gary Marchionini, and Paul Solomon. 2005. Annotating the Web: An exploratory study of Web users\u2019 needs for personal annotation tools. Proceedings of the American Society for Information Science and Technology 42, 1 (2005).",
      "doi": ""
    },
    {
      "text": "Qiong Gu, Li Zhu, and Zhihua Cai. 2009. Evaluation measures of the classification performance of imbalanced data sets. In International symposium on intelligence computation and applications. Springer, 461\u2013471.",
      "doi": ""
    },
    {
      "text": "Andrea Guazzini, Eiko Yoneki, and Giorgio Gronchi. 2015. Cognitive dissonance and social influence effects on preference judgments: An eye tracking based system for their automatic assessment. International Journal of Human-Computer Studies 73 (2015), 12\u201318. https://doi.org/10.1016/j.ijhcs.2014.08.003",
      "doi": ""
    },
    {
      "text": "Dilek Hakkani-T\u00fcr, Malcolm Slaney, Asli Celikyilmaz, and Larry Heck. 2014. Eye gaze for spoken language understanding in multi-modal conversational interactions. In Proceedings of the 16th International Conference on Multimodal Interaction. 263\u2013266.",
      "doi": "10.1145/2663204.2663277"
    },
    {
      "text": "Kristiina Jokinen, Hirohisa Furukawa, Masafumi Nishida, and Seiichi Yamamoto. 2013. Gaze and Turn-Taking Behavior in Casual Conversational Interactions. ACM Trans. Interact. Intell. Syst. 3, 2, Article 12 (Aug. 2013), 30\u00a0pages. https://doi.org/10.1145/2499474.2499481",
      "doi": "10.1145/2499474.2499481"
    },
    {
      "text": "Casey Kennington, Spyridon Kousidis, and David Schlangen. 2013. Interpreting situated dialogue utterances: an update model that uses speech, gaze, and gesture information. Proceedings of SigDial 2013(2013).",
      "doi": ""
    },
    {
      "text": "Anam\u00a0Ahmad Khan, Sadia Nawaz, Joshua Newn, Jason\u00a0M. Lodge, James Bailey, and Eduardo Velloso. 2020. Using voice note-taking to promote learners\u2019 conceptual understanding. arxiv:2012.02927",
      "doi": ""
    },
    {
      "text": "Anam\u00a0Ahmad Khan, Joshua Newn, Ryan\u00a0M. Kelly, Namrata Srivastava, James Bailey, and Eduardo Velloso. 2021. GAVIN: Gaze-Assisted Voice-Based Implicit Note-Taking. ACM Trans. Comput.-Hum. Interact. 28, 4, Article 26 (Aug. 2021), 32\u00a0pages. https://doi.org/10.1145/3453988",
      "doi": "10.1145/3453988"
    },
    {
      "text": "Yea-Seul Kim, Mira Dontcheva, Eytan Adar, and Jessica Hullman. 2019. Vocal Shortcuts for Creative Experts. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (Glasgow, Scotland Uk) (CHI \u201919). Association for Computing Machinery, New York, NY, USA, 1\u201314. https://doi.org/10.1145/3290605.3300562",
      "doi": "10.1145/3290605.3300562"
    },
    {
      "text": "Dimosthenis Kontogiorgos, Elena Sibirtseva, Andre Pereira, Gabriel Skantze, and Joakim Gustafson. 2018. Multimodal reference resolution in collaborative assembly tasks. In Proceedings of the 4th International Workshop on Multimodal Analyses Enabling Artificial Agents in Human-Machine Interaction. 38\u201342.",
      "doi": "10.1145/3279972.3279976"
    },
    {
      "text": "Alfirna\u00a0Rizqi Lahitani, Adhistya\u00a0Erna Permanasari, and Noor\u00a0Akhmad Setiawan. 2016. Cosine similarity to determine similarity measure: Study case in online essay assessment. In 2016 4th International Conference on Cyber and IT Service Management. IEEE, 1\u20136.",
      "doi": ""
    },
    {
      "text": "Quoc Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. In International conference on machine learning. PMLR, 1188\u20131196.",
      "doi": ""
    },
    {
      "text": "Toby Jia-Jun Li, Lindsay Popowski, Tom Mitchell, and Brad\u00a0A Myers. 2021. Screen2Vec: Semantic Embedding of GUI Screens and GUI Components. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 1\u201315.",
      "doi": ""
    },
    {
      "text": "Scott\u00a0M Lundberg and Su-In Lee. 2017. A unified approach to interpreting model predictions. In Advances in neural information processing systems. 4765\u20134774.",
      "doi": ""
    },
    {
      "text": "Chandra\u00a0Sekhar Mantravadi. 2009. Adaptive multimodal integration of speech and gaze. Ph.D. Dissertation. Rutgers University-Graduate School-New Brunswick. https://doi.org/10.7282/T3QC03PM",
      "doi": ""
    },
    {
      "text": "Sven Mayer, Gierad Laput, and Chris Harrison. 2020. Enhancing Mobile Voice Assistants with WorldGaze. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. 1\u201310.",
      "doi": "10.1145/3313831.3376479"
    },
    {
      "text": "Chelsea Myers, Anushay Furqan, Jessica Nebolsky, Karina Caro, and Jichen Zhu. 2018. Patterns for How Users Overcome Obstacles in Voice User Interfaces. Association for Computing Machinery, New York, NY, USA, 1\u20137. https://doi.org/10.1145/3173574.3173580",
      "doi": ""
    },
    {
      "text": "Cuong Nguyen and Feng Liu. 2016. Gaze-Based Notetaking for Learning from Lecture Videos. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (San Jose, California, USA) (CHI \u201916). Association for Computing Machinery, New York, NY, USA, 2093\u20132097.",
      "doi": "10.1145/2858036.2858137"
    },
    {
      "text": "Panupong Pasupat, Tian-Shun Jiang, Evan\u00a0Zheran Liu, Kelvin Guu, and Percy Liang. 2018. Mapping natural language commands to web elements. arXiv preprint arXiv:1808.09132(2018).",
      "doi": ""
    },
    {
      "text": "Annie Piolat, Thierry Olive, and Ronald\u00a0T Kellogg. 2005. Cognitive effort during note taking. Applied cognitive psychology 19, 3 (2005), 291\u2013312.",
      "doi": ""
    },
    {
      "text": "Martin Porcheron, Joel\u00a0E. Fischer, Stuart Reeves, and Sarah Sharples. 2018. Voice Interfaces in Everyday Life. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (Montreal QC, Canada) (CHI \u201918). Association for Computing Machinery, New York, NY, USA, 1\u201312. https://doi.org/10.1145/3173574.3174214",
      "doi": "10.1145/3173574.3174214"
    },
    {
      "text": "Zahar Prasov and Joyce\u00a0Y Chai. 2008. What\u2019s in a gaze? The role of eye-gaze in reference resolution in multimodal conversational interfaces. In Proceedings of the 13th international conference on Intelligent user interfaces. 20\u201329.",
      "doi": ""
    },
    {
      "text": "Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084(2019).",
      "doi": ""
    },
    {
      "text": "Dario Salvucci and Joseph Goldberg. 2000. Identifying fixations and saccades in eye-tracking protocols. Proceedings of the Eye Tracking Research and Applications Symposium, 71\u201378. https://doi.org/10.1145/355017.355028",
      "doi": "10.1145/355017.355028"
    },
    {
      "text": "Korok Sengupta, Min Ke, Raphael Menges, Chandan Kumar, and Steffen Staab. 2018. Hands-free web browsing: Enriching the user experience with gaze and voice modality. In Proceedings of the 2018 ACM Symposium on Eye Tracking Research & Applications. 1\u20133.",
      "doi": "10.1145/3204493.3208338"
    },
    {
      "text": "Hao Shen and Jaideep Sengupta. 2018. Word of mouth versus word of mouse: Speaking about a brand connects you to it more than writing does. Journal of Consumer Research 45, 3 (2018), 595\u2013614. https://doi.org/10.1093/jcr/ucy011",
      "doi": ""
    },
    {
      "text": "Malcolm Slaney, Andreas Stolcke, and Dilek Hakkani-T\u00fcr. 2014. The relation of eye gaze and face pose: Potential impact on speech recognition. In Proceedings of the 16th International Conference on Multimodal Interaction. 144\u2013147.",
      "doi": "10.1145/2663204.2663251"
    },
    {
      "text": "Cagkan Uludagli and Cengiz Acarturk. 2018. User interaction in hands-free gaming: A comparative study of gaze-voice and touchscreen interface control. Turkish Journal of Electrical Engineering and Computer Sciences 26 (07 2018). https://doi.org/10.3906/elk-1710-128",
      "doi": ""
    },
    {
      "text": "Jan Van\u00a0der Kamp and Veronica Sundstedt. 2011. Gaze and voice controlled drawing. In Proceedings of the 1st conference on novel gaze-controlled applications. 1\u20138.",
      "doi": "10.1145/1983302.1983311"
    },
    {
      "text": "Jan van\u00a0der Kamp and Veronica Sundstedt. 2011. Gaze and Voice Controlled Drawing. In Proceedings of the 1st Conference on Novel Gaze-Controlled Applications (Karlskrona, Sweden) (NGCA \u201911). Association for Computing Machinery, New York, NY, USA, Article 9, 8\u00a0pages.",
      "doi": "10.1145/1983302.1983311"
    },
    {
      "text": "Diogo Vieira, Jo\u00e3o\u00a0Dinis Freitas, Cengiz Acart\u00fcrk, Ant\u00f3nio Teixeira, Lu\u00eds Sousa, Samuel Silva, Sara Candeias, and Miguel\u00a0Sales Dias. 2015. \u201d Read That Article\u201d Exploring Synergies between Gaze and Speech Interaction. In Proceedings of the 17th International ACM SIGACCESS Conference on Computers & Accessibility. 341\u2013342.",
      "doi": "10.1145/2700648.2811369"
    },
    {
      "text": "Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2015. Show and tell: A neural image caption generator. In Proceedings of the IEEE conference on computer vision and pattern recognition. 3156\u20133164.",
      "doi": ""
    },
    {
      "text": "Bryan Wang, Gang Li, Xin Zhou, Zhourong Chen, Tovi Grossman, and Yang Li. 2021. Screen2Words: Automatic Mobile UI Summarization with Multimodal Learning. arXiv preprint arXiv:2108.03353(2021).",
      "doi": ""
    },
    {
      "text": "Tom Young, Devamanyu Hazarika, Soujanya Poria, and Erik Cambria. 2018. Recent trends in deep learning based natural language processing. ieee Computational intelligenCe magazine 13, 3 (2018), 55\u201375.",
      "doi": ""
    }
  ]
}