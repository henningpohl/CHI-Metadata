{
  "doi": "10.1145/3491102.3501826",
  "title": "Towards Relatable Explainable AI with the Perceptual Process",
  "published": "2022-04-29",
  "proctitle": "CHI '22: Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems",
  "pages": "1-24",
  "year": 2022,
  "badges": [],
  "abstract": "Machine learning models need to provide contrastive explanations, since people often seek to understand why a puzzling prediction occurred instead of some expected outcome. Current contrastive explanations are rudimentary comparisons between examples or raw features, which remain difficult to interpret, since they lack semantic meaning. We argue that explanations must be more relatable to other concepts, hypotheticals, and associations. Inspired by the perceptual process from cognitive psychology, we propose the XAI Perceptual Processing Framework and RexNet model for relatable explainable AI with Contrastive Saliency, Counterfactual Synthetic, and Contrastive Cues explanations. We investigated the application of vocal emotion recognition, and implemented a modular multi-task deep neural network to predict and explain emotions from speech. From think-aloud and controlled studies, we found that counterfactual explanations were useful and further enhanced with semantic cues, but not saliency explanations. This work provides insights into providing and evaluating relatable contrastive explainable AI for perception applications.",
  "authors": [
    {
      "name": "Wencan Zhang",
      "institution": "Department of Computer Science, National University of Singapore, Singapore",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99660236105",
      "orcid": "missing"
    },
    {
      "name": "Brian Y Lim",
      "institution": "Department of Computer Science, National University of Singapore, Singapore",
      "img": "/do/10.1145/contrib-81416601689/rel-imgonly/brian_lim_2015_-_square.jpg",
      "acmid": "81416601689",
      "orcid": "0000-0002-0543-2414"
    }
  ],
  "references": [
    {
      "text": "Ashraf Abdul, Jo Vermeulen, Danding Wang, Brian\u00a0Y Lim, and Mohan Kankanhalli. 2018. Trends and trajectories for explainable, accountable and intelligible systems: An hci research agenda. In Proceedings of the 2018 CHI conference on human factors in computing systems. 1\u201318.",
      "doi": "10.1145/3173574.3174156"
    },
    {
      "text": "Ashraf Abdul, Christian von\u00a0der Weth, Mohan Kankanhalli, and Brian\u00a0Y Lim. 2020. COGAM: Measuring and Moderating Cognitive Load in Machine Learning Model Explanations. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. 1\u201314.",
      "doi": "10.1145/3313831.3376615"
    },
    {
      "text": "Amina Adadi and Mohammed Berrada. 2018. Peeking inside the black-box: a survey on explainable artificial intelligence (XAI). IEEE access 6(2018), 52138\u201352160.",
      "doi": ""
    },
    {
      "text": "Purvi Agrawal and Sriram Ganapathy. 2020. Interpretable representation learning for speech and audio signals based on relevance weighting. IEEE/ACM Transactions on Audio, Speech, and Language Processing 28 (2020), 2823\u20132836.",
      "doi": "10.1109/TASLP.2020.3030489"
    },
    {
      "text": "Dario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Qiang Cheng, Guoliang Chen, 2016. Deep speech 2: End-to-end speech recognition in english and mandarin. In International conference on machine learning. PMLR, 173\u2013182.",
      "doi": ""
    },
    {
      "text": "Andrew Anderson, Jonathan Dodge, Amrita Sadarangani, Zoe Juozapaitis, Evan Newman, Jed Irvine, Souti Chattopadhyay, Matthew Olson, Alan Fern, and Margaret Burnett. 2020. Mental models of mere mortals with explanations of reinforcement learning. ACM Transactions on Interactive Intelligent Systems (TiiS) 10, 2(2020), 1\u201337.",
      "doi": "10.1145/3366485"
    },
    {
      "text": "Alejandro\u00a0Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del\u00a0Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garc\u00eda, Sergio Gil-L\u00f3pez, Daniel Molina, Richard Benjamins, 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion 58(2020), 82\u2013115.",
      "doi": "10.1016/j.inffus.2019.12.012"
    },
    {
      "text": "Sebastian Bach, Alexander Binder, Gr\u00e9goire Montavon, Frederick Klauschen, Klaus-Robert M\u00fcller, and Wojciech Samek. 2015. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS one 10, 7 (2015), e0130140.",
      "doi": ""
    },
    {
      "text": "Dror Ben-Zeev, Emily\u00a0A Scherer, Rui Wang, Haiyi Xie, and Andrew\u00a0T Campbell. 2015. Next-generation psychiatric assessment: Using smartphone sensors to monitor behavior and mental health.Psychiatric rehabilitation journal 38, 3 (2015), 218.",
      "doi": ""
    },
    {
      "text": "Ruth\u00a0MJ Byrne. 2007. The rational imagination: How people create alternatives to reality. MIT press.",
      "doi": ""
    },
    {
      "text": "Carrie\u00a0J Cai, Jonas Jongejan, and Jess Holbrook. 2019. The effects of example-based explanations in a machine learning interface. In Proceedings of the 24th international conference on intelligent user interfaces. 258\u2013262.",
      "doi": "10.1145/3301275.3302289"
    },
    {
      "text": "Carrie\u00a0J Cai, Emily Reif, Narayan Hegde, Jason Hipp, Been Kim, Daniel Smilkov, Martin Wattenberg, Fernanda Viegas, Greg\u00a0S Corrado, Martin\u00a0C Stumpe, 2019. Human-centered tools for coping with imperfect algorithms during medical decision-making. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. 1\u201314.",
      "doi": "10.1145/3290605.3300234"
    },
    {
      "text": "Edward\u00a0C. Carterette and Morton\u00a0P. Friedman (Eds.). 1978. Perceptual processing. Academic Press, New York.",
      "doi": ""
    },
    {
      "text": "Hao-Fei Cheng, Ruotong Wang, Zheng Zhang, Fiona O\u2019Connell, Terrance Gray, F\u00a0Maxwell Harper, and Haiyi Zhu. 2019. Explaining decision-making algorithms through UI: Strategies to help non-expert stakeholders. In Proceedings of the 2019 chi conference on human factors in computing systems. 1\u201312.",
      "doi": "10.1145/3290605.3300789"
    },
    {
      "text": "Jianlin Cheng, Zheng Wang, and Gianluca Pollastri. 2008. A neural network approach to ordinal regression. In 2008 IEEE international joint conference on neural networks (IEEE world congress on computational intelligence). IEEE, 1279\u20131284.",
      "doi": ""
    },
    {
      "text": "Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, and Jaegul Choo. 2018. Stargan: Unified generative adversarial networks for multi-domain image-to-image translation. In Proceedings of the IEEE conference on computer vision and pattern recognition. 8789\u20138797.",
      "doi": ""
    },
    {
      "text": "Robert\u00a0B Cialdini, Wilhelmina Wosinska, Daniel\u00a0W Barrett, Jonathan Butner, and Malgorzata Gornik-Durose. 1999. Compliance with a request in two cultures: The differential influence of social proof and commitment/consistency on collectivists and individualists. Personality and Social Psychology Bulletin 25, 10 (1999), 1242\u20131253.",
      "doi": ""
    },
    {
      "text": "Sven Coppers, Jan Van\u00a0den Bergh, Kris Luyten, Karin Coninx, Iulianna Van\u00a0der Lek-Ciudin, Tom Vanallemeersch, and Vincent Vandeghinste. 2018. Intellingo: an intelligible translation environment. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems. 1\u201313.",
      "doi": "10.1145/3173574.3174098"
    },
    {
      "text": "T\u00a0Edward Damer. 2012. Attacking faulty reasoning. Cengage Learning.",
      "doi": ""
    },
    {
      "text": "Amit Dhurandhar, Pin-Yu Chen, Ronny Luss, Chun-Chen Tu, Paishun Ting, Karthikeyan Shanmugam, and Payel Das. [n.d.]. Explanations based on the Missing: Towards Contrastive Explanations with Pertinent Negatives. Ann Arbor 1001([n. d.]), 48109.",
      "doi": ""
    },
    {
      "text": "Upol Ehsan, Brent Harrison, Larry Chan, and Mark\u00a0O Riedl. 2018. Rationalization: A neural machine translation approach to generating natural language explanations. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society. 81\u201387.",
      "doi": "10.1145/3278721.3278736"
    },
    {
      "text": "Upol Ehsan, Q\u00a0Vera Liao, Michael Muller, Mark\u00a0O Riedl, and Justin\u00a0D Weisz. 2021. Expanding explainability: Towards social transparency in ai systems. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 1\u201319.",
      "doi": "10.1145/3411764.3445188"
    },
    {
      "text": "Malin Eiband, Hanna Schneider, Mark Bilandzic, Julian Fazekas-Con, Mareike Haug, and Heinrich Hussmann. 2018. Bringing transparency design into practice. In 23rd international conference on intelligent user interfaces. 211\u2013223.",
      "doi": "10.1145/3172944.3172961"
    },
    {
      "text": "Dedre Gentner and Linsey Smith. 2012. Analogical reasoning. Encyclopedia of human behavior 2 (2012), 130\u2013136.",
      "doi": ""
    },
    {
      "text": "Amirata Ghorbani, James Wexler, James\u00a0Y Zou, and Been Kim. 2019. Towards Automatic Concept-based Explanations. Advances in Neural Information Processing Systems 32 (2019), 9277\u20139286.",
      "doi": ""
    },
    {
      "text": "Daniel\u00a0G Goldstein and David Rothschild. 2014. Lay understanding of probability distributions.Judgment & Decision Making 9, 1 (2014).",
      "doi": ""
    },
    {
      "text": "E\u00a0Bruce Goldstein. 2014. Cognitive psychology: Connecting mind, research and everyday experience. Cengage Learning.",
      "doi": ""
    },
    {
      "text": "Cristina Gorrostieta, Richard Brutti, Kye Taylor, Avi Shapiro, Joseph Moran, Ali Azarbayejani, and John Kane. 2018. Attention-based Sequence Classification for Affect Detection.. In Interspeech. 506\u2013510.",
      "doi": ""
    },
    {
      "text": "Yash Goyal, Ziyan Wu, Jan Ernst, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019. Counterfactual visual explanations. In International Conference on Machine Learning. PMLR, 2376\u20132384.",
      "doi": ""
    },
    {
      "text": "Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti, and Dino Pedreschi. 2018. A survey of methods for explaining black box models. ACM computing surveys (CSUR) 51, 5 (2018), 1\u201342.",
      "doi": ""
    },
    {
      "text": "David Gunning and David Aha. 2019. DARPA\u2019s explainable artificial intelligence (XAI) program. AI Magazine 40, 2 (2019), 44\u201358.",
      "doi": ""
    },
    {
      "text": "Zhenliang He, Wangmeng Zuo, Meina Kan, Shiguang Shan, and Xilin Chen. 2019. Attgan: Facial attribute editing by only changing what you want. IEEE transactions on image processing 28, 11 (2019), 5464\u20135478.",
      "doi": ""
    },
    {
      "text": "Lisa\u00a0Anne Hendricks, Ronghang Hu, Trevor Darrell, and Zeynep Akata. 2018. Generating Counterfactual Explanations with Natural Language. In ICML Workshop on Human Interpretability in Machine Learning. 95\u201398.",
      "doi": ""
    },
    {
      "text": "Shawn Hershey, Sourish Chaudhuri, Daniel\u00a0PW Ellis, Jort\u00a0F Gemmeke, Aren Jansen, R\u00a0Channing Moore, Manoj Plakal, Devin Platt, Rif\u00a0A Saurous, Bryan Seybold, 2017. CNN architectures for large-scale audio classification. In 2017 ieee international conference on acoustics, speech and signal processing (icassp). IEEE, 131\u2013135.",
      "doi": ""
    },
    {
      "text": "Denis\u00a0J Hilton. 1990. Conversational processes and causal explanation.Psychological Bulletin 107, 1 (1990), 65.",
      "doi": ""
    },
    {
      "text": "Fred Hohman, Minsuk Kahng, Robert Pienta, and Duen\u00a0Horng Chau. 2018. Visual analytics in deep learning: An interrogative survey for the next frontiers. IEEE transactions on visualization and computer graphics 25, 8(2018), 2674\u20132693.",
      "doi": "10.1109/TVCG.2018.2843369"
    },
    {
      "text": "Zhengwei Huang, Ming Dong, Qirong Mao, and Yongzhao Zhan. 2014. Speech emotion recognition using CNN. In Proceedings of the 22nd ACM international conference on Multimedia. 801\u2013804.",
      "doi": "10.1145/2647868.2654984"
    },
    {
      "text": "Neha Jain, Shishir Kumar, Amit Kumar, Pourya Shamsolmoali, and Masoumeh Zareapoor. 2018. Hybrid deep neural networks for face emotion recognition. Pattern Recognition Letters 115 (2018), 101\u2013106.",
      "doi": ""
    },
    {
      "text": "Patrik\u00a0N Juslin and Petri Laukka. 2001. Impact of intended emotion intensity on cue utilization and decoding accuracy in vocal expression of emotion.Emotion 1, 4 (2001), 381.",
      "doi": ""
    },
    {
      "text": "Daniel Kahneman. 2011. Thinking, fast and slow. Macmillan.",
      "doi": ""
    },
    {
      "text": "Hirokazu Kameoka, Takuhiro Kaneko, Kou Tanaka, and Nobukatsu Hojo. 2018. Stargan-vc: Non-parallel many-to-many voice conversion using star generative adversarial networks. In 2018 IEEE Spoken Language Technology Workshop (SLT). IEEE, 266\u2013273.",
      "doi": ""
    },
    {
      "text": "Hirokazu Kameoka, Kou Tanaka, Damian Kwa\u015bny, Takuhiro Kaneko, and Nobukatsu Hojo. 2020. ConvS2S-VC: Fully convolutional sequence-to-sequence voice conversion. IEEE/ACM Transactions on Audio, Speech, and Language Processing 28 (2020), 1849\u20131863.",
      "doi": "10.1109/TASLP.2020.3001456"
    },
    {
      "text": "Takuhiro Kaneko, Hirokazu Kameoka, Kou Tanaka, and Nobukatsu Hojo. 2019. StarGAN-VC2: Rethinking conditional methods for StarGAN-based voice conversion. arXiv preprint arXiv:1907.12279(2019).",
      "doi": ""
    },
    {
      "text": "Tero Karras, Samuli Laine, and Timo Aila. 2019. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 4401\u20134410.",
      "doi": ""
    },
    {
      "text": "Jacob Devlin Ming-Wei\u00a0Chang Kenton and Lee\u00a0Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of NAACL-HLT. 4171\u20134186.",
      "doi": ""
    },
    {
      "text": "Been Kim, Rajiv Khanna, and Oluwasanmi\u00a0O Koyejo. 2016. Examples are not enough, learn to criticize! criticism for interpretability. Advances in neural information processing systems 29 (2016).",
      "doi": ""
    },
    {
      "text": "Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, 2018. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). In International conference on machine learning. PMLR, 2668\u20132677.",
      "doi": ""
    },
    {
      "text": "Kurt Koffka. 2013. Principles of Gestalt psychology. Routledge.",
      "doi": ""
    },
    {
      "text": "Pang\u00a0Wei Koh and Percy Liang. 2017. Understanding black-box predictions via influence functions. In International Conference on Machine Learning. PMLR, 1885\u20131894.",
      "doi": ""
    },
    {
      "text": "Alex Krizhevsky, Ilya Sutskever, and Geoffrey\u00a0E Hinton. 2012. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems 25 (2012), 1097\u20131105.",
      "doi": ""
    },
    {
      "text": "Andreas Krug, Ren\u00e9 Knaebel, and Sebastian Stober. 2018. Neuron activation profiles for interpreting convolutional speech recognition models. In NeurIPS Workshop on Interpretability and Robustness in Audio, Speech, and Language (IRASL).",
      "doi": ""
    },
    {
      "text": "Andreas Krug and Sebastian Stober. 2018. Introspection for convolutional automatic speech recognition. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. 187\u2013199.",
      "doi": ""
    },
    {
      "text": "Adi Lausen and Kurt Hammerschmidt. 2020. Emotion recognition and confidence ratings predicted by vocal stimulus type and prosodic parameters. Humanities and Social Sciences Communications 7, 1(2020), 1\u201317.",
      "doi": ""
    },
    {
      "text": "Thai Le, Suhang Wang, and Dongwon Lee. 2020. GRACE: Generating Concise and Informative Contrastive Sample to Explain Neural Network Model\u2019s Prediction. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 238\u2013248.",
      "doi": "10.1145/3394486.3403066"
    },
    {
      "text": "Benjamin Letham, Cynthia Rudin, Tyler\u00a0H McCormick, and David Madigan. 2015. Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model. The Annals of Applied Statistics 9, 3 (2015), 1350\u20131371.",
      "doi": ""
    },
    {
      "text": "Chung-Yi Li, Pei-Chieh Yuan, and Hung-Yi Lee. 2020. What does a network layer hear? analyzing hidden representations of end-to-end asr through speech synthesis. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 6434\u20136438.",
      "doi": ""
    },
    {
      "text": "Kunpeng Li, Ziyan Wu, Kuan-Chuan Peng, Jan Ernst, and Yun Fu. 2018. Tell me where to look: Guided attention inference network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 9215\u20139223.",
      "doi": ""
    },
    {
      "text": "Q\u00a0Vera Liao, Daniel Gruen, and Sarah Miller. 2020. Questioning the AI: informing design practices for explainable AI user experiences. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. 1\u201315.",
      "doi": "10.1145/3313831.3376590"
    },
    {
      "text": "Brian\u00a0Y Lim and Anind\u00a0K Dey. 2009. Assessing demand for intelligibility in context-aware applications. In Proceedings of the 11th international conference on Ubiquitous computing. 195\u2013204.",
      "doi": "10.1145/1620545.1620576"
    },
    {
      "text": "Brian\u00a0Y Lim and Anind\u00a0K Dey. 2010. Toolkit to support intelligibility in context-aware applications. In Proceedings of the 12th ACM international conference on Ubiquitous computing. 13\u201322.",
      "doi": "10.1145/1864349.1864353"
    },
    {
      "text": "Brian\u00a0Y Lim and Anind\u00a0K Dey. 2011. Design of an intelligible mobile context-aware application. In Proceedings of the 13th international conference on human computer interaction with mobile devices and services. 157\u2013166.",
      "doi": "10.1145/2037373.2037399"
    },
    {
      "text": "Brian\u00a0Y Lim and Anind\u00a0K Dey. 2011. Investigating intelligibility for uncertain context-aware applications. In Proceedings of the 13th international conference on Ubiquitous computing. 415\u2013424.",
      "doi": "10.1145/2030112.2030168"
    },
    {
      "text": "Brian\u00a0Y Lim and Anind\u00a0K Dey. 2013. Evaluating intelligibility usage and usefulness in a context-aware application. In International Conference on Human-Computer Interaction. Springer, 92\u2013101.",
      "doi": ""
    },
    {
      "text": "Brian\u00a0Y Lim, Anind\u00a0K Dey, and Daniel Avrahami. 2009. Why and why not explanations improve the intelligibility of context-aware intelligent systems. In Proceedings of the SIGCHI conference on human factors in computing systems. 2119\u20132128.",
      "doi": "10.1145/1518701.1519023"
    },
    {
      "text": "Zachary\u00a0C Lipton. 2018. The Mythos of Model Interpretability: In machine learning, the concept of interpretability is both important and slippery.Queue 16, 3 (2018), 31\u201357.",
      "doi": "10.1145/3236386.3241340"
    },
    {
      "text": "Steven\u00a0R Livingstone and Frank\u00a0A Russo. 2018. The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English. PloS one 13, 5 (2018), e0196391.",
      "doi": ""
    },
    {
      "text": "Erfan Loweimi, Peter Bell, and Steve Renals. 2019. On Learning Interpretable CNNs with Parametric Modulated Kernel-Based Filters.. In INTERSPEECH. 3480\u20133484.",
      "doi": ""
    },
    {
      "text": "Xueming Luo, Marco\u00a0Shaojun Qin, Zheng Fang, and Zhe Qu. 2021. Artificial Intelligence Coaches for Sales Agents: Caveats and Solutions. Journal of Marketing 85, 2 (2021), 14\u201332.",
      "doi": ""
    },
    {
      "text": "Christine Ma-Kellams and Jennifer Lerner. 2016. Trust your gut or think carefully? Examining whether an intuitive, versus a systematic, mode of thought produces greater empathic accuracy.Journal of personality and social psychology 111, 5(2016), 674.",
      "doi": ""
    },
    {
      "text": "Raju Maharjan, Per B\u00e6kgaard, and Jakob\u00a0E Bardram. 2019. \u201dHear me out\u201d smart speaker based conversational agent to monitor symptoms in mental health. In Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers. 929\u2013933.",
      "doi": ""
    },
    {
      "text": "Soroosh Mariooryad and Carlos Busso. 2014. Compensating for speaker or lexical variabilities in speech for emotion recognition. Speech Communication 57(2014), 1\u201312.",
      "doi": "10.1016/j.specom.2013.07.011"
    },
    {
      "text": "Tim Miller. 2019. Explanation in artificial intelligence: Insights from the social sciences. Artificial intelligence 267 (2019), 1\u201338.",
      "doi": ""
    },
    {
      "text": "Tim Miller. 2021. Contrastive explanation: A structural-model approach. The Knowledge Engineering Review 36 (2021).",
      "doi": ""
    },
    {
      "text": "Seyedmahdad Mirsamadi, Emad Barsoum, and Cha Zhang. 2017. Automatic speech emotion recognition using recurrent neural networks with local attention. In 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2227\u20132231.",
      "doi": "10.1109/ICASSP.2017.7952552"
    },
    {
      "text": "Gr\u00e9goire Montavon, Sebastian Lapuschkin, Alexander Binder, Wojciech Samek, and Klaus-Robert M\u00fcller. 2017. Explaining nonlinear classification decisions with deep taylor decomposition. Pattern Recognition 65(2017), 211\u2013222.",
      "doi": "10.1016/j.patcog.2016.11.008"
    },
    {
      "text": "Ramaravind\u00a0K Mothilal, Amit Sharma, and Chenhao Tan. 2020. Explaining machine learning classifiers through diverse counterfactual explanations. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. 607\u2013617.",
      "doi": "10.1145/3351095.3372850"
    },
    {
      "text": "Robert\u00a0S Moyer and Richard\u00a0H Bayer. 1976. Mental comparison and the symbolic distance effect. Cognitive Psychology 8, 2 (1976), 228\u2013246.",
      "doi": ""
    },
    {
      "text": "Chris Olah, Alexander Mordvintsev, and Ludwig Schubert. 2017. Feature visualization. Distill 2, 11 (2017), e7.",
      "doi": ""
    },
    {
      "text": "Marc\u00a0D Pell and Sonja\u00a0A Kotz. 2011. On the time course of vocal emotion recognition. PLoS One 6, 11 (2011), e27256.",
      "doi": ""
    },
    {
      "text": "Valery Petrushin. 1999. Emotion in speech: Recognition and application to call centers. In Proceedings of artificial neural networks in engineering, Vol.\u00a0710. 22.",
      "doi": ""
    },
    {
      "text": "Rosalind\u00a0W Picard. 2000. Affective computing.",
      "doi": ""
    },
    {
      "text": "Michael\u00a0I Posner, Mary\u00a0J Nissen, and Raymond\u00a0M Klein. 1976. Visual dominance: an information-processing account of its origins and significance.Psychological review 83, 2 (1976), 157.",
      "doi": ""
    },
    {
      "text": "Forough Poursabzi-Sangdeh, Daniel\u00a0G Goldstein, Jake\u00a0M Hofman, Jennifer\u00a0Wortman Wortman\u00a0Vaughan, and Hanna Wallach. 2021. Manipulating and measuring model interpretability. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 1\u201352.",
      "doi": "10.1145/3411764.3445315"
    },
    {
      "text": "Mohit Prabhushankar, Gukyeong Kwon, Dogancan Temel, and Ghassan AlRegib. 2020. Contrastive explanations in neural networks. In 2020 IEEE International Conference on Image Processing (ICIP). IEEE, 3289\u20133293.",
      "doi": ""
    },
    {
      "text": "Pearl Pu and Li Chen. 2007. Trust-inspiring explanation interfaces for recommender systems. Knowl. Based Syst. 20(2007), 542\u2013556.",
      "doi": "10.1016/j.knosys.2007.04.004"
    },
    {
      "text": "Alec Radford, Luke Metz, and Soumith Chintala. 2015. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434(2015).",
      "doi": ""
    },
    {
      "text": "Alexander Rakhlin, Alexey Shvets, Vladimir Iglovikov, and Alexandr\u00a0A Kalinin. 2018. Deep convolutional neural networks for breast cancer histology image analysis. In International conference image analysis and recognition. Springer, 737\u2013744.",
      "doi": ""
    },
    {
      "text": "Mirco Ravanelli and Yoshua Bengio. 2018. Speaker recognition from raw waveform with sincnet. In 2018 IEEE Spoken Language Technology Workshop (SLT). IEEE, 1021\u20131028.",
      "doi": ""
    },
    {
      "text": "Todd\u00a0R. Reed, Nancy\u00a0E. Reed, and Peter\u00a0A. Fritzson. 2004. Heart sound analysis for symptom detection and computer-aided diagnosis. Simul. Model. Pract. Theory 12 (2004), 129\u2013146.",
      "doi": ""
    },
    {
      "text": "Marco\u00a0Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. \u201d Why should i trust you?\u201d Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. 1135\u20131144.",
      "doi": "10.1145/2939672.2939778"
    },
    {
      "text": "Marco\u00a0Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2018. Anchors: High-precision model-agnostic explanations. In Proceedings of the AAAI conference on artificial intelligence, Vol.\u00a032.",
      "doi": ""
    },
    {
      "text": "Matthew Richardson, Amit Prakash, and Eric Brill. 2006. Beyond PageRank: machine learning for static ranking. In Proceedings of the 15th international conference on World Wide Web. 707\u2013715.",
      "doi": "10.1145/1135777.1135881"
    },
    {
      "text": "Andrew\u00a0Slavin Ross, Michael\u00a0C Hughes, and Finale Doshi-Velez. 2017. Right for the right reasons: training differentiable models by constraining their explanations. In Proceedings of the 26th International Joint Conference on Artificial Intelligence. 2662\u20132670.",
      "doi": ""
    },
    {
      "text": "Disa\u00a0A Sauter, Frank Eisner, Andrew\u00a0J Calder, and Sophie\u00a0K Scott. 2010. Perceptual cues in nonverbal vocal expressions of emotion. Quarterly Journal of Experimental Psychology 63, 11(2010), 2251\u20132272.",
      "doi": ""
    },
    {
      "text": "Annett Schirmer and Sonja\u00a0A Kotz. 2006. Beyond the right hemisphere: brain mechanisms mediating vocal emotional processing. Trends in cognitive sciences 10, 1 (2006), 24\u201330.",
      "doi": ""
    },
    {
      "text": "Edward Segel and Jeffrey Heer. 2010. Narrative visualization: Telling stories with data. IEEE transactions on visualization and computer graphics 16, 6(2010), 1139\u20131148.",
      "doi": ""
    },
    {
      "text": "Ramprasaath\u00a0R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. 2017. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE international conference on computer vision. 618\u2013626.",
      "doi": ""
    },
    {
      "text": "Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2014. Deep inside convolutional networks: Visualising image classification models and saliency maps. (2014).",
      "doi": ""
    },
    {
      "text": "Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic attribution for deep networks. In International Conference on Machine Learning. PMLR, 3319\u20133328.",
      "doi": ""
    },
    {
      "text": "Chun-Hua Tsai, Yue You, Xinning Gui, Yubo Kou, and John\u00a0M Carroll. 2021. Exploring and Promoting Diagnostic Transparency and Explainability in Online Symptom Checkers. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 1\u201317.",
      "doi": "10.1145/3411764.3445101"
    },
    {
      "text": "Panagiotis Tzirakis, Jiehao Zhang, and Bjorn\u00a0W Schuller. 2018. End-to-end speech emotion recognition using deep neural networks. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 5089\u20135093.",
      "doi": "10.1109/ICASSP.2018.8462677"
    },
    {
      "text": "J van\u00a0der Waa, M Robeer, J van Diggelen, M Brinkhuis, and M Neerincx. 2018. Contrastive Explanations with Local Foil Trees. In Proceedings of the ICML Workshop on Human Interpretability in Machine Learning (WHI 2018), Stockholm, Sweden, Vol.\u00a037.",
      "doi": ""
    },
    {
      "text": "Sandra Wachter, Brent Mittelstadt, and Chris Russell. 2017. Counterfactual explanations without opening the black box: Automated decisions and the GDPR. Harv. JL & Tech. 31(2017), 841.",
      "doi": ""
    },
    {
      "text": "Danding Wang, Qian Yang, Ashraf Abdul, and Brian\u00a0Y Lim. 2019. Designing theory-driven user-centric explainable AI. In Proceedings of the 2019 CHI conference on human factors in computing systems. 1\u201315.",
      "doi": "10.1145/3290605.3300831"
    },
    {
      "text": "Danding Wang, Wencan Zhang, and Brian\u00a0Y Lim. 2021. Show or suppress? Managing input uncertainty in machine learning model explanations. Artificial Intelligence 294 (2021), 103456.",
      "doi": ""
    },
    {
      "text": "Rui Wang, Fanglin Chen, Zhenyu Chen, Tianxing Li, Gabriella Harari, Stefanie Tignor, Xia Zhou, Dror Ben-Zeev, and Andrew\u00a0T Campbell. 2014. StudentLife: assessing mental health, academic performance and behavioral trends of college students using smartphones. In Proceedings of the 2014 ACM international joint conference on pervasive and ubiquitous computing. 3\u201314.",
      "doi": "10.1145/2632048.2632054"
    },
    {
      "text": "Xinru Wang and Ming Yin. 2021. Are Explanations Helpful? A Comparative Study of the Effects of Explanations in AI-Assisted Decision-Making. In 26th International Conference on Intelligent User Interfaces. 318\u2013328.",
      "doi": "10.1145/3397481.3450650"
    },
    {
      "text": "Stephan\u00a0W. Wegerich, Alan\u00a0D. Wilks, and R.\u00a0Matthew Pipke. 2003. Nonparametric modeling of vibration signal features for equipment health monitoring. 2003 IEEE Aerospace Conference Proceedings (Cat. No.03TH8652) 7 (2003), 3113\u20133121.",
      "doi": ""
    },
    {
      "text": "Taihong Xiao, Jiapeng Hong, and Jinwen Ma. 2018. Elegant: Exchanging latent encodings with gan for transferring multiple face attributes. In Proceedings of the European conference on computer vision (ECCV). 168\u2013184.",
      "doi": "10.1007/978-3-030-01249-6_11"
    },
    {
      "text": "Ming Yin, Jennifer Wortman\u00a0Vaughan, and Hanna Wallach. 2019. Understanding the effect of accuracy on trust in machine learning models. In Proceedings of the 2019 chi conference on human factors in computing systems. 1\u201312.",
      "doi": "10.1145/3290605.3300509"
    },
    {
      "text": "Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung. 2018. Multimodal speech emotion recognition using audio and text. In 2018 IEEE Spoken Language Technology Workshop (SLT). IEEE, 112\u2013118.",
      "doi": ""
    },
    {
      "text": "Quanshi Zhang and Song-Chun Zhu. 2018. Visual interpretability for deep learning: a survey. Frontiers of Information Technology & Electronic Engineering 19 (2018), 27\u201339.",
      "doi": ""
    },
    {
      "text": "Wencan Zhang, Mariella Dimiccoli, and Brian\u00a0Y Lim. 2020. Debiased-CAM for bias-agnostic faithful visual explanations of deep convolutional networks. arXiv preprint arXiv:2012.05567(2020).",
      "doi": ""
    },
    {
      "text": "Jianfeng Zhao, Xia Mao, and Lijiang Chen. 2019. Speech emotion recognition using deep 1D & 2D CNN LSTM networks. Biomedical Signal Processing and Control 47 (2019), 312\u2013323.",
      "doi": ""
    },
    {
      "text": "Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. 2016. Learning deep features for discriminative localization. In Proceedings of the IEEE conference on computer vision and pattern recognition. 2921\u20132929.",
      "doi": ""
    },
    {
      "text": "Bolei Zhou, Yiyou Sun, David Bau, and Antonio Torralba. 2018. Interpretable basis decomposition for visual explanation. In Proceedings of the European Conference on Computer Vision (ECCV). 119\u2013134.",
      "doi": "10.1007/978-3-030-01237-3_8"
    },
    {
      "text": "Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei\u00a0A Efros. 2017. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEE international conference on computer vision. 2223\u20132232.",
      "doi": ""
    }
  ]
}