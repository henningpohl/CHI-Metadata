{
  "doi": "10.1145/3491102.3517687",
  "title": "Aware: Intuitive Device Activation Using Prosody for Natural Voice Interactions",
  "published": "2022-04-29",
  "proctitle": "CHI '22: Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems",
  "pages": "1-16",
  "year": 2022,
  "badges": [],
  "abstract": "Voice interactive devices often use keyword spotting for device activation. However, this approach suffers from misrecognition of keywords and can respond to keywords not intended for calling the device (e.g., \u201dYou can ask Alexa about it.\u201d), causing accidental device activations. We propose a method that leverages prosodic features to differentiate calling/not-calling voices (F1 score: 0.869), allowing devices to respond only when called upon to avoid misactivation. As a proof of concept, we built a prototype smart speaker called Aware that allows users to control the device activation by speaking the keyword in specific prosody patterns. These patterns are chosen to represent people\u2019s natural calling/not-calling voices, which are uncovered in a study to collect such voices and investigate their prosodic difference. A user study comparing Aware with Amazon Echo shows Aware can activate more correctly (F1 score 0.93 vs. 0.56) and is easy to learn and use.",
  "authors": [
    {
      "name": "Xinlei Zhang",
      "institution": "Graduate School of Interdisciplinary Information Studies / Rekimoto Lab, The University of Tokyo, Japan",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659080170",
      "orcid": "missing"
    },
    {
      "name": "Zixiong Su",
      "institution": "Graduate School of Interdisciplinary Information Studies / Rekimoto Lab, The University of Tokyo, Japan",
      "img": "/do/10.1145/contrib-99659729502/rel-imgonly/profile.jpg",
      "acmid": "99659729502",
      "orcid": "0000-0001-6048-3268"
    },
    {
      "name": "Jun Rekimoto",
      "institution": "Graduate School of Interdisciplinary Information Studies / Rekimoto Lab, The University of Tokyo, Japan and Sony CSL Kyoto, Japan",
      "img": "/do/10.1145/contrib-81100008564/rel-imgonly/p2.png",
      "acmid": "81100008564",
      "orcid": "0000-0002-3629-2514"
    }
  ],
  "references": [
    {
      "text": "Karan Ahuja, Andy Kong, Mayank Goel, and Chris Harrison. 2020. Direction-of-Voice (DoV) Estimation for Intuitive Speech Interaction with Smart Device Ecosystems. In Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology. 1121\u20131131.",
      "doi": "10.1145/3379337.3415588"
    },
    {
      "text": "Saul Albert and Magnus Hamann. 2021. Putting wake words to bed: We speak wake words with systematically varied prosody, but CUIs don\u2019t listen. In CUI 2021-3rd Conference on Conversational User Interfaces. 1\u20135.",
      "doi": "10.1145/3469595.3469608"
    },
    {
      "text": "Tawfiq Ammari, Jofish Kaye, Janice\u00a0Y Tsai, and Frank Bentley. 2019. Music, Search, and IoT: How People (Really) Use Voice Assistants.ACM Trans. Comput. Hum. Interact. 26, 3 (2019), 17\u20131.",
      "doi": "10.1145/3311956"
    },
    {
      "text": "Rainer Banse and Klaus\u00a0R Scherer. 1996. Acoustic profiles in vocal emotion expression.Journal of personality and social psychology 70, 3(1996), 614.",
      "doi": ""
    },
    {
      "text": "Jared Bernstein, Amir Najmi, and Farzad Ehsani. 1999. Subarashii: Encounters in Japanese spoken language education. CALICO journal (1999), 361\u2013384.",
      "doi": ""
    },
    {
      "text": "Richard\u00a0A Bolt. 1980. \u201cPut-that-there\u201d Voice and gesture at the graphics interface. In Proceedings of the 7th annual conference on Computer graphics and interactive techniques. 262\u2013270.",
      "doi": "10.1145/800250.807503"
    },
    {
      "text": "Varun Chandrasekaran, Suman Banerjee, Bilge Mutlu, and Kassem Fawaz. 2021. PowerCut and Obfuscator: An Exploration of the Design Space for Privacy-Preserving Interventions for Smart Speakers. In Seventeenth Symposium on Usable Privacy and Security ({SOUPS} 2021). 535\u2013552.",
      "doi": ""
    },
    {
      "text": "Ailbhe\u00a0N\u00ed Chasaide, Irena Yanushevskaya, and Christer Gobl. 2017. Voice-to-Affect Mapping: Inferences on Language Voice Baseline Settings.. In INTERSPEECH. 1258\u20131262.",
      "doi": ""
    },
    {
      "text": "Henry\u00a0S Cheang and Marc\u00a0D Pell. 2008. The sound of sarcasm. Speech communication 50, 5 (2008), 366\u2013381.",
      "doi": ""
    },
    {
      "text": "Yuxin Chen, Huiying Li, Shan-Yuan Teng, Steven Nagels, Zhijing Li, Pedro Lopes, Ben\u00a0Y Zhao, and Haitao Zheng. 2020. Wearable microphone jamming. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. 1\u201312.",
      "doi": "10.1145/3313831.3376304"
    },
    {
      "text": "Sunming Cheung, Steven\u00a0M Entine, and Jerome\u00a0H Klotz. 1977. Microcomputer voice-response telephone entry for balanced clinical trial randomization. Journal of medical systems 1, 2 (1977), 165\u2013169.",
      "doi": ""
    },
    {
      "text": "Alice Coucke, Alaa Saade, Adrien Ball, Th\u00e9odore Bluche, Alexandre Caulier, David Leroy, Cl\u00e9ment Doumouro, Thibault Gisselbrecht, Francesco Caltagirone, Thibaut Lavril, 2018. Snips voice platform: an embedded spoken language understanding system for private-by-design voice interfaces. arXiv preprint arXiv:1805.10190(2018).",
      "doi": ""
    },
    {
      "text": "Thierry Desot, Fran\u00e7ois Portet, and Michel Vacher. 2019. Towards End-to-End spoken intent recognition in smart home. In 2019 International Conference on Speech Technology and Human-Computer Dialogue (SpeD). IEEE, 1\u20138.",
      "doi": ""
    },
    {
      "text": "Kelly Dobson. 2004. Blendie. In Proceedings of the 5th conference on Designing interactive systems: processes, practices, methods, and techniques. 309\u2013309.",
      "doi": ""
    },
    {
      "text": "Daniel\u00a0J Dubois, Roman Kolcun, Anna\u00a0Maria Mandalari, Muhammad\u00a0Talha Paracha, David\u00a0R Choffnes, and Hamed Haddadi. 2020. When Speakers Are All Ears: Characterizing Misactivations of IoT Smart Speakers.Proc. Priv. Enhancing Technol. 2020, 4 (2020), 255\u2013276.",
      "doi": ""
    },
    {
      "text": "Huan Feng, Kassem Fawaz, and Kang\u00a0G Shin. 2017. Continuous authentication for voice assistants. In Proceedings of the 23rd Annual International Conference on Mobile Computing and Networking. 343\u2013355.",
      "doi": "10.1145/3117811.3117823"
    },
    {
      "text": "Carole\u00a0T Ferrand. 2002. Harmonics-to-noise ratio: an index of vocal aging. Journal of voice 16, 4 (2002), 480\u2013487.",
      "doi": ""
    },
    {
      "text": "Kim Fluitt, Timothy Mermagen, and Tomasz Letowski. 2014. Auditory distance estimation in an open space. Soundscape Semiotics-Localization and Categorization (2014).",
      "doi": ""
    },
    {
      "text": "Ido Freeman, Lutz Roese-Koerner, and Anton Kummert. 2018. Effnet: An efficient structure for convolutional neural networks. In 2018 25th ieee international conference on image processing (icip). IEEE, 6\u201310.",
      "doi": ""
    },
    {
      "text": "Robert\u00a0W Frick. 1985. Communicating emotion: The role of prosodic features.Psychological bulletin 97, 3 (1985), 412.",
      "doi": ""
    },
    {
      "text": "J. Garofolo, Lori Lamel, W. Fisher, Jonathan Fiscus, D. Pallett, N. Dahlgren, and V. Zue. 1992. TIMIT Acoustic-phonetic Continuous Speech Corpus. Linguistic Data Consortium (11 1992).",
      "doi": ""
    },
    {
      "text": "Christer Gobl and Ailbhe\u00a0N\u0131 Chasaide. 2003. The role of voice quality in communicating emotion, mood and attitude. Speech communication 40, 1-2 (2003), 189\u2013212.",
      "doi": ""
    },
    {
      "text": "Masataka Goto, Koji Kitayama, Katsunobu Itou, and Tetsunori Kobayashi. 2004. Speech Spotter: On-demand speech recognition in human-human conversation on the telephone or in face-to-face situations. In Eighth International Conference on Spoken Language Processing.",
      "doi": ""
    },
    {
      "text": "Nele Hellbernd and Daniela Sammler. 2016. Prosody conveys speaker\u2019s intentions: Acoustic cues for speech act perception. Journal of Memory and Language 88 (2016), 70\u201386.",
      "doi": ""
    },
    {
      "text": "Julie\u00a0M Hupp, Melissa\u00a0K Jungers, Celeste\u00a0M Hinerman, and Brandon\u00a0L Porter. 2021. Cup! Cup? Cup: Comprehension of intentional prosody in adults and children. Cognitive Development 57(2021), 100971.",
      "doi": ""
    },
    {
      "text": "Takeo Igarashi and John\u00a0F Hughes. 2001. Voice as sound: using non-verbal voice input for interactive control. In Proceedings of the 14th annual ACM symposium on User interface software and technology. 155\u2013156.",
      "doi": "10.1145/502348.502372"
    },
    {
      "text": "Amazon Inc.[n.d.]. Amazon Echo. https://www.amazon.com/smart-home-devices/b?ie=UTF8&node=9818047011.",
      "doi": ""
    },
    {
      "text": "Apple Inc.[n.d.]. Siri. https://www.apple.com/siri/.",
      "doi": ""
    },
    {
      "text": "Google Inc.[n.d.]. Google Nest. https://store.google.com/category/connected_home?.",
      "doi": ""
    },
    {
      "text": "Yasha Iravantchi, Karan Ahuja, Mayank Goel, Chris Harrison, and Alanson Sample. 2021. PrivacyMic: Utilizing Inaudible Frequencies for Privacy Preserving Daily Activity Recognition. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 1\u201313.",
      "doi": "10.1145/3411764.3445169"
    },
    {
      "text": "Yuki Irie, Shigeki Matsubara, Nobuo Kawaguchi, Yukiko Yamaguchi, and Yasuyoshi Inagaki. 2004. Speech intention understanding based on decision tree learning. In Eighth International Conference on Spoken Language Processing.",
      "doi": ""
    },
    {
      "text": "C Ishi, Hiroshi Ishiguro, and Norihiro Hagita. 2006. Using prosodic and voice quality features for paralinguistic information extraction. In Proc. of Speech Prosody. Citeseer, 883\u2013886.",
      "doi": ""
    },
    {
      "text": "Carlos\u00a0Toshinori Ishi, Hiroshi Ishiguro, and Norihiro Hagita. 2008. Automatic extraction of paralinguistic information using prosodic features related to F0, duration and voice quality. Speech communication 50, 6 (2008), 531\u2013543.",
      "doi": ""
    },
    {
      "text": "Xiaoming Jiang and Marc\u00a0D Pell. 2017. The sound of confidence and doubt. Speech Communication 88(2017), 106\u2013126.",
      "doi": "10.1016/j.specom.2017.01.011"
    },
    {
      "text": "Bjorn Karmann. 2019. Project Alias. https://bjoernkarmann.dk/project_alias/.",
      "doi": ""
    },
    {
      "text": "Akinobu Lee, Tatsuya Kawahara, and Kiyohiro Shikano. 2001. Julius\u2014an open source real-time large vocabulary recognition engine. (2001).",
      "doi": ""
    },
    {
      "text": "Matrix. [n.d.]. Matrix Voice. https://www.matrix.one/products/voice.",
      "doi": ""
    },
    {
      "text": "Sven Mayer, Gierad Laput, and Chris Harrison. 2020. Enhancing Mobile Voice Assistants with WorldGaze. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. 1\u201310.",
      "doi": "10.1145/3313831.3376479"
    },
    {
      "text": "Donald McMillan, Barry Brown, Ikkaku Kawaguchi, Razan Jaber, Jordi Solsona\u00a0Belenguer, and Hideaki Kuzuoka. 2019. Designing with Gaze: Tama\u2013a Gaze Activated Smart-Speaker. Proceedings of the ACM on Human-Computer Interaction 3, CSCW(2019), 1\u201326.",
      "doi": "10.1145/3359278"
    },
    {
      "text": "Abraham Mhaidli, Manikandan\u00a0Kandadai Venkatesh, Yixin Zou, and Florian Schaub. 2020. Listen Only When Spoken To: Interpersonal Communication Cues as Smart Speaker Privacy Controls. Proceedings on Privacy Enhancing Technologies 2020, 2(2020), 251\u2013270.",
      "doi": ""
    },
    {
      "text": "Rachel\u00a0LC Mitchell and Elliott\u00a0D Ross. 2013. Attitudinal prosody: What we know and directions for future study. Neuroscience & Biobehavioral Reviews 37, 3 (2013), 471\u2013479.",
      "doi": ""
    },
    {
      "text": "Jack Mostow 2001. Evaluating tutors that listen: An overview of Project LISTEN.(2001).",
      "doi": ""
    },
    {
      "text": "Robert Ne\u00dfelrath, Mohammad\u00a0Mehdi Moniri, and Michael Feld. 2016. Combining speech, gaze, and micro-gestures for the multimodal control of in-car functions. In 2016 12th International Conference on Intelligent Environments (IE). IEEE, 190\u2013193.",
      "doi": ""
    },
    {
      "text": "II Nicholls. 1988. WL. Computer-assisted telephone interviewing: a general introduction. Telephone survey methodology. New York: John Wiley & Sons Inc (1988), 377\u201385.",
      "doi": ""
    },
    {
      "text": "Liqiang Nie, Mengzhao Jia, Xuemeng Song, Ganglu Wu, Harry Cheng, and Jian Gu. 2021. Multimodal Activation: Awakening Dialog Robots without Wake Words. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. 491\u2013500.",
      "doi": "10.1145/3404835.3462964"
    },
    {
      "text": "Leonardo Pepino, Pablo Riera, and Luciana Ferrer. 2021. Emotion Recognition from Speech Using Wav2vec 2.0 Embeddings. arXiv preprint arXiv:2104.03502(2021).",
      "doi": ""
    },
    {
      "text": "Bastian Pfleging, Stefan Schneegass, and Albrecht Schmidt. 2012. Multimodal interaction in the car: combining speech and gestures on the steering wheel. In Proceedings of the 4th International Conference on Automotive User Interfaces and Interactive Vehicular Applications. 155\u2013162.",
      "doi": "10.1145/2390256.2390282"
    },
    {
      "text": "Patryk Pomykalski, Miko\u0142aj\u00a0P Wo\u017aniak, Pawe\u0142\u00a0W Wo\u017aniak, Krzysztof Grudzie\u0144, Shengdong Zhao, and Andrzej Romanowski. 2020. Considering Wake Gestures for Smart Assistant Use. In Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems. 1\u20138.",
      "doi": ""
    },
    {
      "text": "Yue Qin, Chun Yu, Zhaoheng Li, Mingyuan Zhong, Yukang Yan, and Yuanchun Shi. 2021. ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 1\u201312.",
      "doi": "10.1145/3411764.3445687"
    },
    {
      "text": "Simon Rigoulot, Karyn Fish, and Marc\u00a0D Pell. 2014. Neural correlates of inferring speaker sincerity from white lies: An event-related potential source localization study. Brain research 1565(2014), 48\u201362.",
      "doi": ""
    },
    {
      "text": "Florian Roider, Lars Reisig, and Tom Gross. 2018. Just look: The benefits of gaze-activated voice input in the car. In Adjunct Proceedings of the 10th International Conference on Automotive User Interfaces and Interactive Vehicular Applications. 210\u2013214.",
      "doi": "10.1145/3239092.3265968"
    },
    {
      "text": "Nirupam Roy, Sheng Shen, Haitham Hassanieh, and Romit\u00a0Roy Choudhury. 2018. Inaudible voice commands: The long-range attack and defense. In 15th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 18). 547\u2013560.",
      "doi": ""
    },
    {
      "text": "Daisuke Sakamoto, Takanori Komatsu, and Takeo Igarashi. 2013. Voice augmented manipulation: using paralinguistic information to manipulate mobile devices. In Proceedings of the 15th international conference on Human-computer interaction with mobile devices and services. 69\u201378.",
      "doi": "10.1145/2493190.2493244"
    },
    {
      "text": "Lea Sch\u00f6nherr, Maximilian Golla, Thorsten Eisenhofer, Jan Wiele, Dorothea Kolossa, and Thorsten Holz. 2020. Unacceptable, where is my privacy? exploring accidental triggers of smart speakers. arXiv preprint arXiv:2008.00508(2020).",
      "doi": ""
    },
    {
      "text": "Alex Sciuto, Arnita Saini, Jodi Forlizzi, and Jason\u00a0I Hong. 2018. \u201d Hey Alexa, What\u2019s Up?\u201d A Mixed-Methods Studies of In-Home Conversational Agent Usage. In Proceedings of the 2018 Designing Interactive Systems Conference. 857\u2013868.",
      "doi": "10.1145/3196709.3196772"
    },
    {
      "text": "Dmitriy Serdyuk, Yongqiang Wang, Christian Fuegen, Anuj Kumar, Baiyang Liu, and Yoshua Bengio. 2018. Towards end-to-end spoken language understanding. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 5754\u20135758.",
      "doi": "10.1109/ICASSP.2018.8461785"
    },
    {
      "text": "Edwin Simonnet, Sahar Ghannay, Nathalie Camelin, Yannick Est\u00e8ve, and Renato De\u00a0Mori. 2017. ASR error management for improving spoken language understanding. arXiv preprint arXiv:1705.09515(2017).",
      "doi": ""
    },
    {
      "text": "Shamane Siriwardhana, Tharindu Kaluarachchi, Mark Billinghurst, and Suranga Nanayakkara. 2020. Multimodal Emotion Recognition With Transformer-Based Self Supervised Feature Fusion. IEEE Access 8(2020), 176274\u2013176285.",
      "doi": "10.1109/access.2020.3026823"
    },
    {
      "text": "Paul Warren. 1999. Prosody and language processing. Language processing (1999), 155\u2013188.",
      "doi": ""
    },
    {
      "text": "Yi Xu and Xuejing Sun. 2002. Maximum speed of pitch change and how it may relate to speech. The Journal of the Acoustical Society of America 111, 3 (2002), 1399\u20131413.",
      "doi": ""
    },
    {
      "text": "Jackie Yang, Gaurab Banerjee, Vishesh Gupta, Monica\u00a0S Lam, and James\u00a0A Landay. 2020. Soundr: Head Position and Orientation Prediction Using a Microphone Array. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. 1\u201312.",
      "doi": "10.1145/3313831.3376427"
    },
    {
      "text": "Nicole Yankelovich, Gina-Anne Levow, and Matt Marx. 1995. Designing SpeechActs: Issues in speech user interfaces. In Proceedings of the SIGCHI conference on Human factors in computing systems. 369\u2013376.",
      "doi": "10.1145/223904.223952"
    },
    {
      "text": "Yucan Zhou, Qinghua Hu, Jie Liu, and Yuan Jia. 2015. Combining heterogeneous deep neural networks with conditional random fields for Chinese dialogue act recognition. Neurocomputing 168(2015), 408\u2013417.",
      "doi": "10.1016/j.neucom.2015.05.086"
    }
  ]
}