{
  "doi": "10.1145/3491102.3502104",
  "title": "Understanding the impact of explanations on advice-taking: a user study for AI-based clinical Decision Support Systems",
  "published": "2022-04-29",
  "proctitle": "CHI '22: Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems",
  "pages": "1-9",
  "year": 2022,
  "badges": [
    "Honorable Mention"
  ],
  "abstract": "The field of eXplainable Artificial Intelligence (XAI) focuses on providing explanations for AI systems\u2019 decisions. XAI applications to AI-based Clinical Decision Support Systems (DSS) should increase trust in the DSS by allowing clinicians to investigate the reasons behind its suggestions. In this paper, we present the results of a user study on the impact of advice from a clinical DSS on healthcare providers\u2019 judgment in two different cases: the case where the clinical DSS explains its suggestion and the case it does not. We examined the weight of advice, the behavioral intention to use the system, and the perceptions with quantitative and qualitative measures. Our results indicate a more significant impact of advice when an explanation for the DSS decision is provided. Additionally, through the open-ended questions, we provide some insights on how to improve the explanations in the diagnosis forecasts for healthcare assistants, nurses, and doctors.",
  "authors": [
    {
      "name": "Cecilia Panigutti",
      "institution": "Scuola Normale Superiore, Italy and Computer Science, University of Pisa, Italy",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659495293",
      "orcid": "missing"
    },
    {
      "name": "Andrea Beretta",
      "institution": "ISTI, CNR - Italian National Research Council, Italy",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99660042605",
      "orcid": "missing"
    },
    {
      "name": "Fosca Giannotti",
      "institution": "CNR - Italian National Research Council, Italy and Scuola Normale Superiore, Italy",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81100458577",
      "orcid": "missing"
    },
    {
      "name": "Dino Pedreschi",
      "institution": "Computer Science, University of Pisa, Italy",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81100352341",
      "orcid": "missing"
    }
  ],
  "references": [
    {
      "text": "European Commission 2018. EU General Data Protection Regulation. European Commission. https://ec.europa.eu/commission/sites/beta-political/files/data-protection-factsheet-changes_en.pdf",
      "doi": ""
    },
    {
      "text": "Barbara\u00a0D Adams, Lora\u00a0E Bruyn, S\u00e9bastien Houde, Paul Angelopoulos, Kim Iwasa-Madge, and Carol McCann. 2003. Trust in automated systems. Ministry of National Defence(2003).",
      "doi": ""
    },
    {
      "text": "Anna\u00a0Markella Antoniadi, Yuhan Du, Yasmine Guendouz, Lan Wei, Claudia Mazo, Brett\u00a0A Becker, and Catherine Mooney. 2021. Current challenges and future opportunities for XAI in machine learning-based clinical decision support systems: a systematic review. Applied Sciences 11, 11 (2021), 5088.",
      "doi": ""
    },
    {
      "text": "Vijay Arya, Rachel\u00a0KE Bellamy, Pin-Yu Chen, Amit Dhurandhar, Michael Hind, Samuel\u00a0C Hoffman, Stephanie Houde, Q\u00a0Vera Liao, Ronny Luss, Aleksandra Mojsilovi\u0107, 2019. One explanation does not fit all: A toolkit and taxonomy of ai explainability techniques. arXiv preprint arXiv:1909.03012(2019).",
      "doi": ""
    },
    {
      "text": "Alina\u00a0Jade Barnett, Fides\u00a0Regina Schwartz, Chaofan Tao, Chaofan Chen, Yinhao Ren, Joseph\u00a0Y Lo, and Cynthia Rudin. 2021. A case-based interpretable deep learning model for classification of mass lesions in digital mammography. Nature Machine Intelligence(2021), 1\u201310.",
      "doi": ""
    },
    {
      "text": "Umang Bhatt, Alice Xiang, Shubham Sharma, Adrian Weller, Ankur Taly, Yunhan Jia, Joydeep Ghosh, Ruchir Puri, Jos\u00e9\u00a0MF Moura, and Peter Eckersley. 2020. Explainable machine learning in deployment. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. 648\u2013657.",
      "doi": "10.1145/3351095.3375624"
    },
    {
      "text": "Alan\u00a0F. Blackwell. 2021. Ethnographic artificial intelligence. Interdisciplinary Science Reviews 46, 1-2 (2021), 198\u2013211. https://doi.org/10.1080/03080188.2020.1840226",
      "doi": ""
    },
    {
      "text": "Francesco Bodria, Fosca Giannotti, Riccardo Guidotti, Francesca Naretto, Dino Pedreschi, and Salvatore Rinzivillo. 2021. Benchmarking and survey of explanation methods for black box models. arXiv preprint arXiv:2102.13076(2021).",
      "doi": ""
    },
    {
      "text": "Clark Borst. 2016. Shared mental models in human-machine systems. IFAC-PapersOnLine 49, 19 (2016), 195\u2013200.",
      "doi": ""
    },
    {
      "text": "Andrea Brennen. 2020. What Do People Really Want When They Say They Want\u201d Explainable AI?\u201d We Asked 60 Stakeholders.. In Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems. 1\u20137.",
      "doi": "10.1145/3334480.3383047"
    },
    {
      "text": "Zana Bu\u00e7inca, Phoebe Lin, Krzysztof\u00a0Z Gajos, and Elena\u00a0L Glassman. 2020. Proxy tasks and subjective measures can be misleading in evaluating explainable AI systems. In Proceedings of the 25th International Conference on Intelligent User Interfaces. 454\u2013464.",
      "doi": "10.1145/3377325.3377498"
    },
    {
      "text": "Zana Bu\u00e7inca, Maja\u00a0Barbara Malaya, and Krzysztof\u00a0Z Gajos. 2021. To trust or to think: cognitive forcing functions can reduce overreliance on AI in AI-assisted decision-making. Proceedings of the ACM on Human-Computer Interaction 5, CSCW1(2021), 1\u201321.",
      "doi": "10.1145/3449287"
    },
    {
      "text": "Adrian Bussone, Simone Stumpf, and Dympna O\u2019Sullivan. 2015. The role of explanations on trust and reliance in clinical decision support systems. In 2015 International Conference on Healthcare Informatics. IEEE, 160\u2013169.",
      "doi": "10.1109/ICHI.2015.26"
    },
    {
      "text": "John\u00a0T Cacioppo, Richard\u00a0E Petty, and Chuan Feng\u00a0Kao. 1984. The efficient assessment of need for cognition. Journal of personality assessment 48, 3 (1984), 306\u2013307.",
      "doi": ""
    },
    {
      "text": "B\u00e9atrice Cahour and Jean-Fran\u00e7ois Forzy. 2009. Does projection into use improve trust and exploration? An example with a cruise control system. Safety science 47, 9 (2009), 1260\u20131270.",
      "doi": ""
    },
    {
      "text": "Carrie\u00a0J Cai, Samantha Winter, David Steiner, Lauren Wilcox, and Michael Terry. 2019. \u201d Hello AI\u201d: Uncovering the Onboarding Needs of Medical Practitioners for Human-AI Collaborative Decision-Making. Proceedings of the ACM on Human-computer Interaction 3, CSCW(2019), 1\u201324.",
      "doi": "10.1145/3359206"
    },
    {
      "text": "Hao-Fei Cheng, Ruotong Wang, Zheng Zhang, Fiona O\u2019Connell, Terrance Gray, F.\u00a0Maxwell Harper, and Haiyi Zhu. 2019. Explaining Decision-Making Algorithms through UI: Strategies to Help Non-Expert Stakeholders. Association for Computing Machinery, New York, NY, USA, 1\u201312. https://doi.org/10.1145/3290605.3300789",
      "doi": "10.1145/3290605.3300789"
    },
    {
      "text": "Edward Choi, Mohammad\u00a0Taha Bahadori, Andy Schuetz, Walter\u00a0F Stewart, and Jimeng Sun. 2016. Doctor ai: Predicting clinical events via recurrent neural networks. In Machine learning for healthcare conference. PMLR, 301\u2013318.",
      "doi": ""
    },
    {
      "text": "Edward Choi, Mohammad\u00a0Taha Bahadori, Le Song, Walter\u00a0F Stewart, and Jimeng Sun. 2017. GRAM: graph-based attention model for healthcare representation learning. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 787\u2013795.",
      "doi": "10.1145/3097983.3098126"
    },
    {
      "text": "Giovanni Comand\u00e9. 2020. Unfolding the legal component of trustworthy AI: a must to avoid ethics washing.Version Accepted for Annuario di Diritto Comparato e di Studi Legislativi, forthcoming (2020).",
      "doi": ""
    },
    {
      "text": "Ian Covert, Scott Lundberg, and Su-In Lee. 2020. Explaining by removing: A unified framework for model explanation. arXiv preprint arXiv:2011.14878(2020).",
      "doi": ""
    },
    {
      "text": "Berkeley Dietvorst and Soaham Bharti. 2019. People Reject Even the Best Possible Algorithm in Uncertain Decision Domains. SSRN Electronic Journal(2019). https://doi.org/10.2139/ssrn.3424158",
      "doi": ""
    },
    {
      "text": "Berkeley\u00a0J Dietvorst and Soaham Bharti. 2020. People reject algorithms in uncertain decision domains because they have diminishing sensitivity to forecasting error. Psychological science 31, 10 (2020), 1302\u20131314.",
      "doi": ""
    },
    {
      "text": "Berkeley\u00a0J Dietvorst, Joseph\u00a0P Simmons, and Cade Massey. 2015. Algorithm aversion: People erroneously avoid algorithms after seeing them err.Journal of Experimental Psychology: General 144, 1 (2015), 114.",
      "doi": ""
    },
    {
      "text": "Finale Doshi-Velez and Been Kim. 2017. Towards a rigorous science of interpretable machine learning. arXiv preprint arXiv:1702.08608(2017).",
      "doi": ""
    },
    {
      "text": "Jinyun Duan, Yue Xu, and Lyn\u00a0M Van\u00a0Swol. 2020. Influence of self-concept clarity on advice seeking and utilisation. Asian Journal of Social Psychology(2020).",
      "doi": ""
    },
    {
      "text": "Upol Ehsan, Q\u00a0Vera Liao, Michael Muller, Mark\u00a0O Riedl, and Justin\u00a0D Weisz. 2021. Expanding explainability: Towards social transparency in ai systems. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 1\u201319.",
      "doi": "10.1145/3411764.3445188"
    },
    {
      "text": "Upol Ehsan and Mark\u00a0O. Riedl. 2020. Human-Centered Explainable AI: Towards a Reflective Sociotechnical Approach. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) 12424 LNCS (2020), 449\u2013466. https://doi.org/10.1007/978-3-030-60117-1_33 arxiv:2002.01092",
      "doi": ""
    },
    {
      "text": "Malin Eiband, Daniel Buschek, Alexander Kremer, and Heinrich Hussmann. 2019. The impact of placebic explanations on trust in intelligent systems. In Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems. 1\u20136.",
      "doi": "10.1145/3290607.3312787"
    },
    {
      "text": "European Parliament. 2021. Proposal for a REGULATION OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL LAYING DOWN HARMONISED RULES ON ARTIFICIAL INTELLIGENCE (ARTIFICIAL INTELLIGENCE ACT) AND AMENDING CERTAIN UNION LEGISLATIVE ACTS. https://eur-lex.europa.eu/legal-content/EN/TXT/?qid=1623335154975&uri=CELEX%3A52021PC0206",
      "doi": ""
    },
    {
      "text": "Wenjuan Fan, Jingnan Liu, Shuwan Zhu, and Panos\u00a0M Pardalos. 2018. Investigating the impacting factors for the healthcare professionals to adopt artificial intelligence-based medical diagnosis support system (AIMDSS). Annals of Operations Research(2018), 1\u201326.",
      "doi": ""
    },
    {
      "text": "Bhavya Ghai, Q\u00a0Vera Liao, Yunfeng Zhang, Rachel Bellamy, and Klaus Mueller. 2021. Explainable active learning (xal) toward ai explanations as interfaces for machine teachers. Proceedings of the ACM on Human-Computer Interaction 4, CSCW3(2021), 1\u201328.",
      "doi": "10.1145/3432934"
    },
    {
      "text": "Francesca Gino and Maurice\u00a0E Schweitzer. 2008. Take this advice and shove it. In Academy of Management Proceedings, Vol.\u00a02008. Academy of Management Briarcliff Manor, NY 10510, 1\u20135.",
      "doi": ""
    },
    {
      "text": "Kate Goddard, Abdul Roudsari, and Jeremy\u00a0C Wyatt. 2012. Automation bias: a systematic review of frequency, effect mediators, and mitigators. Journal of the American Medical Informatics Association 19, 1(2012), 121\u2013127.",
      "doi": ""
    },
    {
      "text": "Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Dino Pedreschi, and Fosca Giannotti. 2018. A Survey Of Methods For Explaining Black Box Models. ACM CSUR 51, 5, Article 93 (Aug. 2018), 42\u00a0pages.",
      "doi": ""
    },
    {
      "text": "David Gunning. 2017. Explainable artificial intelligence (xai). Defense Advanced Research Projects Agency (DARPA), nd Web 2 (2017).",
      "doi": ""
    },
    {
      "text": "Ronan Hamon, Henrik Junklewitz, Gianclaudio Malgieri, Paul\u00a0De Hert, Laurent Beslay, and Ignacio Sanchez. 2021. Impossible Explanations? Beyond explainable AI in the GDPR from a COVID-19 use case scenario. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. 549\u2013559.",
      "doi": "10.1145/3442188.3445917"
    },
    {
      "text": "Yukinori Harada, Shinichi Katsukura, Ren Kawamura, and Taro Shimizu. 2021. Effects of a Differential Diagnosis List of Artificial Intelligence on Differential Diagnoses by Physicians: An Exploratory Analysis of Data from a Randomized Controlled Study. International Journal of Environmental Research and Public Health 18, 11(2021), 5562.",
      "doi": ""
    },
    {
      "text": "Nigel Harvey and Ilan Fischer. 1997. Taking advice: Accepting help, improving judgment, and sharing responsibility. Organizational behavior and human decision processes 70, 2 (1997), 117\u2013133.",
      "doi": ""
    },
    {
      "text": "Steven\u00a0D Hillson, Donald\u00a0P Connelly, and Yuanli Liu. 1995. The effects of computer-assisted electrocardiographic interpretation on physicians\u2019 diagnostic decisions. Medical Decision Making 15, 2 (1995), 107\u2013112.",
      "doi": ""
    },
    {
      "text": "Robert\u00a0R Hoffman, Shane\u00a0T Mueller, Gary Klein, and Jordan Litman. 2018. Metrics for explainable AI: Challenges and prospects. arXiv preprint arXiv:1812.04608(2018).",
      "doi": ""
    },
    {
      "text": "Maia Jacobs, Melanie\u00a0F Pradier, Thomas\u00a0H McCoy, Roy\u00a0H Perlis, Finale Doshi-Velez, and Krzysztof\u00a0Z Gajos. 2021. How machine-learning recommendations influence clinician treatment selections: the example of antidepressant selection. Translational psychiatry 11, 1 (2021), 1\u20139.",
      "doi": ""
    },
    {
      "text": "C Krittanawong. 2018. The rise of artificial intelligence and the uncertain future for physicians. European journal of internal medicine 48 (2018), e13\u2013e14.",
      "doi": ""
    },
    {
      "text": "Himabindu Lakkaraju and Osbert Bastani. 2020. \u201dHow Do I Fool You?\u201d: Manipulating User Trust via Misleading Black Box Explanations(AIES \u201920). Association for Computing Machinery, New York, NY, USA, 79\u201385. https://doi.org/10.1145/3375627.3375833",
      "doi": "10.1145/3375627.3375833"
    },
    {
      "text": "Jean-Baptiste Lamy, Boomadevi Sekar, Gilles Guezennec, Jacques Bouaud, and Brigitte S\u00e9roussi. 2019. Explainable artificial intelligence for breast cancer: A visual case-based reasoning approach. Artificial intelligence in medicine 94 (2019), 42\u201353.",
      "doi": ""
    },
    {
      "text": "John\u00a0D Lee and Katrina\u00a0A See. 2004. Trust in automation: Designing for appropriate reliance. Human factors 46, 1 (2004), 50\u201380.",
      "doi": ""
    },
    {
      "text": "Ariel Levy, Monica Agrawal, Arvind Satyanarayan, and David Sontag. 2021. Assessing the Impact of Automated Suggestions on Decision Making: Domain Experts Mediate Model Errors but Take Less Initiative. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 1\u201313.",
      "doi": "10.1145/3411764.3445522"
    },
    {
      "text": "Q\u00a0Vera Liao, Daniel Gruen, and Sarah Miller. 2020. Questioning the AI: informing design practices for explainable AI user experiences. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. 1\u201315.",
      "doi": "10.1145/3313831.3376590"
    },
    {
      "text": "Thomas Lindow, Josefine Kron, Hans Thulesius, Erik Ljungstr\u00f6m, and Olle Pahlm. 2019. Erroneous computer-based interpretations of atrial fibrillation and atrial flutter in a Swedish primary health care setting. Scandinavian journal of primary health care 37, 4 (2019), 426\u2013433.",
      "doi": ""
    },
    {
      "text": "Jennifer\u00a0M Logg, Julia\u00a0A Minson, and Don\u00a0A Moore. 2019. Algorithm appreciation: People prefer algorithmic to human judgment. Organizational Behavior and Human Decision Processes 151 (2019), 90\u2013103.",
      "doi": ""
    },
    {
      "text": "Scott\u00a0M Lundberg and Su-In Lee. 2017. A unified approach to interpreting model predictions. In Proceedings of the 31st international conference on neural information processing systems. 4768\u20134777.",
      "doi": ""
    },
    {
      "text": "Gianclaudio Malgieri and Giovanni Comand\u00e9. 2017. Why a right to legibility of automated decision-making exists in the general data protection regulation. International Data Privacy Law(2017).",
      "doi": ""
    },
    {
      "text": "Vidushi Marda and Shivangi Narayan. 2021. On the importance of ethnographic methods in AI research. Nature Machine Intelligence 3, 3 (2021), 187\u2013189.",
      "doi": ""
    },
    {
      "text": "Carlo Metta, Riccardo Guidotti, Yuan Yin, Patrick Gallinari, and Salvatore Rinzivillo. 2021. Exemplars and Counterexemplars Explanations for Image Classifiers, Targeting Skin Lesion Labeling. In 2021 IEEE Symposium on Computers and Communications (ISCC). IEEE, 1\u20137.",
      "doi": ""
    },
    {
      "text": "Martijn Millecamp, Sidra Naveed, Katrien Verbert, and J\u00fcrgen Ziegler. [n.d.]. To Explain or Not to Explain: the Effects of Personal Characteristics When Explaining Feature-based Recommendations in Different Domains. Technical Report.",
      "doi": ""
    },
    {
      "text": "Tim Miller, Piers Howe, and Liz Sonenberg. 2017. Explainable AI: Beware of inmates running the asylum or: How I learnt to stop worrying and love the social and behavioural sciences. arXiv preprint arXiv:1712.00547(2017).",
      "doi": ""
    },
    {
      "text": "Christoph Molnar, Giuseppe Casalicchio, and Bernd Bischl. 2020. Interpretable machine learning\u2013a brief history, state-of-the-art and challenges. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, 417\u2013431.",
      "doi": ""
    },
    {
      "text": "Jessica Morley, Caio\u00a0CV Machado, Christopher Burr, Josh Cowls, Indra Joshi, Mariarosaria Taddeo, and Luciano Floridi. 2020. The ethics of AI in health care: A mapping review. Social Science & Medicine(2020), 113172.",
      "doi": ""
    },
    {
      "text": "Henrik Mucha, Sebastian Robert, Ruediger Breitschwerdt, and Michael Fellmann. 2021. Interfaces for Explanations in Human-AI Interaction: Proposing a Design Evaluation Approach. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems. 1\u20136.",
      "doi": "10.1145/3411763.3451759"
    },
    {
      "text": "W\u00a0James Murdoch, Chandan Singh, Karl Kumbier, Reza Abbasi-Asl, and Bin Yu. 2019. Definitions, methods, and applications in interpretable machine learning. Proceedings of the National Academy of Sciences 116, 44(2019), 22071\u201322080.",
      "doi": ""
    },
    {
      "text": "Emanuele Neri, Francesca Coppola, Vittorio Miele, Corrado Bibbolino, and Roberto Grassi. 2020. Artificial intelligence: Who is responsible for the diagnosis?",
      "doi": ""
    },
    {
      "text": "Mahsan Nourani, Joanie King, and Eric Ragan. 2020. The role of domain expertise in user trust and the impact of first impressions with intelligent systems. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, Vol.\u00a08. 112\u2013121.",
      "doi": ""
    },
    {
      "text": "Cecilia Panigutti, Riccardo Guidotti, Anna Monreale, and Dino Pedreschi. 2019. Explaining multi-label black-box classifiers for health applications. In International Workshop on Health Intelligence. Springer, 97\u2013110.",
      "doi": ""
    },
    {
      "text": "Cecilia Panigutti, Anna Monreale, Giovanni Comand\u00e9, and Dino Pedreschi. 2022. Ethical, societal and legal issues in deep learning for healthcare. In Deep Learning in Biology and Medicine. World Scientific Publishing.",
      "doi": ""
    },
    {
      "text": "Cecilia Panigutti, Alan Perotti, Andr\u00e9 Panisson, Paolo Bajardi, and Dino Pedreschi. 2021. FairLens: Auditing black-box clinical decision support systems. Information Processing & Management 58, 5 (2021), 102657.",
      "doi": "10.1016/j.ipm.2021.102657"
    },
    {
      "text": "Cecilia Panigutti, Alan Perotti, and Dino Pedreschi. 2020. Doctor XAI: an ontology-based approach to black-box sequential data classification explanations. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. 629\u2013639.",
      "doi": "10.1145/3351095.3372855"
    },
    {
      "text": "Michael\u00a0G Pratt. 2009. From the editors: For the lack of a boilerplate: Tips on writing up (and reviewing) qualitative research.",
      "doi": ""
    },
    {
      "text": "Marco\u00a0Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. \u201d Why should i trust you?\u201d Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. 1135\u20131144.",
      "doi": "10.1145/2939672.2939778"
    },
    {
      "text": "Philipp Schmidt and Felix Biessmann. 2020. Calibrating human-ai collaboration: Impact of risk, ambiguity and transparency on algorithmic bias. In International Cross-Domain Conference for Machine Learning and Knowledge Extraction. Springer, 431\u2013449.",
      "doi": ""
    },
    {
      "text": "Jessica\u00a0M Schwartz, Amanda\u00a0J Moy, Sarah\u00a0C Rossetti, No\u00e9mie Elhadad, and Kenrick\u00a0D Cato. 2021. Clinician involvement in research on machine learning\u2013based predictive clinical decision support for the hospital setting: A scoping review. Journal of the American Medical Informatics Association 28, 3(2021), 653\u2013663.",
      "doi": ""
    },
    {
      "text": "Lucy Shinners, Christina Aggar, Sandra Grace, and Stuart Smith. 2020. Exploring healthcare professionals\u2019 understanding and experiences of artificial intelligence technology use in the delivery of healthcare: an integrative review. Health informatics journal 26, 2 (2020), 1225\u20131236.",
      "doi": ""
    },
    {
      "text": "Linda\u00a0J Skitka, Kathleen\u00a0L Mosier, and Mark Burdick. 1999. Does automation bias decision-making?International Journal of Human-Computer Studies 51, 5 (1999), 991\u20131006.",
      "doi": ""
    },
    {
      "text": "Janet\u00a0A Sniezek and Timothy Buckley. 1995. Cueing and cognitive conflict in judge-advisor decision making. Organizational behavior and human decision processes 62, 2 (1995), 159\u2013174.",
      "doi": ""
    },
    {
      "text": "Janet\u00a0A Sniezek and Lyn\u00a0M Van\u00a0Swol. 2001. Trust, confidence, and expertise in a judge-advisor system. Organizational behavior and human decision processes 84, 2 (2001), 288\u2013307.",
      "doi": ""
    },
    {
      "text": "MT Spil and WR Schuring. 2006. E-Health Systems Diffusion and Use: The Innovation. The Users and the Use IT Model(2006).",
      "doi": ""
    },
    {
      "text": "Lea Strohm, Charisma Hehakaya, Erik\u00a0R Ranschaert, Wouter\u00a0PC Boon, and Ellen\u00a0HM Moors. 2020. Implementation of artificial intelligence (AI) applications in radiology: hindering and facilitating factors. European radiology 30(2020), 5525\u20135532.",
      "doi": ""
    },
    {
      "text": "Sana Tonekaboni, Shalmali Joshi, Melissa\u00a0D McCradden, and Anna Goldenberg. 2019. What clinicians want: contextualizing explainable machine learning for clinical end use. In Machine Learning for Healthcare Conference. PMLR, 359\u2013380.",
      "doi": ""
    },
    {
      "text": "Eric Topol. 2019. Deep medicine: how artificial intelligence can make healthcare human again. Hachette UK.",
      "doi": ""
    },
    {
      "text": "Viswanath Venkatesh and Hillol Bala. 2008. Technology acceptance model 3 and a research agenda on interventions. Decision sciences 39, 2 (2008), 273\u2013315.",
      "doi": ""
    },
    {
      "text": "Viswanath Venkatesh, Michael\u00a0G Morris, Gordon\u00a0B Davis, and Fred\u00a0D Davis. 2003. User acceptance of information technology: Toward a unified view. MIS quarterly (2003), 425\u2013478.",
      "doi": "10.5555/2017197.2017202"
    },
    {
      "text": "Himanshu Verma, Roger Schaer, Julien Reichenbach, Jreige Mario, John\u00a0O Prior, Florian Ev\u00e9quoz, and Adrien\u00a0Rapha\u00ebl Depeursinge. 2021. On Improving Physicians\u2019 Trust in AI: Qualitative Inquiry with Imaging Experts in the Oncological Domain. (2021).",
      "doi": ""
    },
    {
      "text": "Xinru Wang and Ming Yin. 2021. Are Explanations Helpful? A Comparative Study of the Effects of Explanations in AI-Assisted Decision-Making. In 26th International Conference on Intelligent User Interfaces (College Station, TX, USA) (IUI \u201921). Association for Computing Machinery, 318\u2013328.",
      "doi": "10.1145/3397481.3450650"
    },
    {
      "text": "Yao Xie, Melody Chen, David Kao, Ge Gao, and Xiang\u2019Anthony\u2019 Chen. 2020. CheXplain: Enabling Physicians to Explore and Understand Data-Driven, AI-Enabled Medical Imaging Analysis. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. 1\u201313.",
      "doi": "10.1145/3313831.3376807"
    },
    {
      "text": "Kun Yu, Shlomo Berkovsky, Dan Conway, Ronnie Taib, Jianlong Zhou, and Fang Chen. 2016. Trust and reliance based on system accuracy. In Proceedings of the 2016 Conference on User Modeling Adaptation and Personalization. 223\u2013227.",
      "doi": "10.1145/2930238.2930290"
    },
    {
      "text": "Muhan Zhang, Christopher\u00a0R King, Michael Avidan, and Yixin Chen. 2020. Hierarchical Attention Propagation for Healthcare Representation Learning. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 249\u2013256.",
      "doi": "10.1145/3394486.3403067"
    },
    {
      "text": "Yunfeng Zhang, Q\u00a0Vera Liao, and Rachel\u00a0KE Bellamy. 2020. Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. 295\u2013305.",
      "doi": "10.1145/3351095.3372852"
    }
  ]
}