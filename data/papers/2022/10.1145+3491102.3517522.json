{
  "doi": "10.1145/3491102.3517522",
  "title": "Debiased-CAM to mitigate image perturbations with faithful visual explanations of machine learning",
  "published": "2022-04-29",
  "proctitle": "CHI '22: Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems",
  "pages": "1-32",
  "year": 2022,
  "badges": [],
  "abstract": "Model explanations such as saliency maps can improve user trust in AI by highlighting important features for a prediction. However, these become distorted and misleading when explaining predictions of images that are subject to systematic error (bias) by perturbations and corruptions. Furthermore, the distortions persist despite model fine-tuning on images biased by different factors (blur, color temperature, day/night). We present Debiased-CAM to recover explanation faithfulness across various bias types and levels by training a multi-input, multi-task model with auxiliary tasks for explanation and bias level predictions. In simulation studies, the approach not only enhanced prediction accuracy, but also generated highly faithful explanations about these predictions as if the images were unbiased. In user studies, debiased explanations improved user task performance, perceived truthfulness and perceived helpfulness. Debiased training can provide a versatile platform for robust performance and explanation faithfulness for a wide range of applications with data biases.",
  "authors": [
    {
      "name": "Wencan Zhang",
      "institution": "Department of Computer Science, National University of Singapore, Singapore",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99660236105",
      "orcid": "missing"
    },
    {
      "name": "Mariella Dimiccoli",
      "institution": "Institut de Rob\u00f2tica i Inform\u00e0tica Industrial, CSIC-UPC, Spain",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81440605454",
      "orcid": "missing"
    },
    {
      "name": "Brian Y Lim",
      "institution": "Department of Computer Science, National University of Singapore, Singapore",
      "img": "/do/10.1145/contrib-81416601689/rel-imgonly/brian_lim_2015_-_square.jpg",
      "acmid": "81416601689",
      "orcid": "0000-0002-0543-2414"
    }
  ],
  "references": [
    {
      "text": "Ashraf Abdul, Jo Vermeulen, Danding Wang, Brian\u00a0Y Lim, and Mohan Kankanhalli. 2018. Trends and trajectories for explainable, accountable and intelligible systems: An hci research agenda. In Proceedings of the 2018 CHI conference on human factors in computing systems. 1\u201318.",
      "doi": "10.1145/3173574.3174156"
    },
    {
      "text": "Ashraf Abdul, Christian von\u00a0der Weth, Mohan Kankanhalli, and Brian\u00a0Y Lim. 2020. COGAM: Measuring and Moderating Cognitive Load in Machine Learning Model Explanations. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. 1\u201314.",
      "doi": "10.1145/3313831.3376615"
    },
    {
      "text": "Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim. 2018. Sanity checks for saliency maps. In Advances in Neural Information Processing Systems. 9505\u20139515.",
      "doi": ""
    },
    {
      "text": "Mahmoud Afifi and Michael\u00a0S Brown. 2019. What else can fool deep learning? Addressing color constancy errors on deep neural network performance. In Proceedings of the IEEE International Conference on Computer Vision. 243\u2013252.",
      "doi": ""
    },
    {
      "text": "Ahmed Alqaraawi, Martin Schuessler, Philipp Wei\u00df, Enrico Costanza, and Nadia Berthouze. 2020. Evaluating saliency map explanations for convolutional neural networks: a user study. In Proceedings of the 25th International Conference on Intelligent User Interfaces. 275\u2013285.",
      "doi": "10.1145/3377325.3377519"
    },
    {
      "text": "Ronald Angel and William Gronfein. 1988. The use of subjective information in statistical models. American Sociological Review(1988), 464\u2013473.",
      "doi": ""
    },
    {
      "text": "Daniel Avrahami, James Fogarty, and Scott\u00a0E Hudson. 2007. Biases in human estimation of interruptibility: effects and implications for practice. In Proceedings of the SIGCHI conference on Human factors in computing systems. 50\u201360.",
      "doi": "10.1145/1240624.1240632"
    },
    {
      "text": "Sebastian Bach, Alexander Binder, Gr\u00e9goire Montavon, Frederick Klauschen, Klaus-Robert M\u00fcller, and Wojciech Samek. 2015. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS one 10, 7 (2015), e0130140.",
      "doi": ""
    },
    {
      "text": "Gagan Bansal, Tongshuang Wu, Joyce Zhou, Raymond Fok, Besmira Nushi, Ece Kamar, Marco\u00a0Tulio Ribeiro, and Daniel Weld. 2021. Does the whole exceed its parts? the effect of ai explanations on complementary team performance. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 1\u201316.",
      "doi": "10.1145/3411764.3445717"
    },
    {
      "text": "David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. 2017. Network dissection: Quantifying interpretability of deep visual representations. In Proceedings of the IEEE conference on computer vision and pattern recognition. 6541\u20136549.",
      "doi": ""
    },
    {
      "text": "Zoya Bylinskii, Tilke Judd, Aude Oliva, Antonio Torralba, and Fr\u00e9do Durand. 2018. What do different evaluation metrics tell us about saliency models?IEEE transactions on pattern analysis and machine intelligence 41, 3(2018), 740\u2013757.",
      "doi": ""
    },
    {
      "text": "Mitchell Charity. [n.d.]. What color is a blackbody? - some pixel rgb values.http://www.vendian.org/mncharity/dir3/blackbody/.",
      "doi": ""
    },
    {
      "text": "Aditya Chattopadhay, Anirban Sarkar, Prantik Howlader, and Vineeth\u00a0N Balasubramanian. 2018. Grad-cam++: Generalized gradient-based visual explanations for deep convolutional networks. In 2018 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 839\u2013847.",
      "doi": ""
    },
    {
      "text": "Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A simple framework for contrastive learning of visual representations. In International conference on machine learning. PMLR, 1597\u20131607.",
      "doi": ""
    },
    {
      "text": "Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, and C\u00a0Lawrence Zitnick. 2015. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325(2015).",
      "doi": ""
    },
    {
      "text": "Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jos\u00e9\u00a0MF Moura, Devi Parikh, and Dhruv Batra. 2017. Visual dialog. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 326\u2013335.",
      "doi": ""
    },
    {
      "text": "Robert\u00a0G Davis and Dolores\u00a0N Ginthner. 1990. Correlated color temperature, illuminance level, and the Kruithof curve. Journal of the Illuminating Engineering Society 19, 1 (1990), 27\u201338.",
      "doi": ""
    },
    {
      "text": "Maria De-Arteaga, Riccardo Fogliato, and Alexandra Chouldechova. 2020. A case for humans-in-the-loop: Decisions in the presence of erroneous algorithmic scores. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. 1\u201312.",
      "doi": "10.1145/3313831.3376638"
    },
    {
      "text": "Adeline Delavande and Susann Rohwedder. 2008. Eliciting subjective probabilities in Internet surveys. Public Opinion Quarterly 72, 5 (2008), 866\u2013891.",
      "doi": ""
    },
    {
      "text": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition. Ieee, 248\u2013255.",
      "doi": ""
    },
    {
      "text": "Mariella Dimiccoli, Juan Mar\u00edn, and Edison Thomaz. 2018. Mitigating bystander privacy concerns in egocentric activity recognition with deep learning and intentional image degradation. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 1, 4 (2018), 1\u201318.",
      "doi": "10.1145/3161190"
    },
    {
      "text": "Jonathan Dodge, Q\u00a0Vera Liao, Yunfeng Zhang, Rachel\u00a0KE Bellamy, and Casey Dugan. 2019. Explaining models: an empirical study of how explanations impact fairness judgment. In Proceedings of the 24th international conference on intelligent user interfaces. 275\u2013285.",
      "doi": "10.1145/3301275.3302310"
    },
    {
      "text": "Ann-Kathrin Dombrowski, Maximillian Alber, Christopher Anders, Marcel Ackermann, Klaus-Robert M\u00fcller, and Pan Kessel. 2019. Explanations can be manipulated and geometry is to blame. Advances in Neural Information Processing Systems 32 (2019), 13589\u201313600.",
      "doi": ""
    },
    {
      "text": "Ann-Kathrin Dombrowski, Christopher\u00a0J. Anders, Klaus-Robert M\u00fcller, and Pan Kessel. 2022. Towards Robust Explanations for Deep Neural Networks. Pattern Recognit. 121(2022), 108194.",
      "doi": "10.1016/j.patcog.2021.108194"
    },
    {
      "text": "Finale Doshi-Velez and Been Kim. 2017. Towards a rigorous science of interpretable machine learning. arXiv preprint arXiv:1702.08608(2017).",
      "doi": ""
    },
    {
      "text": "Mengnan Du, Ninghao Liu, and Xia Hu. 2019. Techniques for interpretable machine learning. Commun. ACM 63, 1 (2019), 68\u201377.",
      "doi": "10.1145/3359786"
    },
    {
      "text": "Upol Ehsan, Q\u00a0Vera Liao, Michael Muller, Mark\u00a0O Riedl, and Justin\u00a0D Weisz. 2021. Expanding explainability: Towards social transparency in ai systems. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 1\u201319.",
      "doi": "10.1145/3411764.3445188"
    },
    {
      "text": "Gabriel Erion, Joseph\u00a0D Janizek, Pascal Sturmfels, Scott\u00a0M Lundberg, and Su-In Lee. 2021. Improving performance of deep learning models with axiomatic attribution priors and expected gradients. Nature Machine Intelligence(2021), 1\u201312.",
      "doi": ""
    },
    {
      "text": "Andre Esteva, Brett Kuprel, Roberto\u00a0A Novoa, Justin Ko, Susan\u00a0M Swetter, Helen\u00a0M Blau, and Sebastian Thrun. 2017. Dermatologist-level classification of skin cancer with deep neural networks. nature 542, 7639 (2017), 115\u2013118.",
      "doi": ""
    },
    {
      "text": "Ruth\u00a0C Fong and Andrea Vedaldi. 2017. Interpretable explanations of black boxes by meaningful perturbation. In Proceedings of the IEEE International Conference on Computer Vision. 3429\u20133437.",
      "doi": ""
    },
    {
      "text": "Amirata Ghorbani, Abubakar Abid, and James Zou. 2019. Interpretation of neural networks is fragile. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol.\u00a033. 3681\u20133688.",
      "doi": "10.1609/aaai.v33i01.33013681"
    },
    {
      "text": "Daniel\u00a0G Goldstein and David Rothschild. 2014. Lay understanding of probability distributions.Judgment & Decision Making 9, 1 (2014).",
      "doi": ""
    },
    {
      "text": "Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti, and Dino Pedreschi. 2018. A survey of methods for explaining black box models. ACM computing surveys (CSUR) 51, 5 (2018), 1\u201342.",
      "doi": ""
    },
    {
      "text": "Cathal Gurrin, Hideo Joho, Frank Hopfgartner, Liting Zhou, and Rami Albatal. 2016. Ntcir lifelog: The first test collection for lifelog research. In Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval. 705\u2013708.",
      "doi": "10.1145/2911451.2914680"
    },
    {
      "text": "Leif Hancox-Li. 2020. Robustness in machine learning explanations: does it matter?. In Proceedings of the 2020 conference on fairness, accountability, and transparency. 640\u2013647.",
      "doi": "10.1145/3351095.3372836"
    },
    {
      "text": "Dan Hendrycks and Thomas Dietterich. 2019. Benchmarking neural network robustness to common corruptions and perturbations. arXiv preprint arXiv:1903.12261(2019).",
      "doi": ""
    },
    {
      "text": "Dan Hendrycks, Norman Mu, Ekin\u00a0D. Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. 2020. AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty. Proceedings of the International Conference on Learning Representations (ICLR) (2020).",
      "doi": ""
    },
    {
      "text": "Fred Hohman, Minsuk Kahng, Robert Pienta, and Duen\u00a0Horng Chau. 2018. Visual analytics in deep learning: An interrogative survey for the next frontiers. IEEE transactions on visualization and computer graphics 25, 8(2018), 2674\u20132693.",
      "doi": "10.1109/TVCG.2018.2843369"
    },
    {
      "text": "Fred Hohman, Haekyu Park, Caleb Robinson, and Duen Horng\u00a0Polo Chau. 2019. S ummit: Scaling deep learning interpretability by visualizing activation and attribution summarizations. IEEE transactions on visualization and computer graphics 26, 1(2019), 1096\u20131106.",
      "doi": ""
    },
    {
      "text": "Jeremy Howard. [n.d.]. The imagenette dataset.https://github.com/fastai/imagenette. Github.",
      "doi": ""
    },
    {
      "text": "Minsuk Kahng, Pierre\u00a0Y Andrews, Aditya Kalro, and Duen\u00a0Horng Chau. 2017. ActiVis: Visual exploration of industry-scale deep neural network models. IEEE transactions on visualization and computer graphics 24, 1(2017), 88\u201397.",
      "doi": ""
    },
    {
      "text": "Harmanpreet Kaur, Harsha Nori, Samuel Jenkins, Rich Caruana, Hanna\u00a0M. Wallach, and Jennifer\u00a0Wortman Vaughan. 2020. Interpreting Interpretability: Understanding Data Scientists\u2019 Use of Interpretability Tools for Machine Learning. Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (2020).",
      "doi": "10.1145/3313831.3376219"
    },
    {
      "text": "Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, 2018. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). In International conference on machine learning. PMLR, 2668\u20132677.",
      "doi": ""
    },
    {
      "text": "Jacob Kittley-Davies, Ahmed Alqaraawi, Rayoung Yang, Enrico Costanza, Alex Rogers, and Sebastian Stein. 2019. Evaluating the effect of feedback from different computer vision processing stages: a comparative lab study. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. 1\u201312.",
      "doi": "10.1145/3290605.3300273"
    },
    {
      "text": "Ren\u00e9\u00a0F Kizilcec. 2016. How much information? Effects of transparency on trust in an algorithmic interface. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems. 2390\u20132395.",
      "doi": "10.1145/2858036.2858402"
    },
    {
      "text": "Pang\u00a0Wei Koh, Thao Nguyen, Yew\u00a0Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and Percy Liang. 2020. Concept bottleneck models. In International Conference on Machine Learning. PMLR, 5338\u20135348.",
      "doi": ""
    },
    {
      "text": "Nikos Komodakis and Sergey Zagoruyko. 2017. Paying more attention to attention: improving the performance of convolutional neural networks via attention transfer. In ICLR.",
      "doi": ""
    },
    {
      "text": "James Konow. 2005. Blind spots: The effects of information and stakes on fairness bias and dispersion. Social Justice Research 18, 4 (2005), 349\u2013390.",
      "doi": ""
    },
    {
      "text": "Alex Krizhevsky, Ilya Sutskever, and Geoffrey\u00a0E Hinton. 2012. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems 25 (2012), 1097\u20131105.",
      "doi": ""
    },
    {
      "text": "Orest Kupyn, Volodymyr Budzan, Mykola Mykhailych, Dmytro Mishkin, and Ji\u0159\u00ed Matas. 2018. Deblurgan: Blind motion deblurring using conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition. 8183\u20138192.",
      "doi": ""
    },
    {
      "text": "Pierre-Yves Laffont, Zhile Ren, Xiaofeng Tao, Chao Qian, and James Hays. 2014. Transient attributes for high-level understanding and editing of outdoor scenes. ACM Transactions on graphics (TOG) 33, 4 (2014), 1\u201311.",
      "doi": ""
    },
    {
      "text": "Isaac Lage, Emily Chen, Jeffrey He, Menaka Narayanan, Been Kim, Sam Gershman, and Finale Doshi-Velez. 2019. An evaluation of the human-interpretability of explanation. arXiv preprint arXiv:1902.00006(2019).",
      "doi": ""
    },
    {
      "text": "Isaac Lage, Andrew\u00a0Slavin Ross, Been Kim, Samuel\u00a0J Gershman, and Finale Doshi-Velez. 2018. Human-in-the-loop interpretability prior. Advances in neural information processing systems 31 (2018).",
      "doi": ""
    },
    {
      "text": "Himabindu Lakkaraju and Osbert Bastani. 2020. \u201d How do I fool you?\u201d Manipulating User Trust via Misleading Black Box Explanations. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society. 79\u201385.",
      "doi": "10.1145/3375627.3375833"
    },
    {
      "text": "Markus Langer, Daniel Oster, Timo Speith, Holger Hermanns, Lena K\u00e4stner, Eva Schmidt, Andreas Sesing, and Kevin Baum. 2021. What do we want from Explainable Artificial Intelligence (XAI)?\u2013A stakeholder perspective on XAI and a conceptual model guiding interdisciplinary XAI research. Artificial Intelligence 296 (2021), 103473.",
      "doi": ""
    },
    {
      "text": "Jia Li, Changqun Xia, Yafei Song, Shu Fang, and Xiaowu Chen. 2015. A data-driven metric for comprehensive evaluation of saliency models. In Proceedings of the IEEE international conference on computer vision. 190\u2013198.",
      "doi": "10.1109/ICCV.2015.30"
    },
    {
      "text": "Kunpeng Li, Ziyan Wu, Kuan-Chuan Peng, Jan Ernst, and Yun Fu. 2018. Tell me where to look: Guided attention inference network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 9215\u20139223.",
      "doi": ""
    },
    {
      "text": "Brian\u00a0Y Lim and Anind\u00a0K Dey. 2011. Investigating intelligibility for uncertain context-aware applications. In Proceedings of the 13th international conference on Ubiquitous computing. 415\u2013424.",
      "doi": "10.1145/2030112.2030168"
    },
    {
      "text": "Brian\u00a0Y Lim, Anind\u00a0K Dey, and Daniel Avrahami. 2009. Why and why not explanations improve the intelligibility of context-aware intelligent systems. In Proceedings of the SIGCHI conference on human factors in computing systems. 2119\u20132128.",
      "doi": "10.1145/1518701.1519023"
    },
    {
      "text": "Zachary\u00a0C Lipton. 2018. The Mythos of Model Interpretability: In machine learning, the concept of interpretability is both important and slippery.Queue 16, 3 (2018), 31\u201357.",
      "doi": "10.1145/3236386.3241340"
    },
    {
      "text": "Scott\u00a0M Lundberg and Su-In Lee. 2017. A unified approach to interpreting model predictions. In Advances in neural information processing systems. 4765\u20134774.",
      "doi": ""
    },
    {
      "text": "Ian McLoughlin, Haomin Zhang, Zhipeng Xie, Yan Song, and Wei Xiao. 2015. Robust sound event classification using deep neural networks. IEEE/ACM Transactions on Audio, Speech, and Language Processing 23, 3(2015), 540\u2013552.",
      "doi": "10.1109/TASLP.2015.2389618"
    },
    {
      "text": "Tim Miller. 2019. Explanation in artificial intelligence: Insights from the social sciences. Artificial intelligence 267 (2019), 1\u201338.",
      "doi": ""
    },
    {
      "text": "Michael\u00a0J Muller and Sandra Kogan. 2010. Grounded theory method in HCI and CSCW. Cambridge: IBM Center for Social Software 28, 2 (2010), 1\u201346.",
      "doi": ""
    },
    {
      "text": "Mahsan Nourani, Samia Kabir, Sina Mohseni, and Eric\u00a0D Ragan. 2019. The effects of meaningful and meaningless explanations on trust and perceived system accuracy in intelligent systems. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, Vol.\u00a07. 97\u2013105.",
      "doi": ""
    },
    {
      "text": "Chris Olah, Alexander Mordvintsev, and Ludwig Schubert. 2017. Feature visualization. Distill 2, 11 (2017), e7.",
      "doi": ""
    },
    {
      "text": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics. 311\u2013318.",
      "doi": ""
    },
    {
      "text": "Michael\u00a0I Posner, Charles\u00a0R Snyder, and R Solso. 2004. Attention and cognitive control. Cognitive psychology: Key readings 205 (2004).",
      "doi": ""
    },
    {
      "text": "Forough Poursabzi-Sangdeh, Daniel\u00a0G Goldstein, Jake\u00a0M Hofman, Jennifer\u00a0Wortman Wortman\u00a0Vaughan, and Hanna Wallach. 2021. Manipulating and measuring model interpretability. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 1\u201352.",
      "doi": "10.1145/3411764.3445315"
    },
    {
      "text": "Harish\u00a0Guruprasad Ramaswamy 2020. Ablation-CAM: Visual Explanations for Deep Convolutional Network via Gradient-free Localization. In The IEEE Winter Conference on Applications of Computer Vision. 983\u2013991.",
      "doi": ""
    },
    {
      "text": "Marco\u00a0Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. \u201d Why should i trust you?\u201d Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. 1135\u20131144.",
      "doi": "10.1145/2939672.2939778"
    },
    {
      "text": "Neal\u00a0J. Roese and Kathleen\u00a0D. Vohs. 2012. Hindsight Bias. Perspectives on Psychological Science 7 (2012), 411 \u2013 426.",
      "doi": ""
    },
    {
      "text": "Andrew\u00a0Slavin Ross, Michael\u00a0C Hughes, and Finale Doshi-Velez. 2017. Right for the right reasons: training differentiable models by constraining their explanations. In Proceedings of the 26th International Joint Conference on Artificial Intelligence. 2662\u20132670.",
      "doi": ""
    },
    {
      "text": "Cynthia Rudin. 2019. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence 1, 5 (2019), 206\u2013215.",
      "doi": ""
    },
    {
      "text": "Michael Ryoo, Brandon Rothrock, Charles Fleming, and Hyun\u00a0Jong Yang. 2017. Privacy-preserving human activity recognition from extreme low resolution. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol.\u00a031.",
      "doi": ""
    },
    {
      "text": "Takaya Saito and Marc Rehmsmeier. 2015. The precision-recall plot is more informative than the ROC plot when evaluating binary classifiers on imbalanced datasets. PloS one 10, 3 (2015), e0118432.",
      "doi": ""
    },
    {
      "text": "Patrick Schramowski, Wolfgang Stammer, Stefano Teso, Anna Brugger, Franziska Herbert, Xiaoting Shao, Hans-Georg Luigs, Anne-Katrin Mahlein, and Kristian Kersting. 2020. Making deep neural networks right for the right scientific reasons by interacting with their explanations. Nature Machine Intelligence 2, 8 (2020), 476\u2013486.",
      "doi": ""
    },
    {
      "text": "Ramprasaath\u00a0R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. 2017. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE international conference on computer vision. 618\u2013626.",
      "doi": ""
    },
    {
      "text": "Ramprasaath\u00a0R Selvaraju, Stefan Lee, Yilin Shen, Hongxia Jin, Shalini Ghosh, Larry Heck, Dhruv Batra, and Devi Parikh. 2019. Taking a hint: Leveraging explanations to make vision and language models more grounded. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 2591\u20132600.",
      "doi": ""
    },
    {
      "text": "William\u00a0F Sharpe, Daniel\u00a0G Goldstein, and Phil\u00a0W Blythe. 2000. The distribution builder: A tool for inferring investor preferences. preprint (2000).",
      "doi": ""
    },
    {
      "text": "Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2014. Deep inside convolutional networks: Visualising image classification models and saliency maps. (2014).",
      "doi": ""
    },
    {
      "text": "Karen Simonyan and Andrew Zisserman. 2015. Very Deep Convolutional Networks for Large-Scale Image Recognition. In International Conference on Learning Representations.",
      "doi": ""
    },
    {
      "text": "Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic attribution for deep networks. arXiv preprint arXiv:1703.01365(2017).",
      "doi": ""
    },
    {
      "text": "Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. 2016. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition. 2818\u20132826.",
      "doi": ""
    },
    {
      "text": "Igor Vasiljevic, Ayan Chakrabarti, and Gregory Shakhnarovich. 2016. Examining the impact of blur on recognition by convolutional networks. arXiv preprint arXiv:1611.05760(2016).",
      "doi": ""
    },
    {
      "text": "Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2015. Show and tell: A neural image caption generator. In Proceedings of the IEEE conference on computer vision and pattern recognition. 3156\u20133164.",
      "doi": ""
    },
    {
      "text": "Danding Wang, Qian Yang, Ashraf Abdul, and Brian\u00a0Y Lim. 2019. Designing theory-driven user-centric explainable AI. In Proceedings of the 2019 CHI conference on human factors in computing systems. 1\u201315.",
      "doi": "10.1145/3290605.3300831"
    },
    {
      "text": "Danding Wang, Wencan Zhang, and Brian\u00a0Y Lim. 2021. Show or suppress? Managing input uncertainty in machine learning model explanations. Artificial Intelligence 294 (2021), 103456.",
      "doi": ""
    },
    {
      "text": "Haofan Wang, Zifan Wang, Mengnan Du, Fan Yang, Zijian Zhang, Sirui Ding, Piotr Mardziel, and Xia Hu. 2020. Score-CAM: Score-Weighted Visual Explanations for Convolutional Neural Networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. 24\u201325.",
      "doi": ""
    },
    {
      "text": "Xinru Wang and Ming Yin. 2021. Are Explanations Helpful? A Comparative Study of the Effects of Explanations in AI-Assisted Decision-Making. In 26th International Conference on Intelligent User Interfaces. 318\u2013328.",
      "doi": "10.1145/3397481.3450650"
    },
    {
      "text": "Fumeng Yang, Zhuanyi Huang, Jean Scholtz, and Dustin\u00a0L Arendt. 2020. How do visual explanations foster end users\u2019 appropriate trust in machine learning?. In Proceedings of the 25th International Conference on Intelligent User Interfaces. 189\u2013201.",
      "doi": "10.1145/3377325.3377480"
    },
    {
      "text": "Ming Yin, Jennifer Wortman\u00a0Vaughan, and Hanna Wallach. 2019. Understanding the effect of accuracy on trust in machine learning models. In Proceedings of the 2019 chi conference on human factors in computing systems. 1\u201312.",
      "doi": "10.1145/3290605.3300509"
    },
    {
      "text": "Matthew\u00a0D Zeiler and Rob Fergus. 2014. Visualizing and understanding convolutional networks. In European conference on computer vision. Springer, 818\u2013833.",
      "doi": ""
    },
    {
      "text": "Quanshi Zhang, Ying Nian\u00a0Wu, and Song-Chun Zhu. 2018. Interpretable convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 8827\u20138836.",
      "doi": ""
    },
    {
      "text": "Quan-shi Zhang and Song-Chun Zhu. 2018. Visual interpretability for deep learning: a survey. Frontiers of Information Technology & Electronic Engineering 19, 1(2018), 27\u201339.",
      "doi": ""
    },
    {
      "text": "Wencan Zhang and Brian\u00a0Y Lim. 2022. Towards Relatable Explainable AI with the Perceptual Process. Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems (2022).",
      "doi": ""
    },
    {
      "text": "Xinyang Zhang, Ningfei Wang, Hua Shen, Shouling Ji, Xiapu Luo, and Ting Wang. 2020. Interpretable deep learning under fire. In 29th {USENIX} Security Symposium ({USENIX} Security 20).",
      "doi": ""
    },
    {
      "text": "Yunfeng Zhang, Q\u00a0Vera Liao, and Rachel\u00a0KE Bellamy. 2020. Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. 295\u2013305.",
      "doi": "10.1145/3351095.3372852"
    },
    {
      "text": "Zijian Zhang, Jaspreet Singh, Ujwal Gadiraju, and Avishek Anand. 2019. Dissonance between human and machine understanding. Proceedings of the ACM on Human-Computer Interaction 3, CSCW(2019), 1\u201323.",
      "doi": "10.1145/3359158"
    },
    {
      "text": "Xuejun Zhao, Wencan Zhang, Xiaokui Xiao, and Brian Lim. 2021. Exploiting Explanations for Model Inversion Attacks. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 682\u2013692.",
      "doi": ""
    },
    {
      "text": "Stephan Zheng, Yang Song, Thomas Leung, and Ian\u00a0J. Goodfellow. 2016. Improving the Robustness of Deep Neural Networks via Stability Training. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016), 4480\u20134488.",
      "doi": ""
    },
    {
      "text": "Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. 2016. Learning deep features for discriminative localization. In Proceedings of the IEEE conference on computer vision and pattern recognition. 2921\u20132929.",
      "doi": ""
    },
    {
      "text": "Bolei Zhou, Yiyou Sun, David Bau, and Antonio Torralba. 2018. Interpretable basis decomposition for visual explanation. In Proceedings of the European Conference on Computer Vision (ECCV). 119\u2013134.",
      "doi": "10.1007/978-3-030-01237-3_8"
    }
  ]
}