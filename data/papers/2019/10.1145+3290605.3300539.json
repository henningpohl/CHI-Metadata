{
  "doi": "10.1145/3290605.3300539",
  "title": "VoiceAssist: Guiding Users to High-Quality Voice Recordings",
  "published": "2019-05-02",
  "proctitle": "CHI '19: Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems",
  "pages": "1-6",
  "year": 2019,
  "badges": [],
  "abstract": "Voice recording is a challenging task with many pitfalls due to sub-par recording environments, mistakes in recording setup, microphone quality, etc. Newcomers to voice recording often have difficulty recording their voice, leading to recordings with low sound quality. Many amateur recordings of poor quality have two key problems: too much reverberation (echo), and too much background noise (e.g. fans, electronics, street noise). We present VoiceAssist, a system that helps inexperienced users produce high quality recordings by providing real-time visual feedback on audio quality. We integrate modern audio quality measures into an interactive human-machine feedback loop, so that the audio quality can be maximized at capture-time. We demonstrate the utility of this feedback for improving the recording quality with a user study. When presented with visual feedback about recording quality, users produced recordings that were strongly preferred by third-party listeners, when compared to recordings made without this feedback.",
  "tags": [
    "interfaces",
    "creativity support tools",
    "speech",
    "narration",
    "active capture",
    "feedback",
    "voice recording",
    "audio quality"
  ],
  "authors": [
    {
      "name": "Prem Seetharaman",
      "institution": "Northwestern University, Evanston, IL, USA",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99658646609",
      "orcid": "missing"
    },
    {
      "name": "Gautham Mysore",
      "institution": "Adobe Research, San Francisco, CA, USA",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81418598351",
      "orcid": "missing"
    },
    {
      "name": "Bryan Pardo",
      "institution": "Northwestern University, Evanston, IL, USA",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81452611845",
      "orcid": "missing"
    },
    {
      "name": "Paris Smaragdis",
      "institution": "University of Illinois at Urbana Champaign, Urbana, IL, USA",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81100524250",
      "orcid": "missing"
    },
    {
      "name": "Celso Gomes",
      "institution": "Adobe Research, Seattle, WA, USA",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659366606",
      "orcid": "missing"
    }
  ],
  "references": [
    {
      "text": "Michael Berouti, Richard Schwartz, and John Makhoul. 1979. Enhancement of speech corrupted by acoustic noise. In Acoustics, Speech, and Signal Processing, IEEE International Conference on ICASSP'79., Vol. 4. IEEE, 208--211.",
      "doi": ""
    },
    {
      "text": "JS Bradley, R Reich, and SG Norcross. 1999. A just noticeable difference in C 50 for speech. Applied Acoustics 58, 2 (1999), 99--108.",
      "doi": ""
    },
    {
      "text": "Scott Carter, John Adcock, John Doherty, and Stacy Branham. 2010. NudgeCam: Toward targeted, higher quality media capture. In Proceedings of the 18th ACM international conference on Multimedia. ACM, 615--618.  ",
      "doi": "10.1145/1873951.1874034"
    },
    {
      "text": "Mark Cartwright, Bryan Pardo, Gautham J Mysore, and Matt Hoffman. 2016. Fast and easy crowdsourced perceptual audio evaluation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on. IEEE, 619--623.",
      "doi": "10.1109/ICASSP.2016.7471749"
    },
    {
      "text": "Ana Ram\u00edrez Chang and Marc Davis. 2005. Designing systems that direct human action. In CHI'05 Extended Abstracts on Human Factors in Computing Systems. ACM, 1260--1263.  ",
      "doi": "10.1145/1056808.1056891"
    },
    {
      "text": "Marc Davis. 2003. Active capture: integrating human-computer interaction and computer vision/audition to automate media capture. In Multimedia and Expo, 2003. ICME'03. Proceedings. 2003 International Conference on, Vol. 2. IEEE, II--185. ",
      "doi": "10.5555/1153922.1154495"
    },
    {
      "text": "Jeffrey Heer, Nathaniel S Good, Ana Ramirez, Marc Davis, and Jennifer Mankoff. 2004. Presiding over accidents: system direction of human action. In Proceedings of the SIGCHI Conference on human factors in computing systems. ACM, 463--470.  ",
      "doi": "10.1145/985692.985751"
    },
    {
      "text": "T Houtgast and H JMi Steeneken. 1973. The modulation transfer function in room acoustics as a predictor of speech intelligibility. Acta Acustica United With Acustica 28, 1 (1973), 66--73.",
      "doi": ""
    },
    {
      "text": "Alan B Johnston and Daniel C Burnett. 2012. WebRTC: APIs and RTCWEB protocols of the HTML5 real-time web. Digital Codex LLC. ",
      "doi": "10.5555/2432294"
    },
    {
      "text": "Kazutaka Kurihara, Masataka Goto, Jun Ogata, Yosuke Matsusaka, and Takeo Igarashi. 2007. Presentation sensei: a presentation training system using speech and image processing. In Proceedings of the 9th international conference on Multimodal interfaces. ACM, 358--365.  ",
      "doi": "10.1145/1322192.1322256"
    },
    {
      "text": "Philipos C Loizou. 2007. Speech enhancement: theory and practice. CRC press. ",
      "doi": "10.5555/2484638"
    },
    {
      "text": "Patrick A Naylor and Nikolay D Gaubitch. 2010. Speech dereverberation. Springer Science & Business Media. ",
      "doi": "10.5555/1892051"
    },
    {
      "text": "Santiago Pascual, Antonio Bonafonte, and Joan Serra. 2017. SEGAN: Speech enhancement generative adversarial network. arXiv preprint arXiv:1703.09452 (2017).",
      "doi": ""
    },
    {
      "text": "Cyril Plapous, Claude Marro, and Pascal Scalart. 2006. Improved signalto-noise ratio estimation for speech enhancement. IEEE Transactions on Audio, Speech, and Language Processing 14, 6 (2006), 2098--2108.  ",
      "doi": "10.1109/TASL.2006.872621"
    },
    {
      "text": "Yogesh Singh Rawat and Mohan S Kankanhalli. 2017. Clicksmart: A context-aware viewpoint recommendation system for mobile photography. IEEE Transactions on Circuits and Systems for Video Technology 27, 1 (2017), 149--158.  ",
      "doi": "10.1109/TCSVT.2016.2555658"
    },
    {
      "text": "Antony W Rix, John G Beerends, Michael P Hollier, and Andries P Hekstra. 2001. Perceptual evaluation of speech quality (PESQ)-a new method for speech quality assessment of telephone networks and codecs. In Acoustics, Speech, and Signal Processing, 2001. Proceedings. (ICASSP'01), Vol. 2. IEEE, 749--752.  ",
      "doi": "10.1109/ICASSP.2001.941023"
    },
    {
      "text": "Steve Rubin, Floraine Berthouzoz, Gautham J Mysore, and Maneesh Agrawala. 2015. Capture-time feedback for recording scripted narration. In Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology. ACM, 191--199.  ",
      "doi": "10.1145/2807442.2807464"
    },
    {
      "text": "Prem Seetharaman, Gautham Mysore, Paris Smaragdis, and Bryan Pardo. 2018. Blind Estimation of the Speech Transmission Index for Speech Quality Prediction. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018.",
      "doi": ""
    },
    {
      "text": "Mike Senior. 2018. How can I remove background noise from a voice recording? (Oct 2018). https://www.soundonsound.com/sound-advice/ q-how-can-i-remove-background-noise-voice-recording",
      "doi": ""
    },
    {
      "text": "Jongseo Sohn, Nam Soo Kim, and Wonyong Sung. 1999. A statistical model-based voice activity detection. IEEE signal processing letters 6, 1 (1999), 1--3.",
      "doi": ""
    },
    {
      "text": "Cees H Taal, Richard C Hendriks, Richard Heusdens, and Jesper Jensen. 2010. A short-time objective intelligibility measure for time-frequency weighted noisy speech. In Acoustics Speech and Signal Processing (ICASSP), 2010 IEEE International Conference on. IEEE, 4214--4217.",
      "doi": ""
    },
    {
      "text": "Thilo Thiede, William C Treurniet, Roland Bitto, Christian Schmidmer, Thomas Sporer, John G Beerends, and Catherine Colomes. 2000. PEAQ-The ITU standard for objective measurement of perceived audio quality. Journal of the Audio Engineering Society 48, 1/2 (2000), 3--29.",
      "doi": ""
    },
    {
      "text": "Masashi Unoki, Kyohei Sasaki, Ryota Miyauchi, Masato Akagi, and Nam Soo Kim. 2013. Blind method of estimating speech transmission index from reverberant speech signals. In Signal Processing Conference (EUSIPCO), 2013 Proceedings of the 21st European. IEEE, 1--5.",
      "doi": ""
    },
    {
      "text": "Xiong Xiao, Shengkui Zhao, Xionghu Zhong, Douglas L Jones, Eng Siong Chng, and Haizhou Li. 2015. Learning to Estimate Reverberation Time in Noisy and Reverberant Rooms. In Sixteenth Annual Conference of the International Speech Communication Association.",
      "doi": ""
    }
  ]
}