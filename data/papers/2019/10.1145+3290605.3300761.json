{
  "doi": "10.1145/3290605.3300761",
  "title": "Cicero: Multi-Turn, Contextual Argumentation for Accurate Crowdsourcing",
  "published": "2019-05-02",
  "proctitle": "CHI '19: Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems",
  "pages": "1-14",
  "year": 2019,
  "badges": [],
  "abstract": "Traditional approaches for ensuring high quality crowdwork have failed to achieve high-accuracy on difficult problems. Aggregating redundant answers often fails on the hardest problems when the majority is confused. Argumentation has been shown to be effective in mitigating these drawbacks. However, existing argumentation systems only support limited interactions and show workers general justifications, not context-specific arguments targeted to their reasoning. This paper presents Cicero, a new workflow that improves crowd accuracy on difficult tasks by engaging workers in multi-turn, contextual discussions through real-time, synchronous argumentation. Our experiments show that compared to previous argumentation systems which only improve the average individual worker accuracy by 6.8 percentage points on the Relation Extraction domain, our workflow achieves 16.7 percentage point improvement. Furthermore, previous argumentation approaches don't apply to tasks with many possible answers; in contrast, Cicero works well in these cases, raising accuracy from 66.7% to 98.8% on the Codenames domain.",
  "authors": [
    {
      "name": "Quanze Chen",
      "institution": "University of Washington, Seattle, WA, USA",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659365511",
      "orcid": "missing"
    },
    {
      "name": "Jonathan Bragg",
      "institution": "University of Washington, Seattle, WA, USA",
      "img": "/do/10.1145/contrib-81456631418/rel-imgonly/headshot.jpeg",
      "acmid": "81456631418",
      "orcid": "0000-0001-5460-9047"
    },
    {
      "name": "Lydia B. Chilton",
      "institution": "Columbia University, New York, NY, USA",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659480701",
      "orcid": "0000-0002-1737-1276"
    },
    {
      "name": "Dan S. Weld",
      "institution": "University of Washington, Seattle, WA, USA",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659376950",
      "orcid": "0000-0002-3255-0109"
    }
  ],
  "references": [
    {
      "text": "Gabor Angeli, Julie Tibshirani, Jean Wu, and Christopher D. Manning. 2014. Combining Distant and Partial Supervision for Relation Extraction. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25--29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL, Alessandro Moschitti, Bo Pang, and Walter Daelemans (Eds.). ACL, 1556--1567. http://aclweb.org/anthology/D/D14/D14--1164.pdf",
      "doi": ""
    },
    {
      "text": "Michael S Bernstein, Greg Little, Robert C Miller, Bj\u00f6rn Hartmann, Mark S Ackerman, David R Karger, David Crowell, and Katrina Panovich. 2010. Soylent: a word processor with a crowd inside. In UIST '10 Proceedings of the 23nd annual ACM symposium on User interface software and technology. ACM Press, 313--322.  ",
      "doi": "10.1145/1866029.1866078"
    },
    {
      "text": "J. Bigham, C. Jayant, H. Ji, G. Little,, A. Miller, R. Miller, A. Tatarowicz, B. White, S. White, and T. Yeh. 2010. VizWiz: Nearly Real-Time Answers to Visual Questions. In UIST.  ",
      "doi": "10.1145/1866029.1866080"
    },
    {
      "text": "Jonathan Bragg, Mausam, and Daniel S. Weld. 2018. Sprout: CrowdPowered Task Design for Crowdsourcing. In UIST.  ",
      "doi": "10.1145/3242587.3242598"
    },
    {
      "text": "Justin Cheng, Jaime Teevan, and Michael S. Bernstein. 2015. Measuring Crowdsourcing Effort with Error-Time Curves. In CHI.  ",
      "doi": "10.1145/2702123.2702145"
    },
    {
      "text": "L. Chilton, G. Little, D. Edge, D. Weld, and J. Landay. 20 Cascade: Crowdsourcing Taxonomy Creation. In CHI '13. ACM Press, New York, NY, USA.  ",
      "doi": "10.1145/2470654.2466265"
    },
    {
      "text": "Peng Dai, Christopher H. Lin, Mausam, and Daniel S. Weld. 2013. POMDP-based control of workflows for crowdsourcing. Artificial Intelligence 202 (2013), 52--85.  ",
      "doi": "10.1016/j.artint.2013.06.002"
    },
    {
      "text": "A.P. Dawid and A. M. Skene. 1979. Maximum Likelihood Estimation of Observer Error-rates using the EM Algorithm. Applied Statistics 28, 1 (1979), 20--28.",
      "doi": ""
    },
    {
      "text": "Gianluca Demartini, Djellel Eddine Difallah, and Philippe Cudr\u00e9Mauroux. 2012. ZenCrowd: Leveraging Probabilistic Reasoning and Crowdsourcing Techniques for Large-scale Entity Linking. In Proceedings of the 21st International Conference on World Wide Web (WWW '12). ACM, New York, NY, USA, 469--478.  ",
      "doi": "10.1145/2187836.2187900"
    },
    {
      "text": "Jeff Donahue and Kristen Grauman. 2011. Annotator rationales for visual recognition. In ICCV 2011.  ",
      "doi": "10.1109/ICCV.2011.6126394"
    },
    {
      "text": "Steven Dow, Anand Pramod Kulkarni, Scott R. Klemmer, and Bj\u00f6rn Hartmann. 2012. Shepherding the crowd yields better work. In CSCW.  ",
      "doi": "10.1145/2145204.2145355"
    },
    {
      "text": "Ryan Drapeau, Lydia B. Chilton, Jonathan Bragg, and Daniel S. Weld. 2016. MicroTalk: Using Argumentation to Improve Crowdsourcing Accuracy.",
      "doi": ""
    },
    {
      "text": "Ralph Grishman. 1997. Information extraction: Techniques and challenges. In Information extraction a multidisciplinary approach to an emerging information technology. Springer, 10--27. ",
      "doi": "10.5555/645856.669801"
    },
    {
      "text": "Chien-Ju Ho and Ming Yin. 2018. Working in Pairs: Understanding the Effects of Worker Interactions in Crowdwork. CoRR abs/1810.09634 (2018).",
      "doi": ""
    },
    {
      "text": "Ting-Hao Kenneth Huang and Jeffrey P Bigham. 2017. A 10-MonthLong Deployment Study of On-Demand Recruiting for Low-Latency Crowdsourcing. In Proceedings of The fifth AAAI Conference on Human Computation and Crowdsourcing (HCOMP 2017).",
      "doi": ""
    },
    {
      "text": "Ting-Hao Kenneth Huang, Walter S Lasecki, Amos Azaria, and Jeffrey P Bigham. 2016. \" Is There Anything Else I Can Help You With?\" Challenges in Deploying an On-Demand Crowd-Powered Conversational Agent. In Fourth AAAI Conference on Human Computation and Crowdsourcing.",
      "doi": ""
    },
    {
      "text": "R. L. Rogers J. P. Kincaid, R. P. Fishburne Jr and B. S. Chissom. 1975. Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel. In Technical report, DTIC Document.",
      "doi": ""
    },
    {
      "text": "Ece Kamar, Severin Hacker, and Eric Horvitz. 2012. Combining human and machine intelligence in large-scale crowdsourcing. In Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems-Volume 1. International Foundation for Autonomous Agents and Multiagent Systems, 467--474. ",
      "doi": "10.5555/2343576.2343643"
    },
    {
      "text": "David R. Karger, Sewoong Oh, and Devavrat Shah. 201 Budgetoptimal Crowdsourcing using Low-rank Matrix Approximations. In Conference on Communication, Control, and Computing.",
      "doi": ""
    },
    {
      "text": "Masaki Kobayashi, Hiromi Morita, Masaki Matsubara, Nobuyuki Shimizu, and Atsuyuki Morishima. 2018. An Empirical Study on Short- and Long-Term Effects of Self-Correction in Crowdsourced Microtasks. In HCOMP.",
      "doi": ""
    },
    {
      "text": "Travis Kriplean, Jonathan T. Morgan, Deen Freelon, Alan Borning, and Lance Bennett. 2011. ConsiderIt: improving structured public deliberation. In CHI Extended Abstracts.  ",
      "doi": "10.1145/1979742.1979869"
    },
    {
      "text": "Anand Kulkarni, Matthew Can, and Bj\u00f6rn Hartmann. 2012. Collaboratively crowdsourcing workflows with Turkomatic. In CSCW. ACM Press, New York, New York, USA.  ",
      "doi": "10.1145/2145204.2145354"
    },
    {
      "text": "Chinmay Kulkarni, Koh Pang Wei, Huy Le, Daniel Chia, Kathryn Papadopoulos, Justin Cheng, Daphne Koller, and Scott R Klemmer. 2015. Peer and self assessment in massive online classes. In Design thinking research. Springer, 131--168.",
      "doi": ""
    },
    {
      "text": "Tianyi Li, Kurt Luther, and Chris North. 2018. CrowdIA: Solving Mysteries with Crowdsourced Sensemaking. In PACMHCI.  ",
      "doi": "10.1145/3274374"
    },
    {
      "text": "Christopher H. Lin, Mausam, and Daniel S. Weld. 2014. To Re(label), or Not To Re(label). In HCOMP.",
      "doi": ""
    },
    {
      "text": "Greg Little, Lydia B. Chilton, Max Goldman, and Robert C. Miller. 2009. TurKit: Tools for Iterative Tasks on Mechanical Turk. In Human Computation Workshop (HComp2009).  ",
      "doi": "10.1145/1600150.1600159"
    },
    {
      "text": "Angli Liu, Stephen Soderland, Jonathan Bragg, Christopher H. Lin, Xiao Ling, and Daniel S. Weld. 2016. Effective Crowd Annotation for Relation Extraction. In Proceedings of NAACL and HLT 2016.",
      "doi": ""
    },
    {
      "text": "Angli Liu, Stephen Soderland, Jonathan Bragg, Christopher H. Lin, Xiao Ling, and Daniel S. Weld. 2016. Effective Crowd Annotation for Relation Extraction. In Proceedings of NAACL and HLT 2016.",
      "doi": ""
    },
    {
      "text": "Andrew Mao, Yiling Chen, Krzysztof Z. Gajos, David C. Parkes, Ariel D. Procaccia, and Haoqi Zhang. 2012. TurkServer: Enabling Synchronous and Longitudinal Online Experiments. In The Fourth Workshop on Human Computation (HCOMP 2012).",
      "doi": ""
    },
    {
      "text": "Tyler McDonnell, Matthew Lease, Tamer Elsayad, and Mucahid Kutlu. 2016. Why Is That Relevant? Collecting Annotator Rationales for Relevance Judgments. In Proceedings of the 4th AAAI Conference on Human Computation and Crowdsourcing (HCOMP). 10.",
      "doi": ""
    },
    {
      "text": "Sarah Michaels, Catherine O'Connor, and Lauren B Resnick. 2008. Deliberative discourse idealized and realized: Accountable talk in the classroom and in civic life. Studies in philosophy and education 27, 4 (2008), 283--297.",
      "doi": ""
    },
    {
      "text": "Jon Noronha, Eric Hysen, Haoqi Zhang, and Krzysztof Z Gajos. 2011. Platemate: crowdsourcing nutritional analysis from food photographs. In Proceedings of the 24th annual ACM symposium on User interface software and technology. ACM, 1--12.  ",
      "doi": "10.1145/2047196.2047198"
    },
    {
      "text": "David Oleson, Alexander Sorokin, Greg P Laughlin, Vaughn Hester, John Le, and Lukas Biewald. 2011. Programmatic Gold: Targeted and Scalable Quality Assurance in Crowdsourcing.. In Human Computation Workshop. 11. ",
      "doi": "10.5555/2908698.2908706"
    },
    {
      "text": "Maria Pershina, Bonan Min, Wei Xu, and Ralph Grishman. 2014. Infusion of labeled data into distant supervision for relation extraction. In Proceedings of ACL.",
      "doi": ""
    },
    {
      "text": "Drazen Prelec and H. Sebastian Seung. 2007. An algorithm that finds truth even if most people are wrong. (2007).",
      "doi": ""
    },
    {
      "text": "Daniela Retelny, S\u00e9bastien Robaszkiewicz, Alexandra To, Walter S Lasecki, Jay Patel, Negar Rahmati, Tulsee Doshi, Melissa Valentine, and Michael S Bernstein. 2014. Expert crowdsourcing with flash teams. In Proceedings of the 27th annual ACM symposium on User interface software and technology. ACM, 75--85.  ",
      "doi": "10.1145/2642918.2647409"
    },
    {
      "text": "Niloufar Salehi and Michael S. Bernstein. 2018. Hive: Collective Design Through Network Rotation. In PACMHCI.  ",
      "doi": "10.1145/3274420"
    },
    {
      "text": "Mike Schaekermann, Joslin Goh, Kate Larson, and Edith Law. 2018. Resolvable vs. Irresolvable Disagreement: A Study on Worker Deliberation in Crowd Work. In PACMHCI.  ",
      "doi": "10.1145/3274423"
    },
    {
      "text": "Rion Snow, Brendan O'Connor, Daniel Jurafsky, and A. Ng. 2008. Cheap and Fast - But is it Good? Evaluating Non-Expert Annotations for Natural Language Tasks. In EMNLP'08. ",
      "doi": "10.5555/1613715.1613751"
    },
    {
      "text": "Yu-An Sun, Christopher R Dance, Shourya Roy, and Greg Little. 2011. How to assure the quality of human computation tasks when majority voting fails. In Workshop on Computational Social Science and the Wisdom of Crowds, NIPS.",
      "doi": ""
    },
    {
      "text": "Mihai Surdeanu. 20 Overview of the TAC2013 Knowledge Base Population Evaluation: English Slot Filling and Temporal Slot Filling. In TAC 2013.",
      "doi": ""
    },
    {
      "text": "Peter Welinder, Steve Branson, Serge Belongie, and Pietro Perona. 2010. The Multidimensional Wisdom of Crowds. In NIPS. ",
      "doi": "10.5555/2997046.2997166"
    },
    {
      "text": "Jacob Whitehill, Paul Ruvolo, Tingfan Wu, Jacob Bergsma, and Javier Movellan. 2009. Whose Vote Should Count More: Optimal Integration of Labels from Laberlers of Unknown Expertise. In In Proc. of NIPS. 2035--2043. ",
      "doi": "10.5555/2984093.2984321"
    },
    {
      "text": "J. Wiebe, R. Bruce, and T. O'Hara. 1999. Development and use of a gold-standard data set for subjectivity classifications. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics. 246--253. http://dl.acm.org/citation. cfm?id=1034678.1034721  ",
      "doi": "10.3115/1034678.1034721"
    },
    {
      "text": "Omar F Zaidan, Jason Eisner, and Christine D Piatko. 200 Using \"annotator rationales\" to improve machine learning for text categorization. In Proceedings of NAACL and HLT 2007.",
      "doi": ""
    },
    {
      "text": "Amy X. Zhang and Justin Cranshaw. 2018. Making Sense of Group Chat through Collaborative Tagging and Summarization. In PACMHCI.  ",
      "doi": "10.1145/3274465"
    },
    {
      "text": "Ce Zhang, Feng Niu, Christopher R\u00e9, and Jude Shavlik. 2012. Big data versus the crowd: Looking for relationships in all the right places. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. Association for Computational Linguistics, 825--834. ",
      "doi": "10.5555/2390524.2390640"
    },
    {
      "text": "Sharon Zhou, Melissa Valentine, and Michael S. Bernstein. 2018. In Search of the Dream Team: Temporally Constrained Multi-Armed Bandits for Identifying Effective Team Structures.",
      "doi": "10.1145/3173574.3173682"
    },
    {
      "text": "Haiyi Zhu, Steven P. Dow, Robert E. Kraut, and Aniket Kittur. 2014. Reviewing Versus Doing: Learning and Performance in Crowd Assessment. In Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work & Social Computing (CSCW '14). ACM, New York, NY, USA, 1445--1455.  ",
      "doi": "10.1145/2531602.2531718"
    }
  ]
}