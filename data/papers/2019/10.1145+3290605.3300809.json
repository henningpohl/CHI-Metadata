{
  "doi": "10.1145/3290605.3300809",
  "title": "Gamut: A Design Probe to Understand How Data Scientists Understand Machine Learning Models",
  "published": "2019-05-02",
  "proctitle": "CHI '19: Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems",
  "pages": "1-13",
  "year": 2019,
  "badges": [],
  "abstract": "Without good models and the right tools to interpret them, data scientists risk making decisions based on hidden biases, spurious correlations, and false generalizations. This has led to a rallying cry for model interpretability. Yet the concept of interpretability remains nebulous, such that researchers and tool designers lack actionable guidelines for how to incorporate interpretability into models and accompanying tools. Through an iterative design process with expert machine learning researchers and practitioners, we designed a visual analytics system, Gamut, to explore how interactive interfaces could better support model interpretation. Using Gamut as a probe, we investigated why and how professional data scientists interpret models, and how interface affordances can support data scientists in answering questions about model interpretability. Our investigation showed that interpretability is not a monolithic concept: data scientists have different reasons to interpret models and tailor explanations for specific audiences, often balancing competing concerns of simplicity and completeness. Participants also asked to use Gamut in their work, highlighting its potential to help data scientists understand their own data.",
  "authors": [
    {
      "name": "Fred Hohman",
      "institution": "Georgia Institute of Technology, Atlanta, GA, USA",
      "img": "/do/10.1145/contrib-99659161281/rel-imgonly/portrait.jpg",
      "acmid": "99659161281",
      "orcid": "0000-0002-4164-844X"
    },
    {
      "name": "Andrew Head",
      "institution": "University of California, Berkeley, Berkeley, CA, USA",
      "img": "/do/10.1145/contrib-99658703835/rel-imgonly/img_13703.jpg",
      "acmid": "99658703835",
      "orcid": "0000-0002-1523-3347"
    },
    {
      "name": "Rich Caruana",
      "institution": "Microsoft Research, Redmond, WA, USA",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81100100877",
      "orcid": "0000-0002-6383-7786"
    },
    {
      "name": "Robert DeLine",
      "institution": "Microsoft Research, Redmond, WA, USA",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81350602833",
      "orcid": "0000-0001-8885-8367"
    },
    {
      "name": "Steven M. Drucker",
      "institution": "Microsoft Research, Redmond, WA, USA",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81100090466",
      "orcid": "0000-0002-5022-9343"
    }
  ],
  "references": [
    {
      "text": "Ashraf Abdul, Jo Vermeulen, Danding Wang, Brian Y Lim, and Mohan Kankanhalli. 2018. Trends and trajectories for explainable, accountable and intelligible systems: an HCI research agenda. In ACM Conference on Human Factors in Computing Systems. ACM, 582.  ",
      "doi": "10.1145/3173574.3174156"
    },
    {
      "text": "Saleema Amershi, Max Chickering, Steven M Drucker, Bongshin Lee, Patrice Simard, and Jina Suh. 2015. Modeltracker: redesigning performance analysis tools for machine learning. In ACM Conference on Human Factors in Computing Systems. ACM, 337--346.  ",
      "doi": "10.1145/2702123.2702509"
    },
    {
      "text": "Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2016. Machine bias. ProPublica, May 23 (2016).",
      "doi": ""
    },
    {
      "text": "Or Biran and Courtenay Cotton. 2017. Explanation and justification in machine learning: a survey. In IJCAI Workshop on Explainable AI.",
      "doi": ""
    },
    {
      "text": "Michael Bostock, Vadim Ogievetsky, and Jeffrey Heer. 2011. D3 datadriven documents. IEEE Transactions on Visualization and Computer Graphics 12 (2011), 2301--2309.  ",
      "doi": "10.1109/TVCG.2011.185"
    },
    {
      "text": "Michael Brooks, Saleema Amershi, Bongshin Lee, Steven M Drucker, Ashish Kapoor, and Patrice Simard. 2015. FeatureInsight: visual support for error-driven feature ideation in text classification. In IEEE Conference on Visual Analytics Science and Technology. IEEE, 105--112.",
      "doi": ""
    },
    {
      "text": "Joy Buolamwini and Timnit Gebru. 2018. Gender shades: intersectional accuracy disparities in commercial gender classification. In ACM Conference on Fairness, Accountability and Transparency. 77--91.",
      "doi": ""
    },
    {
      "text": "Aylin Caliskan, Joanna J Bryson, and Arvind Narayanan. 2017. Semantics derived automatically from language corpora contain human-like biases. Science 356, 6334 (2017), 183--186.",
      "doi": ""
    },
    {
      "text": "Mackinlay Card. 1999. Readings in information visualization: using vision to think. Morgan Kaufmann. ",
      "doi": "10.5555/300679"
    },
    {
      "text": "Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie Elhadad. 2015. Intelligible models for healthcare: predicting pneumonia risk and hospital 30-day readmission. In ACM International Conference on Knowledge Discovery and Data Mining. ACM, 1721--1730.  ",
      "doi": "10.1145/2783258.2788613"
    },
    {
      "text": "Jason Chuang, Daniel Ramage, Christopher Manning, and Jeffrey Heer. 2012. Interpretation and trust: designing model-driven visualizations for text analysis. In ACM Conference on Human Factors in Computing Systems. ACM, 443--452.  ",
      "doi": "10.1145/2207676.2207738"
    },
    {
      "text": "Dennis Collaris, Leo M Vink, and Jarke J van Wijk. 2018. Instancelevel explanations for fraud detection: a case study. ICML Workshop on Human Interpretability in Machine Learning (2018).",
      "doi": ""
    },
    {
      "text": "Kristin A Cook and James J Thomas. 2005. Illuminating the path: the research and development agenda for visual analytics. Technical Report. Pacific Northwest National Lab. Richland, WA, USA.",
      "doi": ""
    },
    {
      "text": "Joseph A Cruz and David S Wishart. 2006. Applications of machine learning in cancer prediction and prognosis. Cancer Informatics 2 (2006), 117693510600200030.",
      "doi": ""
    },
    {
      "text": "Finale Doshi-Velez and Been Kim. 2017. Towards a rigorous science of interpretable machine learning. arXiv preprint arXiv:1702.08608 (2017).",
      "doi": ""
    },
    {
      "text": "Finale Doshi-Velez, Mason Kortz, Ryan Budish, Chris Bavitz, Sam Gershman, David O'Brien, Stuart Schieber, James Waldo, David Weinberger, and Alexandra Wood. 2017. Accountability of AI under the law: the role of explanation. arXiv preprint arXiv:1711.01134 (2017).",
      "doi": ""
    },
    {
      "text": "Jerome H Friedman. 2001. Greedy function approximation: a gradient boosting machine. Annals of Statistics (2001), 1189--1232.",
      "doi": ""
    },
    {
      "text": "Bill Gaver, Tony Dunne, and Elena Pacenti. 1999. Design: cultural probes. Interactions 6, 1 (1999), 21--29.  ",
      "doi": "10.1145/291224.291235"
    },
    {
      "text": "Marco Gillies, Rebecca Fiebrink, Atau Tanaka, J\u00e9r\u00e9mie Garcia, Fr\u00e9d\u00e9ric Bevilacqua, Alexis Heloir, Fabrizio Nunnari, Wendy Mackay, Saleema Amershi, Bongshin Lee, et al. 2016. Human-centred machine learning. In ACM Conference Extended Abstracts on Human Factors in Computing Systems. ACM, 3558--3565.  ",
      "doi": "10.1145/2851581.2856492"
    },
    {
      "text": "Leilani H Gilpin, David Bau, Ben Z Yuan, Ayesha Bajwa, Michael Specter, and Lalana Kagal. 2018. Explaining explanations: an approach to evaluating interpretability of machine learning. arXiv preprint arXiv:1806.00069 (2018).",
      "doi": ""
    },
    {
      "text": "Bryce Goodman and Seth Flaxman. 2016. European Union regulations on algorithmic decision-making and a \"right to explanation\". ICML Workshop on Human Interpretability in Machine Learning (2016).",
      "doi": ""
    },
    {
      "text": "Connor Graham and Mark Rouncefield. 2008. Probes and participation. In Conference on Participatory Design. Indiana University, 194--197. ",
      "doi": "10.5555/1795234.1795272"
    },
    {
      "text": "David Gunning. 2017. Explainable artificial intelligence (xai). Defense Advanced Research Projects Agency (DARPA) (2017).",
      "doi": ""
    },
    {
      "text": "Trevor J Hastie and Robert Tibshirani. 1990. Generalized additive models. In Chapman & Hall/CRC.",
      "doi": ""
    },
    {
      "text": "Fred Hohman, Minsuk Kahng, Robert Pienta, and Duen Horng Chau. 2018. Visual analytics in deep learning: an interrogative survey for the next frontiers. IEEE Transactions on Visualization and Computer Graphics (2018).",
      "doi": "10.1109/TVCG.2018.2843369"
    },
    {
      "text": "Hilary Hutchinson, Wendy Mackay, Bo Westerlund, Benjamin B Bederson, Allison Druin, Catherine Plaisant, Michel Beaudouin-Lafon, St\u00e9phane Conversy, Helen Evans, Heiko Hansen, et al. 2003. Technology probes: inspiring design for and with families. In ACM Conference on Human Factors in Computing Systems. ACM, 17--24.  ",
      "doi": "10.1145/642611.642616"
    },
    {
      "text": "Neal Jean, Marshall Burke, Michael Xie, W Matthew Davis, David B Lobell, and Stefano Ermon. 2016. Combining satellite imagery and machine learning to predict poverty. Science 353, 6301 (2016), 790--794.",
      "doi": ""
    },
    {
      "text": "Kelvyn Jones and Simon Almond. 1992. Moving out of the linear rut: the possibilities of generalized additive models. Transactions of the Institute of British Geographers (1992), 434--447.",
      "doi": ""
    },
    {
      "text": "Michael I Jordan and Tom M Mitchell. 2015. Machine learning: trends, perspectives, and prospects. Science 349, 6245 (2015), 255--260.",
      "doi": ""
    },
    {
      "text": "Minsuk Kahng, Pierre Y Andrews, Aditya Kalro, and Duen Horng Polo Chau. 2018. Activis: Visual exploration of industry-scale deep neural network models. IEEE Transactions on Visualization and Computer Graphics 24, 1 (2018), 88--97.",
      "doi": ""
    },
    {
      "text": "Minsuk Kahng, Dezhi Fang, and Duen Horng Polo Chau. 2016. Visual exploration of machine learning results using data cube analysis. In Workshop on Human-In-the-Loop Data Analytics. ACM.  ",
      "doi": "10.1145/2939502.2939503"
    },
    {
      "text": "Konstantina Kourou, Themis P Exarchos, Konstantinos P Exarchos, Michalis V Karamouzis, and Dimitrios I Fotiadis. 2015. Machine learning applications in cancer prognosis and prediction. Computational and Structural Biotechnology Journal 13 (2015), 8--17.",
      "doi": ""
    },
    {
      "text": "Josua Krause, Aritra Dasgupta, Jordan Swartz, Yindalon Aphinyanaphongs, and Enrico Bertini. 2017. A workflow for isual diagnostics of binary classifiers using instance-level explanations. IEEE Conference on Visual Analytics Science and Technology (2017).",
      "doi": ""
    },
    {
      "text": "Josua Krause, Adam Perer, and Enrico Bertini. 2014. INFUSE: interactive feature selection for predictive modeling of high dimensional data. IEEE Transactions on Visualization and Computer Graphics 20, 12 (2014), 1614--1623.",
      "doi": ""
    },
    {
      "text": "Josua Krause, Adam Perer, and Enrico Bertini. 2018. A user study on the effect of aggregating explanations for interpreting machine learning models. ACM KDD Workshop on Interactive Data Exploration and Analytics (2018).",
      "doi": ""
    },
    {
      "text": "Josua Krause, Adam Perer, and Kenney Ng. 2016. Interacting with predictions: visual inspection of black-box machine learning models. In ACM Conference on Human Factors in Computing Systems. ACM, 5686--5697.  ",
      "doi": "10.1145/2858036.2858529"
    },
    {
      "text": "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems. 1097--1105. ",
      "doi": "10.5555/2999134.2999257"
    },
    {
      "text": "Zachary C Lipton. 2016. The mythos of model interpretability. ICML Workshop on Human Interpretability in Machine Learning (2016).",
      "doi": ""
    },
    {
      "text": "Yin Lou, Rich Caruana, and Johannes Gehrke. 2012. Intelligible models for classification and regression. In ACM International Conference on Knowledge Discovery and Data Mining. ACM, 150--158.  ",
      "doi": "10.1145/2339530.2339556"
    },
    {
      "text": "Yin Lou, Rich Caruana, Johannes Gehrke, and Giles Hooker. 2013. Accurate intelligible models with pairwise interactions. In ACM International Conference on Knowledge Discovery and Data Mining. ACM, 623--631.  ",
      "doi": "10.1145/2487575.2487579"
    },
    {
      "text": "Junhua Lu, Wei Chen, Yuxin Ma, Junming Ke, Zongzhuang Li, Fan Zhang, and Ross Maciejewski. 2017. Recent progress and trends in predictive visual analytics. Frontiers of Computer Science 11, 2 (2017), 192--207.  ",
      "doi": "10.1007/s11704-016-6028-y"
    },
    {
      "text": "Yafeng Lu, Rolando Garcia, Brett Hansen, Michael Gleicher, and Ross Maciejewski. 2017. The state-of-the-art in predictive visual analytics. In Computer Graphics Forum, Vol. 36. Wiley Online Library, 539--562.  ",
      "doi": "10.1111/cgf.13210"
    },
    {
      "text": "Michael Madaio, Shang-Tse Chen, Oliver L Haimson, Wenwen Zhang, Xiang Cheng, Matthew Hinds-Aldrich, Duen Horng Chau, and Bistra Dilkina. 2016. Firebird: predicting fire risk and prioritizing fire inspections in atlanta. In ACM International Conference on Knowledge Discovery and Data Mining. ACM, 185--194.  ",
      "doi": "10.1145/2939672.2939682"
    },
    {
      "text": "Sean McGregor, Hailey Buckingham, Thomas G Dietterich, Rachel Houtman, Claire Montgomery, and Ronald Metoyer. 2017. Interactive visualization for testing markov decision processes: MDPVIS. Journal of Visual Languages & Computing 39 (2017), 93--106.  ",
      "doi": "10.1016/j.jvlc.2016.10.007"
    },
    {
      "text": "Tim Miller. 2017. Explanation in artificial intelligence: insights from the social sciences. arXiv preprint arXiv:1706.07269 (2017).",
      "doi": ""
    },
    {
      "text": "Yao Ming, Huamin Qu, and Enrico Bertini. 2019. RuleMatrix: visualizing and understanding classifiers with rules. IEEE Transactions on Visualization and Computer Graphics 25, 1 (2019), 342--352.",
      "doi": "10.1109/TVCG.2018.2864812"
    },
    {
      "text": "Christoph Molnar. 2018. Interpretable machine learning. https://christophm.github.io/interpretable-ml-book/. https://christophm.github.io/interpretable-ml-book/.",
      "doi": ""
    },
    {
      "text": "Gr\u00e9goire Montavon, Wojciech Samek, and Klaus-Robert M\u00fcller. 2017. Methods for interpreting and understanding deep neural networks. Digital Signal Processing (2017).",
      "doi": ""
    },
    {
      "text": "Menaka Narayanan, Emily Chen, Jeffrey He, Been Kim, Sam Gershman, and Finale Doshi-Velez. 2018. How do humans understand explanations from machine learning systems? an evaluation of the human-interpretability of explanation. arXiv preprint arXiv:1802.00682 (2018).",
      "doi": ""
    },
    {
      "text": "Google PAIR. 2018. What-If Tool. (2018). https://pair-code.github.io/ what-if-tool/",
      "doi": ""
    },
    {
      "text": "Parliament and Council of the European Union. 2016. General Data Protection Regulation. (2016).",
      "doi": ""
    },
    {
      "text": "Forough Poursabzi-Sangdeh, Daniel G Goldstein, Jake M Hofman, Jennifer Wortman Vaughan, and Hanna Wallach. 2017. Manipulating and measuring model interpretability. NIPS Women in Machine Learning Workshop (2017).",
      "doi": ""
    },
    {
      "text": "Donghao Ren, Saleema Amershi, Bongshin Lee, Jina Suh, and Jason D Williams. 2017. Squares: supporting interactive performance analysis for multiclass classifiers. IEEE Transactions on Visualization and Computer Graphics 23, 1 (2017), 61--70.  ",
      "doi": "10.1109/TVCG.2016.2598828"
    },
    {
      "text": "Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. Why should i trust you?: explaining the predictions of any classifier. In ACM International Conference on Knowledge Discovery and Data Mining. ACM, 1135--1144.  ",
      "doi": "10.1145/2939672.2939778"
    },
    {
      "text": "Dominik Sacha, Michael Sedlmair, Leishi Zhang, John A Lee, Jaakko Peltonen, Daniel Weiskopf, Stephen C North, and Daniel A Keim. 2017. What you see is what you can change: human-centered machine learning by interactive visualization. Neurocomputing 268 (2017), 164-- 175.  ",
      "doi": "10.1016/j.neucom.2017.01.105"
    },
    {
      "text": "Matthias Schmid and Torsten Hothorn. 2008. Boosting additive models using component-wise P-splines. Computational Statistics & Data Analysis 53, 2 (2008), 298--311.  ",
      "doi": "10.1016/j.csda.2008.09.009"
    },
    {
      "text": "Daniel Serv\u00e9n and Charlie Brummitt. 2018. pyGAM: generalized additive models in python.",
      "doi": ""
    },
    {
      "text": "David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. 2017. Mastering the game of Go without human knowledge. Nature 550, 7676 (2017), 354.",
      "doi": ""
    },
    {
      "text": "Bhavkaran Singh Walia, Qianyi Hu, Jeffrey Chen, Fangyan Chen, Jessica Lee, Nathan Kuo, Palak Narang, Jason Batts, Geoffrey Arnold, and Michael Madaio. 2018. A dynamic pipeline for spatio-temporal fire risk prediction. In ACM International Conference on Knowledge Discovery & Data Mining. ACM, 764--773.  ",
      "doi": "10.1145/3219819.3219913"
    },
    {
      "text": "Simone Stumpf, Vidya Rajaram, Lida Li, Weng-Keen Wong, Margaret Burnett, Thomas Dietterich, Erin Sullivan, and Jonathan Herlocker. 2009. Interacting meaningfully with machine learning systems: three experiments. International Journal of Human-Computer Studies 67, 8 (2009), 639--662.  ",
      "doi": "10.1016/j.ijhcs.2009.03.004"
    },
    {
      "text": "Sarah Tan, Rich Caruana, Giles Hooker, and Yin Lou. 2018. Distilland-compare: auditing black-box models using transparent model distillation. AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society (2018).  ",
      "doi": "10.1145/3278721.3278725"
    },
    {
      "text": "Sandra Wachter, Brent Mittelstadt, and Chris Russell. 2017. Counterfactual explanations without ppening the black box: automated decisions and the GDPR. arXiv preprint arXiv:1711.00399 (2017).",
      "doi": ""
    },
    {
      "text": "Daniel S. Weld and Gagan Bansal. 2018. Intelligible artificial intelligence. arXiv preprint arXiv:1803.04263 (2018).",
      "doi": ""
    },
    {
      "text": "Simon N Wood. 2006. Generalized additive models: an introduction with R. Chapman and Hall/CRC. ",
      "doi": "10.5555/1207472"
    },
    {
      "text": "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google's neural machine translation system: bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144 (2016).",
      "doi": ""
    },
    {
      "text": "Qian Yang, Jina Suh, Nan-Chen Chen, and Gonzalo Ramos. 2018. Grounding interactive machine learning tool design in how nonexperts actually build models. In Designing Interactive Systems Conference. ACM, 573--584.  ",
      "doi": "10.1145/3196709.3196729"
    },
    {
      "text": "Jiawei Zhang, Yang Wang, Piero Molino, Lezhi Li, and David S Ebert. 2019. Manifold: a model-agnostic framework for interpretation and diagnosis of machine learning Models. IEEE Transactions on Visualization and Computer Graphics 25, 1 (2019), 364--373.",
      "doi": "10.1109/tvcg.2018.2864499"
    }
  ]
}