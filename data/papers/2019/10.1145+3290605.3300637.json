{
  "doi": "10.1145/3290605.3300637",
  "title": "Understanding and Mitigating Worker Biases in the Crowdsourced Collection of Subjective Judgments",
  "published": "2019-05-02",
  "proctitle": "CHI '19: Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems",
  "pages": "1-12",
  "year": 2019,
  "badges": [],
  "abstract": "Crowdsourced data acquired from tasks that comprise a subjective component (e.g. opinion detection, sentiment analysis) is potentially affected by the inherent bias of crowd workers who contribute to the tasks. This can lead to biased and noisy ground-truth data, propagating the undesirable bias and noise when used in turn to train machine learning models or evaluate systems. In this work, we aim to understand the influence of workers' own opinions on their performance in the subjective task of bias detection. We analyze the influence of workers' opinions on their annotations corresponding to different topics. Our findings reveal that workers with strong opinions tend to produce biased annotations. We show that such bias can be mitigated to improve the overall quality of the data collected. Experienced crowd workers also fail to distance themselves from their own opinions to provide unbiased annotations.",
  "tags": [
    "bias",
    "workers",
    "crowdsourcing",
    "microtasks"
  ],
  "authors": [
    {
      "name": "Christoph Hube",
      "institution": "Leibniz Universt\u00e4t Hannover, Hannover, Germany",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659154392",
      "orcid": "missing"
    },
    {
      "name": "Besnik Fetahu",
      "institution": "Leibniz Universt\u00e4t Hannover, Hannover, Germany",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81508708819",
      "orcid": "missing"
    },
    {
      "name": "Ujwal Gadiraju",
      "institution": "Leibniz Universt\u00e4t Hannover, Hannover, Lower Saxony, Germany",
      "img": "/do/10.1145/contrib-83458903457/rel-imgonly/ujwal-circle.png",
      "acmid": "83458903457",
      "orcid": "0000-0002-6189-6539"
    }
  ],
  "references": [
    {
      "text": "Alan Aipe and Ujwal Gadiraju. 2018. SimilarHITs: Revealing the Role of Task Similarity in Microtask Crowdsourcing. In Proceedings of the 29th on Hypertext and Social Media. ACM, 115--122.  ",
      "doi": "10.1145/3209542.3209558"
    },
    {
      "text": "Ricardo Baeza-Yates. 2018. Bias on the web. Commun. ACM 61, 6 (2018), 54--61.  ",
      "doi": "10.1145/3209581"
    },
    {
      "text": "W Lance Bennett. 2016. News: The politics of illusion. University of Chicago Press.",
      "doi": ""
    },
    {
      "text": "Maxwell T Boykoff and Jules M Boykoff. 2004. Balance as bias: global warming and the US prestige press. Global environmental change 14, 2 (2004), 125--136.",
      "doi": ""
    },
    {
      "text": "R\u00f3ger Brown. 1960. Gilman. The Pronouns of the Power and Solidarity (1960).",
      "doi": ""
    },
    {
      "text": "Carrie J Cai, Shamsi T Iqbal, and Jaime Teevan. 2016. Chain reactions: The impact of order on microtask chains. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems. ACM, 3143--3154.  ",
      "doi": "10.1145/2858036.2858237"
    },
    {
      "text": "Lydia B Chilton, John J Horton, Robert C Miller, and Shiri Azenkot. 2010. Task search in a human computation market. In Proceedings of the ACM SIGKDD workshop on human computation. ACM, 1--9.  ",
      "doi": "10.1145/1837885.1837889"
    },
    {
      "text": "Djellel Eddine Difallah, Michele Catasta, Gianluca Demartini, Panagiotis G. Ipeirotis, and Philippe Cudr\u00e9-Mauroux. 2015. The Dynamics of Micro-Task Crowdsourcing: The Case of Amazon MTurk. In Proceedings of the 24th International Conference on World Wide Web, WWW 2015, Florence, Italy, May 18--22, 2015. 238--247.  ",
      "doi": "10.1145/2736277.2741685"
    },
    {
      "text": "Carsten Eickhoff. 2018. Cognitive Biases in Crowdsourcing. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining. ACM, 162--170.  ",
      "doi": "10.1145/3159652.3159654"
    },
    {
      "text": "Carsten Eickhoff and Arjen P de Vries. 2013. Increasing cheat robustness of crowdsourcing tasks. Information retrieval 16, 2 (2013), 121--137.  ",
      "doi": "10.1007/s10791-011-9181-9"
    },
    {
      "text": "Boi Faltings, Radu Jurca, Pearl Pu, and Bao Duy Tran. 2014. Incentives to counter bias in human computation. In Second AAAI conference on human computation and crowdsourcing.",
      "doi": ""
    },
    {
      "text": "Gavan J Fitzsimons, J Wesley Hutchinson, Patti Williams, Joseph W Alba, Tanya L Chartrand, Joel Huber, Frank R Kardes, Geeta Menon, Priya Raghubir, J Edward Russo, et al. 2002. Non-conscious influences on consumer choice. Marketing Letters 13, 3 (2002), 269--279.",
      "doi": ""
    },
    {
      "text": "Roger Fowler. 2013. Language in the News: Discourse and Ideology in the Press. Routledge.",
      "doi": ""
    },
    {
      "text": "Liye Fu, Cristian Danescu-Niculescu-Mizil, and Lillian Lee. 2016. Tiebreaker: Using language models to quantify gender bias in sports journalism. arXiv preprint arXiv:1607.03895 (2016).",
      "doi": ""
    },
    {
      "text": "Ujwal Gadiraju, Alessandro Checco, Neha Gupta, and Gianluca Demartini. 2017. Modus operandi of crowd workers: The invisible role of microtask work environments. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 1, 3 (2017), 49.  ",
      "doi": "10.1145/3130914"
    },
    {
      "text": "Ujwal Gadiraju, Besnik Fetahu, Ricardo Kawase, Patrick Siehndel, and Stefan Dietze. 2017. Using worker self-assessments for competencebased pre-selection in crowdsourcing microtasks. ACM Transactions on Computer-Human Interaction (TOCHI) 24, 4 (2017), 30.  ",
      "doi": "10.1145/3119930"
    },
    {
      "text": "Ujwal Gadiraju, Ricardo Kawase, and Stefan Dietze. 2014. A taxonomy of microtasks on the web. In 25th ACM Conference on Hypertext and Social Media, HT '14, Santiago, Chile, September 1--4, 2014. 218--223.  ",
      "doi": "10.1145/2631775.2631819"
    },
    {
      "text": "Ujwal Gadiraju, Ricardo Kawase, Stefan Dietze, and Gianluca Demartini. 2015. Understanding malicious behavior in crowdsourcing platforms: The case of online surveys. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems. ACM, 1631-- 1640.  ",
      "doi": "10.1145/2702123.2702443"
    },
    {
      "text": "Ujwal Gadiraju, Jie Yang, and Alessandro Bozzon. 2017. Clarity is a worthwhile quality: On the role of task clarity in microtask crowdsourcing. In Proceedings of the 28th ACM Conference on Hypertext and Social Media. ACM, 5--14.  ",
      "doi": "10.1145/3078714.3078715"
    },
    {
      "text": "Stephan Greene and Philip Resnik. 2009. More than words: Syntactic packaging and implicit sentiment. In Proceedings of human language technologies: The 2009 annual conference of the north american chapter of the association for computational linguistics. Association for Computational Linguistics, 503--511. ",
      "doi": "10.5555/1620754.1620827"
    },
    {
      "text": "Sture Holm. 1979. A simple sequentially rejective multiple test procedure. Scandinavian journal of statistics (1979), 65--70.",
      "doi": ""
    },
    {
      "text": "David S Holmes. 1968. Dimensions of projection. Psychological bulletin 69, 4 (1968), 248.",
      "doi": ""
    },
    {
      "text": "David S Holmes. 1978. Projection as a defense mechanism. Psychological Bulletin 85, 4 (1978), 677.",
      "doi": ""
    },
    {
      "text": "Tobias Hossfeld, Christian Keimel, Matthias Hirth, Bruno Gardlo, Julian Habigt, Klaus Diepold, and Phuoc Tran-Gia. 2014. Best practices for QoE crowdtesting: QoE assessment with crowdsourcing. IEEE Transactions on Multimedia 16, 2 (2014), 541--558.  ",
      "doi": "10.1109/TMM.2013.2291663"
    },
    {
      "text": "Christoph Hube and Besnik Fetahu. 2018. Detecting Biased Statements in Wikipedia. In Companion of the The Web Conference 2018 on The Web Conference 2018. International World Wide Web Conferences Steering Committee, 1779--1786.  ",
      "doi": "10.1145/3184558.3191640"
    },
    {
      "text": "Christoph Hube and Besnik Fetahu. 2018. Neural Based Statement Classification for Biased Language. arXiv preprint arXiv:1811.05740 (2018).  ",
      "doi": "10.1145/3289600.3291018"
    },
    {
      "text": "Nguyen Quoc Viet Hung, Nguyen Thanh Tam, Lam Ngoc Tran, and Karl Aberer. 2013. An evaluation of aggregation techniques in crowdsourcing. In International Conference on Web Information Systems Engineering. Springer, 1--15.",
      "doi": ""
    },
    {
      "text": "Panagiotis G Ipeirotis, Foster Provost, and Jing Wang. 2010. Quality management on amazon mechanical turk. In Proceedings of the ACM SIGKDD workshop on human computation. ACM, 64--67.  ",
      "doi": "10.1145/1837885.1837906"
    },
    {
      "text": "Mohit Iyyer, Peter Enns, Jordan Boyd-Graber, and Philip Resnik. 2014. Political ideology detection using recursive neural networks. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Vol. 1. 1113--1122.",
      "doi": ""
    },
    {
      "text": "Ayush Jain, Akash Das Sarma, Aditya Parameswaran, and Jennifer Widom. 2017. Understanding workers, developing effective tasks, and enhancing marketplace dynamics: a study of a large crowdsourcing marketplace. Proceedings of the VLDB Endowment 10, 7 (2017), 829--840.  ",
      "doi": "10.14778/3067421.3067431"
    },
    {
      "text": "Ling Jiang, Christian Wagner, and Bonnie Nardi. 2015. Not Just in it for the Money: A Qualitative Investigation of Workers' Perceived Benefits of Micro-task Crowdsourcing. In System Sciences (HICSS), 2015 48th Hawaii International Conference on. IEEE, 773--782.  ",
      "doi": "10.1109/HICSS.2015.98"
    },
    {
      "text": "Ece Kamar, Ashish Kapoor, and Eric Horvitz. 2015. Identifying and accounting for task-dependent bias in crowdsourcing. In Third AAAI Conference on Human Computation and Crowdsourcing.",
      "doi": ""
    },
    {
      "text": "David R. Karger, Sewoong Oh, and Devavrat Shah. 2011. Iterative Learning for Reliable Crowdsourcing Systems. In Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems 2011. Proceedings of a meeting held 1214 December 2011, Granada, Spain. 1953--1961. http://papers.nips.cc/ paper/4396-iterative-learning-for-reliable-crowdsourcing-systems ",
      "doi": "10.5555/2986459.2986677"
    },
    {
      "text": "Nicolas Kaufmann, Thimo Schulze, and Daniel Veit. 2011. More than fun and money. Worker Motivation in Crowdsourcing-A Study on Mechanical Turk.. In AMCIS, Vol. 11. 1--11.",
      "doi": ""
    },
    {
      "text": "Aniket Kittur, Jeffrey V Nickerson, Michael Bernstein, Elizabeth Gerber, Aaron Shaw, John Zimmerman, Matt Lease, and John Horton. 2013. The future of crowd work. In Proceedings of the 2013 conference on Computer supported cooperative work. ACM, 1301--1318.  ",
      "doi": "10.1145/2441776.2441923"
    },
    {
      "text": "Klaus Krippendorff. 2011. Computing Krippendorff's alpha-reliability. (2011).",
      "doi": ""
    },
    {
      "text": "Qiang Liu, Jian Peng, and Alexander T. Ihler. 2012. Variational Inference for Crowdsourcing. In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3--6, 2012, Lake Tahoe, Nevada, United States. 701--709. http://papers.nips.cc/paper/ 4627-variational-inference-for-crowdsourcing ",
      "doi": "10.5555/2999134.2999212"
    },
    {
      "text": "John Lyons. 1970. New horizons in linguistics, Volume. (1970).",
      "doi": ""
    },
    {
      "text": "Catherine C Marshall and Frank M Shipman. 2013. Experiences surveying the crowd: Reflections on methods, participation, and reliability. In Proceedings of the 5th Annual ACM Web Science Conference. ACM, 234--243.  ",
      "doi": "10.1145/2464464.2464485"
    },
    {
      "text": "Nolan Miller, Paul Resnick, and Richard Zeckhauser. 2005. Eliciting informative feedback: The peer-prediction method. Management Science 51, 9 (2005), 1359--1373.  ",
      "doi": "10.1287/mnsc.1050.0379"
    },
    {
      "text": "Edward Newell and Derek Ruths. 2016. How one microtask affects another. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems. ACM, 3155--3166.  ",
      "doi": "10.1145/2858036.2858490"
    },
    {
      "text": "Drazen Prelec. 2004. A Bayesian truth serum for subjective data. science 306, 5695 (2004), 462--466.",
      "doi": ""
    },
    {
      "text": "Vikas C. Raykar, Shipeng Yu, Linda H. Zhao, Anna Jerebko, Charles Florin, Gerardo Hermosillo Valadez, Luca Bogoni, and Linda Moy. 2009. Supervised Learning from Multiple Experts: Whom to Trust when Everyone Lies a Bit. In Proceedings of the 26th Annual International Conference on Machine Learning (ICML '09). ACM, New York, NY, USA, 889--896.  ",
      "doi": "10.1145/1553374.1553488"
    },
    {
      "text": "Marta Recasens, Cristian Danescu-Niculescu-Mizil, and Dan Jurafsky. 2013. Linguistic Models for Analyzing and Detecting Biased Language.. In ACL (1). 1650--1659.",
      "doi": ""
    },
    {
      "text": "Jakob Rogstadius, Vassilis Kostakos, Aniket Kittur, Boris Smus, Jim Laredo, and Maja Vukovic. 2011. An assessment of intrinsic and extrinsic motivation on task performance in crowdsourcing markets. ICWSM 11 (2011), 17--21.",
      "doi": ""
    },
    {
      "text": "Suzanne Romaine et al. 2000. Language in society: An introduction to sociolinguistics. Oxford University Press.",
      "doi": ""
    },
    {
      "text": "Dietram A Scheufele. 1999. Framing as a theory of media effects. Journal of communication 49, 1 (1999), 103--122.",
      "doi": ""
    },
    {
      "text": "G\u00fcn R Semin and Klaus Fiedler. 1988. The cognitive functions of linguistic categories in describing persons: Social cognition and language. Journal of personality and Social Psychology 54, 4 (1988), 558.",
      "doi": ""
    },
    {
      "text": "Aaron D Shaw, John J Horton, and Daniel L Chen. 2011. Designing incentives for inexpert human raters. In Proceedings of the ACM 2011 conference on Computer supported cooperative work. ACM, 275--284.  ",
      "doi": "10.1145/1958824.1958865"
    },
    {
      "text": "Anselm L Strauss. 1987. Qualitative analysis for social scientists. Cambridge University Press.",
      "doi": ""
    },
    {
      "text": "James Surowiecki. 2005. The wisdom of crowds. Anchor. ",
      "doi": "10.5555/1095645"
    },
    {
      "text": "Fabian L. Wauthier and Michael I. Jordan. 2011. Bayesian Bias Mitigation for Crowdsourcing. In Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems 2011. Proceedings of a meeting held 12--14 December 2011, Granada, Spain. 1800--1808. http://papers.nips.cc/paper/ 4311-bayesian-bias-mitigation-for-crowdsourcing ",
      "doi": "10.5555/2986459.2986660"
    },
    {
      "text": "Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew Bell, and Melanie Martin. 2004. Learning subjective language. Computational linguistics 30, 3 (2004), 277--308.  ",
      "doi": "10.1162/0891201041850885"
    },
    {
      "text": "Tae Yano, Philip Resnik, and Noah A Smith. 2010. Shedding (a thousand points of) light on biased language. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon's Mechanical Turk. Association for Computational Linguistics, 152--158. ",
      "doi": "10.5555/1866696.1866719"
    },
    {
      "text": "Honglei Zhuang and Joel Young. 2015. Leveraging In-Batch Annotation Bias for Crowdsourced Active Learning. In Proceedings of the Eighth ACM International Conference on Web Search and Data Mining, WSDM 2015, Shanghai, China, February 2--6, 2015. 243--252.  ",
      "doi": "10.1145/2684822.2685301"
    }
  ]
}