{
  "doi": "10.1145/2556288.2557161",
  "title": "SmartVoice: a presentation support system for overcoming the language barrier",
  "published": "2014-04-26",
  "proctitle": "CHI '14: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems",
  "pages": "1563-1570",
  "year": 2014,
  "badges": [],
  "abstract": "In most cases, speeches or presentations at an international event are required to be given in a common language (e.g. English). However, for people who are not proficient in that common language, delivering presentations fluently is very difficult. Simultaneous translation seems to be a solution, but besides its high cost, simultaneous translation undermines the nature of the presentation by substituting the real voice of the lecturer as well as his/her emotions. In this paper, we propose \"SmartVoice\", a presentation support system, which aims to overcome language barriers. By tracking the lip motion of the lecturer, SmartVoice controls the playback of the narration, which is a sound data prepared in advance or created automatically using a voice synthesizer. SmartVoice also controls the intonation of the sound based on the position and shape of the lecturer's mouth. As the lecturer can talk at his/her own pace with the voice automatically following, it appears as if he/she talks in his/her own voice. In our user evaluation, we confirmed that audiences find it difficult to distinguish between the narration generated by SmartVoice and that by a real voice. We also discuss the possibility of applying SmartVoice to fields other than multi-language presentation support, such as Automated Dialogue Replacement and language study.",
  "authors": [
    {
      "name": "Xiang Li",
      "institution": "Interfaculty Initiative in Information Studies of The univ of Tokyo, Meguro, Tokyo, Japan",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "87959151757",
      "orcid": "missing"
    },
    {
      "name": "Jun Rekimoto",
      "institution": "The University of Tokyo & Sony CSL, Meguro-ku, Tokyo, Japan",
      "img": "/do/10.1145/contrib-81100008564/rel-imgonly/p2.png",
      "acmid": "81100008564",
      "orcid": "0000-0002-3629-2514"
    }
  ],
  "references": [
    {
      "text": "openframeworks. http://www.openframeworks.cc/.",
      "doi": ""
    },
    {
      "text": "ofxFaceTracker. http://github.com/kylemcdonald/ofxFaceTracker/",
      "doi": ""
    },
    {
      "text": "Dirac3L. http://dirac.dspdimension.com/",
      "doi": ""
    },
    {
      "text": "Lyons. M. J., Haehnel. M, and Tetsutani. N. The Mouthesizer: A Facial Gesture Musical Interface. In Conference Abstracts, SIGGRAPH 2001, ACM (LA, 2001), 230.",
      "doi": ""
    },
    {
      "text": "Lyons. M. J., Tetsutani. N. Facing the Music: A Facial Action Controlled Musical Interface. In Proc, CHI 2001, Conference on Human Factors in Computing Systems 2001, ACM (Seattle, 2001), 309--310.  ",
      "doi": "10.1145/634067.634250"
    },
    {
      "text": "T. Weise, S. Bouaziz, H. Li, and M. Pauly (2011). Realtime Performance-Based Facial Animation. ACM Trans. Graph., 30(4), 77.  ",
      "doi": "10.1145/2010324.1964972"
    }
  ]
}