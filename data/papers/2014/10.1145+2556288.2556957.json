{
  "doi": "10.1145/2556288.2556957",
  "title": "Improving automatic speech recognition through head pose driven visual grounding",
  "published": "2014-04-26",
  "proctitle": "CHI '14: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems",
  "pages": "3235-3238",
  "year": 2014,
  "badges": [],
  "abstract": "In this paper, we present a multimodal speech recognition system for real world scene description tasks. Given a visual scene, the system dynamically biases its language model based on the content of the visual scene and visual attention of the speaker. Visual attention is used to focus on likely objects within the scene. Given a spoken description the system then uses the visually biased language model to process the speech. The system uses head pose as a proxy for the visual attention of the speaker. Readily available standard computer vision algorithms are used to recognize the objects in the scene and automatic real time head pose estimation is done using depth data captured via a Microsoft Kinect. The system was evaluated on multiple participants. Overall, incorporating visual information into the speech recognizer greatly improved speech recognition accuracy. The rapidly decreasing cost of 3D sensing technologies such as the Kinect allows systems with similar underlying principles to be used for many speech recognition tasks where there is visual information.",
  "authors": [
    {
      "name": "Soroush Vosoughi",
      "institution": "Massachusetts Institute of Technology, Cambridge, MA, USA",
      "img": "/do/10.1145/contrib-81413591729/rel-imgonly/soroushcropped.png",
      "acmid": "81413591729",
      "orcid": "missing"
    }
  ],
  "references": [
    {
      "text": "Bradski, G. The OpenCV Library. Dr. Dobb's Journal of Software Tools (2000).",
      "doi": ""
    },
    {
      "text": "Coco, M. I., and Keller, F. Scan patterns predict sentence production in the cross-modal processing of visual scenes. Cognitive Science (2012).",
      "doi": ""
    },
    {
      "text": "Fanelli, G., Gall, J., and Van Gool, L. Real time head pose estimation with random regression forests. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, IEEE (2011), 617--624.  ",
      "doi": "10.1109/CVPR.2011.5995458"
    },
    {
      "text": "Griffin, Z. M., and Bock, K. What the eyes say about speaking. Psychological science 11, 4 (2000), 274--279.",
      "doi": ""
    },
    {
      "text": "Henderson, J. M. Human gaze control during real-world scene perception. Trends in cognitive sciences 7, 11 (2003), 498--504.",
      "doi": ""
    },
    {
      "text": "Just, M. A., and Carpenter, P. A. Eye fixations and cognitive processes. Cognitive Psychology 8, 4 (1976), 441--480.",
      "doi": ""
    },
    {
      "text": "Kaur, M., Tremaine, M., Huang, N., Wilder, J., Gacovski, Z., Flippo, F., and Mantravadi, C. S. Where is it? event synchronization in gaze-speech input systems. In Proceedings of the 5th international conference on Multimodal interfaces, ACM (2003), 151--158.  ",
      "doi": "10.1145/958432.958463"
    },
    {
      "text": "Prasov, Z., and Chai, J. Y. What's in a gaze?: the role of eye-gaze in reference resolution in multimodal conversational interfaces. In Proceedings of the 13th international conference on Intelligent user interfaces, ACM (2008), 20--29.  ",
      "doi": "10.1145/1378773.1378777"
    },
    {
      "text": "Prasov, Z., and Chai, J. Y. Fusing eye gaze with speech recognition hypotheses to resolve exophoric references in situated dialogue. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics (2010), 471--481. ",
      "doi": "10.5555/1870658.1870704"
    },
    {
      "text": "Qvarfordt, P., Beymer, D., and Zhai, S. Realtourist-a study of augmenting human-human and human-computer dialogue with eye-gaze overlay. Human-Computer Interaction-INTERACT 2005 (2005), 767--780.  ",
      "doi": "10.1007/11555261_61"
    },
    {
      "text": "Qvarfordt, P., and Zhai, S. Conversing with the user based on eye-gaze patterns. In Proceedings of the SIGCHI conference on Human factors in computing systems, ACM (2005), 221--230.  ",
      "doi": "10.1145/1054972.1055004"
    },
    {
      "text": "Roy, D., and Mukherjee, N. Visual context driven semantic priming of speech recognition and understanding. Computer Speech and Language (2003).",
      "doi": ""
    },
    {
      "text": "Roy, D. K. Learning visually grounded words and syntax for a scene description task. Computer Speech & Language 16, 3 (2002), 353--385.",
      "doi": ""
    },
    {
      "text": "Weide., H. The Carnegie Mellon University Pronunciation Dictionary, release 0.6. Carnegie Mellon University, 1998.",
      "doi": ""
    },
    {
      "text": "Young, S. J., Evermann, G., Gales, M. J. F., Hain, T., Kershaw, D., Moore, G., Odell, J., Ollason, D., Povey, D., Valtchev, V., and Woodland, P. C. The HTK Book, version 3.4. Cambridge University Engineering Department, Cambridge, UK, 2006.",
      "doi": ""
    }
  ]
}