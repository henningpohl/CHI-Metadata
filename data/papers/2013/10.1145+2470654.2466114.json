{
  "doi": "10.1145/2470654.2466114",
  "title": "Understanding palm-based imaginary interfaces: the role of visual and tactile cues when browsing",
  "published": "2013-04-27",
  "proctitle": "CHI '13: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems",
  "pages": "889-898",
  "year": 2013,
  "badges": [],
  "abstract": "Imaginary Interfaces are screen-less ultra-mobile interfaces. Previously we showed that even though they offer no visual feedback they allow users to interact spatially, e.g., by pointing at a location on their non-dominant hand. The primary goal of this paper is to provide a deeper understanding of palm-based imaginary interfaces, i.e., why they work. We perform our exploration using an interaction style inspired by interfaces for visually impaired users. We implemented a system that audibly announces target names as users scrub across their palm. Based on this interface, we conducted three studies. We found that (1) even though imaginary interfaces cannot display visual contents, users' visual sense remains the main mechanism that allows users to control the interface, as they watch their hands interact. (2) When we remove the visual sense by blindfolding, the tactile cues of both hands feeling each other in part replace the lacking visual cues, keeping imaginary interfaces usable. (3) While we initially expected the cues sensed by the pointing finger to be most important, we found instead that it is the tactile cues sensed by the palm that allow users to orient themselves most effectively. While these findings are primarily intended to deepen our understanding of Imaginary Interfaces, they also show that eyes-free interfaces located on skin outperform interfaces on physical devices. In particular, this suggests that palm-based imaginary interfaces may have benefits for visually impaired users, potentially outperforming the touchscreen-based devices they use today.",
  "authors": [
    {
      "name": "Sean G. Gustafson",
      "institution": "Hasso Plattner Institute, Potsdam, Germany",
      "img": "/do/10.1145/contrib-81350569912/rel-imgonly/seanportrait.jpg",
      "acmid": "81350569912",
      "orcid": "missing"
    },
    {
      "name": "Bernhard Rabe",
      "institution": "Hasso Plattner Institute, Potsdam, Germany",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81421597882",
      "orcid": "missing"
    },
    {
      "name": "Patrick M. Baudisch",
      "institution": "Hasso Plattner Institute, Potsdam, Germany",
      "img": "/do/10.1145/contrib-81100137268/rel-imgonly/15g9310crop3.jpg",
      "acmid": "81100137268",
      "orcid": "0000-0002-5005-6868"
    }
  ],
  "references": [
    {
      "text": "Apple. VoiceOver for iPhone. http://www.apple.com/accessibility/iphone/vision.html",
      "doi": ""
    },
    {
      "text": "Bolanowski S. J., Verrillo R. T., McGlone F. Passive, active and intraactive (self) touch. Behavioural Brain Research 148, (2004), 41--45.",
      "doi": ""
    },
    {
      "text": "Brewster, S., Lumsden, J., Bell, M., Hall, M. and Tasker, S. Multimodal 'eyes-free' interaction techniques for wearable devices. In Proc. CHI, (2003), 473--480.  ",
      "doi": "10.1145/642611.642694"
    },
    {
      "text": "Chen, X., Marquardt, N., Tang, A., Boring, S. and Greenberg, S. Extending a mobile device's interaction space through body-centric interaction. In Proc. MobileHCI, (2012), 151--160.  ",
      "doi": "10.1145/2371574.2371599"
    },
    {
      "text": "Code Factory. Mobile Speak. http://www.codefactory.es/en/products.asp?id=316",
      "doi": ""
    },
    {
      "text": "Dezfuli, N., Khalilbeigi, M., Huber, J., M\u00fcller, F. and M\u00fchlh\u00e4user, M. PalmRC: imaginary palm-based remote control for eyes-free television interaction. In Proc. EuroiTV, (2012), 27--34.  ",
      "doi": "10.1145/2325616.2325623"
    },
    {
      "text": "Driver J., and Spence C. Attention and the crossmodal construction of space. Trends in Cognitive Sciences 2, 7, (1998), 254--262.",
      "doi": ""
    },
    {
      "text": "Folmer, E. and Morelli, T. Spatial gestures using a tactile-proprioceptive display. In Proc. TEI, (2012), 139--142.  ",
      "doi": "10.1145/2148131.2148161"
    },
    {
      "text": "Fuentes, C. T., Bastian, A. J. Where is your arm? Variations in proprioception across space and tasks. Journal of Neurophysiology 103, 1 (2010), 164--171.",
      "doi": ""
    },
    {
      "text": "Gibson, J. J. Observations on active touch. Psychological Review 69, 6, (1962), 477--491.",
      "doi": ""
    },
    {
      "text": "Goldstein, M. and Chincholle, D. Finger-joint gesture wearable keypad. In Proc. MobileHCI, (1999), 9--18.",
      "doi": ""
    },
    {
      "text": "Gollner, U., Bieling, T. and Joost, G. Mobile Lorm Glove: introducing a communication device for deaf-blind people. In Proc. TEI, (2012), 127--130.  ",
      "doi": "10.1145/2148131.2148159"
    },
    {
      "text": "Gustafson, S., Bierwirth, D. and Baudisch, P. Imaginary Interfaces: spatial interaction with empty hands and without visual feedback. In Proc. UIST, (2010), 3--12.  ",
      "doi": "10.1145/1866029.1866033"
    },
    {
      "text": "Gustafson, S., Holz, C. and Baudisch, P. Imaginary Phone: learning imaginary interfaces by transferring spatial memory from a familiar device. In Proc. UIST, (2011), 283--292.  ",
      "doi": "10.1145/2047196.2047233"
    },
    {
      "text": "Harrison, C., Benko, H. and Wilson, A. D. OmniTouch: wearable multitouch interaction everywhere. In Proc. UIST, (2011), 441--450.  ",
      "doi": "10.1145/2047196.2047255"
    },
    {
      "text": "Harrison, C., Tan, D. and Morris, D. Skinput: appropriating the body as an input surface. In Proc. CHI, (2010), 453--462.  ",
      "doi": "10.1145/1753326.1753394"
    },
    {
      "text": "Kane, S. K., Bigham, J. P. and Wobbrock, J. O. Slide Rule: making mobile touch screens accessible to blind people using multi-touch interaction techniques. In Proc. ASSETS, (2008), 73--80.  ",
      "doi": "10.1145/1414471.1414487"
    },
    {
      "text": "Kane, S. K., Jayant, C., Wobbrock, J. O. and Ladner, R. E. Freedom to roam: a study of mobile device adoption and accessibility for people with visual and motor disabilities. In Proc. ASSETS, (2009), 115--122.  ",
      "doi": "10.1145/1639642.1639663"
    },
    {
      "text": "Kuester, F., Chen, M., Phair, M. E. and Mehring, C. Towards keyboard independent touch typing in VR. In Proc. VRST, (2005), 86--95.  ",
      "doi": "10.1145/1101616.1101635"
    },
    {
      "text": "Landua, S. and Wells, L. Merging tactile sensory input and audio data by means of the Talking Tactile Tablet. In Proc. Eurohaptics, (2003), 414--418.",
      "doi": ""
    },
    {
      "text": "Ladavas, E., Farne, A., Zeloni, G. and di Pellegrino, G. Seeing or not seeing where your hands are. Experimental Brain Research 31, (2000), 458--467.",
      "doi": ""
    },
    {
      "text": "Li, K. A., Baudisch, P. and Hinckley, K. Blindsight: eyes-free access to mobile phones. In Proc. CHI, (2008), 1389--1398.  ",
      "doi": "10.1145/1357054.1357273"
    },
    {
      "text": "Li, F. C., Dearman, D. and Truong, K. N. Virtual Shelves: interactions with orientation aware devices. In Proc. UIST, (2009), 125--128.  ",
      "doi": "10.1145/1622176.1622200"
    },
    {
      "text": "Li, F. C., Dearman, D. and Truong, K. N. Leveraging proprioception to make mobile phones more accessible to users with visual impairments. In Proc. ASSETS, (2010), 187--194.  ",
      "doi": "10.1145/1878803.1878837"
    },
    {
      "text": "Lin, S.-Y., Su, Z.-H., Cheng, K.-Y., Liang, R.-H., Kuo, T.-H. and Chen, B.-Y. PUB - Point Upon Body: exploring eyes-free interactions and methods on an arm. In Proc. UIST, (2011), 481--488.  ",
      "doi": "10.1145/2047196.2047259"
    },
    {
      "text": "Maravita, A., Spence, C., Driver, J. Multisensory integration and the body schema: close to hand and within reach. Current Biology 13, (July 2003), R531--R539.",
      "doi": ""
    },
    {
      "text": "McGookin, D., Brewster, S. and Jiang, W. W. Investigating touchscreen accessibility for people with visual impairments. In Proc. NordiCHI, (2008), 298--307.  ",
      "doi": "10.1145/1463160.1463193"
    },
    {
      "text": "Mistry, P., Maes, P. and Chang, L. WUW - wear Ur world: a wearable gestural interface. In CHI Ext. Abs., (2009), 4111--4116.  ",
      "doi": "10.1145/1520340.1520626"
    },
    {
      "text": "Oakley, I. and Park, J. Motion marking menus: an eyes-free approach to motion input for handheld devices. IJHCS 67, 6 (2009), 515--532.  ",
      "doi": "10.1016/j.ijhcs.2009.02.002"
    },
    {
      "text": "Pirhonen, A., Brewster, S. and Holguin, C. Gestural and audio metaphors as a means of control for mobile devices. In Proc. CHI, (2002), 291--298.  ",
      "doi": "10.1145/503376.503428"
    },
    {
      "text": "Shoemaker, G., Tsukitani, T., Kitamura, Y. and Booth, K. S. Bodycentric interaction techniques for very large wall displays. In Proc. NordiCHI, (2010), 463--472.  ",
      "doi": "10.1145/1868914.1868967"
    },
    {
      "text": "Strachan, S., Murray-Smith, R. and O'Modhrain, S. BodySpace: inferring body pose for natural control of a music player. In CHI Ext. Abs., (2007), 2001--2006.  ",
      "doi": "10.1145/1240866.1240939"
    },
    {
      "text": "Tamaki, E., Miyaki, T. and Rekimoto, J. Brainy Hand: an ear-worn hand gesture interaction device. In CHI Ext. Abs., (2009), 4255--4260.  ",
      "doi": "10.1145/1520340.1520649"
    },
    {
      "text": "Vallbo, A. B. and Johansson, R. S. The tactile sensory innervation of the glabrous skin of the human hand. Active Touch, the Mechanism of Recognition of Objects by Manipulation, (1978), 29--54.",
      "doi": ""
    },
    {
      "text": "Vanderheiden, G. C. Use of audio-haptic interface techniques to allow nonvisual access to touchscreen appliances. In Proc. Human Factors and Ergonomics Society (Poster), (1996), 1266.",
      "doi": ""
    },
    {
      "text": "Voisin, J., Lamarre, Y. and Chapman, C. E. Haptic discrimination of object shape in humans: contribution of cutaneous and proprioceptive inputs. Experimental Brain Research 145, 2 (2002), 251--260.",
      "doi": ""
    },
    {
      "text": "Zhao, S., Dragicevic, P., Chignell, M., Balakrishnan, R. and Baudisch, P. earPod: eyes-free menu selection using touch input and reactive audio feedback. In Proc. CHI, (2007), 1395--1404.  ",
      "doi": "10.1145/1240624.1240836"
    }
  ]
}