{
  "doi": "10.1145/3025453.3025644",
  "title": "A Multifaceted Study on Eye Contact based Speaker Identification in Three-party Conversations",
  "published": "2017-05-02",
  "proctitle": "CHI '17: Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems",
  "pages": "3011-3021",
  "year": 2017,
  "badges": [],
  "abstract": "To precisely understand human gaze behaviors in three-party conversations, this work is dedicated to look into whether the speaker can be reliably identified from the interlocutors in a three-party conversation on the basis of the interactive behaviors of eye contact, where speech signals are not provided. Derived from a pre-recorded, multimodal, and three-party conversational behavior dataset, a statistical framework is pro- posed to determine who is the speaker from the interactive behaviors of eye contact. Additionally, with the aid of virtual human technologies, a user study is conducted to study whether subjects are capable of distinguishing the speaker from the listeners according to the gaze behaviors of the interlocutors alone. Our results show that eye contact provides a reliable cue for the identification of the speaker in three-party conversations.",
  "tags": [
    "eye-head coordination",
    "perception of gaze",
    "face-to-face communication",
    "human-human interaction",
    "eye gaze",
    "head gestures",
    "eye contact",
    "nonverbal behaviors",
    "multiparty conversation"
  ],
  "authors": [
    {
      "name": "Yu Ding",
      "institution": "University of Houston, Houston, TX, USA",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659161066",
      "orcid": "missing"
    },
    {
      "name": "Yuting Zhang",
      "institution": "University of Houston, Houston, TX, USA",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659162019",
      "orcid": "missing"
    },
    {
      "name": "Meihua Xiao",
      "institution": "East China Jiaotong University, Nanchang, Jiangxi, China",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659161770",
      "orcid": "missing"
    },
    {
      "name": "Zhigang Deng",
      "institution": "University of Houston, Houston, TX, USA",
      "img": "/do/10.1145/contrib-81100180219/rel-imgonly/recent_photo.jpg",
      "acmid": "81100180219",
      "orcid": "missing"
    }
  ],
  "references": [
    {
      "text": "X. Anguera, S. Bozonnet, N. Evans, C. Fredouille, G. Friedland, and O. Vinyals. 2012. Speaker Diarization: A Review of Recent Research. IEEE Transactions on Audio, Speech, and Language Processing 20, 2 (2012), 356--370.  ",
      "doi": "10.1109/TASL.2011.2125954"
    },
    {
      "text": "A. C. Beall, J. N. Bailenson, J. Loomis, J. Blascovich, and C. S. Rex. 2003. Non-Zero-Sum Gaze in Immersive Virtual Environments. In International Conference on Human-Computer Interaction. 1108--1112.",
      "doi": ""
    },
    {
      "text": "J. K Burgoon, L. A. Stern, and L. Dillman. 1995. Interpersonal Adaptation: Dyadic Interaction Patterns. Cambridge University Press. ",
      "doi": ""
    },
    {
      "text": "C. Busso, Z. Deng, M. Grimm, U. Neumann, and S. Narayanan. 2007. Rigid Head Motion in Expressive Speech Animation: Analysis and Synthesis. IEEE Trans. on Audio, Speech & Language Processing 15, 3 (2007), 1075--1086.  ",
      "doi": "10.1109/TASL.2006.885910"
    },
    {
      "text": "C. Busso, Z. Deng, U. Neumann, and S. Narayanan. 2005. Natural head motion synthesis driven by acoustic prosodic features. JVCA 16, 3--4 (2005), 283--290.",
      "doi": ""
    },
    {
      "text": "J. Cassell and K. R. Th\u00f3risson. 1999. The Power of a Nod and a Glance: Envelope Vs. Emotional Feedback in Animated Conversational Agents. Applied Artificial Intelligence 13, 4 & 5 (1999), 519--538. ",
      "doi": ""
    },
    {
      "text": "R. Alex Colburn, Michael F. Cohen, and Steven M. Drucker. 2000. The Role of Eye Gaze in Avatar Mediated Conversational Interfaces. Technical Report MSR-TR-2000--81. Microsoft Research. 9 pages.",
      "doi": ""
    },
    {
      "text": "Y. Ding, C. Pelachaud, and T. Arti\u00e8res. 2013. Modeling Multimodal Behaviors from Speech Prosody. In International Conference on Intelligent Virtual Agents. 217--228. ",
      "doi": ""
    },
    {
      "text": "Y Ding, M Radenen, T Arti\u00e8res, and C Pelachaud. 2012. Eyebrow Motion Synthesis Driven by Speech. In Workshop Affect, Compagnon Artificiel, Interaction (WACAI). 103--110.",
      "doi": ""
    },
    {
      "text": "Y. Ding, M. Radenen, T. Arti\u00e8res, and C. Pelachaud. 2013. Speech-driven Eyebrow Motion Synthesis With Contextual Markovian Models. In ICASSP. 3756--3760. ",
      "doi": ""
    },
    {
      "text": "M. Garau, M. Slater, S. Bee, and M. A. Sasse. 2001. The impact of eye gaze on communication using humanoid avatars. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 309--316.  ",
      "doi": "10.1145/365024.365121"
    },
    {
      "text": "D. Gatica-Perez, G. Lathoud, J. Odobez, and I. McCowan. 2007. Audiovisual probabilistic tracking of multiple speakers in meetings. IEEE Transactions on Audio, Speech, and Language Processing 15, 2 (2007), 601--616.  ",
      "doi": "10.1109/TASL.2006.881678"
    },
    {
      "text": "S. Gorga and K. Otsuka. 2010. Conversation scene analysis based on dynamic Bayesian network and image-based gaze detection. In ICMI-MLMI. 54.  ",
      "doi": "10.1145/1891903.1891969"
    },
    {
      "text": "D.K.J. Heylen, I. van Es, A. Nijholt, and Dr. E.M.A.G. van Dijk. 2002. Experimenting with the Gaze of a Conversational Agent. In Proceedings International CLASS Workshop on Natural, Intelligent and Effective Interaction in Multimodal Dialogue Systems. 93--100.",
      "doi": ""
    },
    {
      "text": "G. Hofer and H. Shimodaira. 2007. Automatic Head Motion Prediction from Speech Data. In Proc. Interspeech. 722--725.",
      "doi": ""
    },
    {
      "text": "H. Hung, Y. Huang, C. Yeo, and D. Gatica-Perez. 2008. Associating audio-visual activity cues in a dominance estimation framework. In IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops. 1--6. ",
      "doi": ""
    },
    {
      "text": "R. Ishii, S. Kumano, and K. Otsuka. 2015. Predicting next speaker based on head movement in multi-party meetings. In ICASSP. 2319--2323. ",
      "doi": ""
    },
    {
      "text": "R. Ishii, K. Otsuka, S. Kumano, and J. Yamato. 2014. Analysis and modeling of next speaking start timing based on gaze behavior in multi-party meetings. In ICASSP. 694--698. ",
      "doi": ""
    },
    {
      "text": "R. Ishii, K. Otsuka, S. Kumano, and J. Yamato. 2016. Prediction of Who Will Be the Next Speaker and When Using Gaze Behavior in Multiparty Meetings. ACM Trans. Interact. Intell. Syst. 6, 1 (2016), 4:1--4:31.",
      "doi": ""
    },
    {
      "text": "K. Ishizuka, S. Araki, K. Otsuka, T. Nakatani, and M. Fujimoto. 2009. A Speaker Diarization Method Based on the Probabilistic Fusion of Audio-visual Location Information. In ICMI-MLMI. 55--62.  ",
      "doi": "10.1145/1647314.1647327"
    },
    {
      "text": "T. Kawahara, T. Iwatate, and K. Takanashi. 2012. Prediction of Turn-Taking by Combining Prosodic and Eye-Gaze Information in Poster Conversations. In INTERSPEECH. 727--730.",
      "doi": ""
    },
    {
      "text": "C. L. Kleinke. 1986. Gaze and eye contact: A research review. Psychological Bulletin 100, 1 (1986), 78--100. ",
      "doi": ""
    },
    {
      "text": "B. J. Lance and S. C. Marsella. 2008. A Model of Gaze for the Purpose of Emotional Expression in Virtual Embodied Agents. In AAMAS. 199--206.",
      "doi": ""
    },
    {
      "text": "B. H. Le, X. Ma, and Z. Deng. 2012. Live Speech Driven Head-and-Eye Motion Generators. IEEE Transactions on Visualization and Computer Graphics 18, 11 (2012), 1902--1914.  ",
      "doi": "10.1109/TVCG.2012.74"
    },
    {
      "text": "X. Ma and Z. Deng. 2009. Natural Eye Motion Synthesis by Modeling Gaze-Head Coupling. In IEEE Virtual Reality Conference. 143--150.",
      "doi": ""
    },
    {
      "text": "R. Maatman, J. Gratch, and S. Marsella. 2005. Natural behavior of a listening agent. In International Workshop on IVA. 25--36.  ",
      "doi": "10.1007/11550617_3"
    },
    {
      "text": "S. Mariooryad and C. Busso. 2012. Generating Human-Like Behaviors Using Joint, Speech-Driven Models for Conversational Agents. IEEE Transactions on Audio, Speech, and Language Processing 20, 8 (2012), 2329--2340.  ",
      "doi": "10.1109/TASL.2012.2201476"
    },
    {
      "text": "S. Masuko and J. Hoshino. 2007. Head-eye Animation Corresponding to a Conversation for CG Characters. Comput. Graph. Forum 26 (2007), 303--312. ",
      "doi": ""
    },
    {
      "text": "L. Morency, I. de Kok, and J. Gratch. 2008. Predicting Listener Backchannels: A Probabilistic Multimodal Approach. In International Conference on Intelligent Virtual Agents. 176--190.  ",
      "doi": "10.1007/978-3-540-85483-8_18"
    },
    {
      "text": "L. Morency, I. de Kok, and J. Gratch. 2010. A probabilistic multimodal approach for predicting listener backchannels. Autonomous Agents and Multi-Agent Systems 20, 1 (2010), 70--84.  ",
      "doi": "10.1007/s10458-009-9092-y"
    },
    {
      "text": "A. Normoyle, J. B. Badler, T. Fan, N. I. Badler, V. J. Cassol, and S. R. Musse. 2013. Evaluating Perceived Trust from Procedurally Animated Gaze. In Motion in Games. 141--148.  ",
      "doi": "10.1145/2522628.2522630"
    },
    {
      "text": "A. Noulas, G. Englebienne, and B. J. A. Krose. 2012. Multimodal Speaker Diarization. IEEE Trans. Pattern Anal. Mach. Intell. 34, 1 (2012), 79--93.  ",
      "doi": "10.1109/TPAMI.2011.47"
    },
    {
      "text": "K. Otsuka. 2011. Multimodal Conversation Scene Analysis for Understanding People's Communicative Behaviors in Face-to-Face Meetings. In Human Interface and the Management of Information. 171--179. ",
      "doi": ""
    },
    {
      "text": "K. Otsuka, H. Sawada, and J. Yamato. 2007. Automatic inference of cross-modal nonverbal interactions in multiparty conversations: \"who responds to whom, when, and how?\" from gaze, head gestures, and utterances. In ICMI. 255--262.",
      "doi": ""
    },
    {
      "text": "R. J. Rienks, R. Poppe, and D. Heylen. 2010. Differences in head orientation behavior for speakers and listeners: an experiment in a virtual environment. ACM Transactions on Applied Perception 7, 1 (2010), 2:1--2:13.",
      "doi": "10.1145/1658349.1658351"
    },
    {
      "text": "K. Ruhland, C. E Peters, S. Andrist, J. B Badler, N. I Badler, M. Gleicher, B. Mutlu, and R. McDonnell. 2015. A Review of Eye Gaze in Virtual Agents, Social Robotics and HCI: Behaviour Generation, User Interaction and Perception. Computer Graphics Forum 34, 6 (2015), 299--326.  ",
      "doi": "10.1111/cgf.12603"
    },
    {
      "text": "R. Stiefelhagen. 2002. Tracking Focus of Attention in Meetings. In ICMI. 273--280.  ",
      "doi": "10.1109/ICMI.2002.1167006"
    },
    {
      "text": "S. E. Tranter and D. A. Reynolds. 2006. An overview of automatic speaker diarization systems. IEEE Transactions on Audio, Speech, and Language Processing 14, 5 (2006), 1557--1565.  ",
      "doi": "10.1109/TASL.2006.878256"
    },
    {
      "text": "R. Vertegaal, G. van der Veer, and H. Vons. 2000. Effects of Gaze on Multiparty Mediated Communication. In Graphics Interface. Morgan Kaufmann, 95--102.",
      "doi": ""
    }
  ]
}