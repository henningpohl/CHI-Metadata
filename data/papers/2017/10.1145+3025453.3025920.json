{
  "doi": "10.1145/3025453.3025920",
  "title": "Supporting Making Fixations and the Effect on Gaze Gesture Performance",
  "published": "2017-05-02",
  "proctitle": "CHI '17: Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems",
  "pages": "3022-3033",
  "year": 2017,
  "badges": [],
  "abstract": "Gaze gestures are deliberate patterns of eye movements that can be used to invoke commands. These are less reliant on accurate measurement and calibration than other gaze-based interaction techniques. These may be used with wearable displays fitted with eye tracking capability, or as part of an assistive technology. The visual stimuli in the information on the display that can act as fixation targets may or may not be sparse and will vary over time. The paper describes an experiment to investigate how the amount of information provided on a display to assist making fixations affects gaze gesture performance. The impact of providing visualization guides and small fixation targets on the time to complete gestures and error rates is presented. The number and durations of fixations made during gesture completion is used to explain differences in performance as a result of practice and direction of eye movement.",
  "authors": [
    {
      "name": "Howell Istance",
      "institution": "University of Tampere, Tampere, Finland",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81100211265",
      "orcid": "missing"
    },
    {
      "name": "Aulikki I. Hyrskykari",
      "institution": "University of Tampere, Tampere, Finland",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81100280101",
      "orcid": "missing"
    }
  ],
  "references": [
    {
      "text": "Deepak Akkil, Andreas Lucero, Jari Kangas, Tero Jokela, Marja Salmimaa, and Roope Raisamo R. 2016. User Expectations of Everyday Gaze Interaction on Smartglasses. In Proceedings of the 9th Nordic Conference on Human-Computer Interaction (NordiCHI '16). ACM, NY, NY, USA, Article 24, 10 pages. DOI: https://doi.org/10.1145/2971485.2971496",
      "doi": "10.1145/2971485.2971496"
    },
    {
      "text": "Mihai B\u00e2ce, Teemu Lepp\u00e4nen, David Gil de Gomez, and Argenis Ramirez Gomez. 2016. ubiGaze: ubiquitous augmented reality messaging using gaze gestures. In SIGGRAPH ASIA 2016 Mobile Graphics and Interactive Applications (SA '16). ACM, NY, NY, USA, , Article 11 , 5 pages. DOI: https://doi.org/10.1145/2999508.2999530",
      "doi": "10.1145/2999508.2999530"
    },
    {
      "text": "Wolfgang Becker. 1991. Saccades. In R. Carpenter, Vision and Visual Dysfunction, vol. 8: Eye Movements. Eye movements (pp. 95--137). London: Macmillan.",
      "doi": ""
    },
    {
      "text": "Nikolaus Bee, and Elisabeth Andre. 2008. Writing with Your Eye: A Dwell Time Free Writing System Adapted to the Nature of Human Eye Gaze. In E. Andr\u00e9, L. Dybkjaer, W. Minker, H. Neumann, R. Pieraccini, & M. Weber, Lecture Notes in Computer Science (Vol. 5078, pp. 111--122). Berlin: Springer  ",
      "doi": "10.1007/978-3-540-69369-7_13"
    },
    {
      "text": "Denis Cousineau. 2005. Confidence intervals in withinsubject designs: A simpler solution to Loftus and Masson's method. Tutorial in Quantitative Methods for Psychology, 1(1), 4--45. http://www.tqmp.org/Content/vol01--1/p042/p042.pdf ",
      "doi": ""
    },
    {
      "text": "Murtaza Dhuliawala, Juyong Lee, Junichi Shimizu, Andreas Bulling, Kai Kunze, Thad Starner, and Woontack Woo. 2016. Smooth eye movement interaction using EOG glasses. In Proceedings of the 18th ACM International Conference on Multimodal Interaction (ICMI 2016). ACM, NY, NY, USA, 307--311. DOI: https://doi.org/10.1145/2993148.2993181",
      "doi": "10.1145/2993148.2993181"
    },
    {
      "text": "Heiko Drewes, and Albrecht Schmidt. (2007) Interacting with the Computer Using Gaze Gestures. In: Baranauskas C., Palanque P., Abascal J., Barbosa S.D.J. (eds) Human-Computer Interaction -- INTERACT 2007. INTERACT 2007. Lecture Notes in Computer Science, vol 4663. Springer, Berlin, Heidelberg ",
      "doi": ""
    },
    {
      "text": "Heiko Drewes, Alexander De Luca, and Albrecht Schmidt. 2007. Eye-gaze interaction for mobile phones. In Proceedings of the 4th international conference on mobile technology, applications, and systems and the 1st international symposium on Computer human interaction in mobile technology (Mobility '07). ACM, NY, NY, USA, 364--371. DOI=http://dx.doi.org/10.1145/1378063.1378122  ",
      "doi": "10.1145/1378063.1378122"
    },
    {
      "text": "Henna Heikkil\u00e4, and Kari-Jouko R\u00e4ih\u00e4. 2009. Speed and accuracy of gaze gestures. Journal of Eye Movement Research, 3(2), pp. 1--14. DOI: http://dx.doi.org/10.16910/jemr.3.2.1",
      "doi": ""
    },
    {
      "text": "Henna Heikkil\u00e4 and Kari-Jouko R\u00e4ih\u00e4. 2012. Simple gaze gestures and the closure of the eyes as an interaction technique. In Proceedings of the Symposium on Eye Tracking Research and Applications (ETRA '12), Stephen N. Spencer (Ed.). ACM, NY, NY, USA, 147--154. DOI=http://dx.doi.org/10.1145/2168556.2168579  ",
      "doi": "10.1145/2168556.2168579"
    },
    {
      "text": "Kenneth Holmqvist, Marcus Nystr\u00f6m, Richard Andersson, Richard Dewhurst, Halszka Jarodzka, and Joost van de Weijer. (2011). Eye tracking: a comprehensive guide to methods and measures. Oxford: Oxford University Press",
      "doi": ""
    },
    {
      "text": "Anke Huckauf and Mario H. Urbina. 2008. Gazing with pEYEs: towards a universal input for various applications. In Proceedings of the 2008 symposium on Eye tracking research & applications (ETRA '08). ACM, NY, NY, USA, 51--54. DOI: https://doi.org/10.1145/1344471.1344483",
      "doi": "10.1145/1344471.1344483"
    },
    {
      "text": "Aulikki Hyrskykari, Howell Istance, and Stephen Vickers. 2012. Gaze gestures or dwell-based interaction? In Proceedings of the Symposium on Eye Tracking Research and Applications (ETRA '12), Stephen N. Spencer (Ed.). ACM, NY, NY, USA, 229--232. DOI=http://dx.doi.org/10.1145/2168556.2168602  ",
      "doi": "10.1145/2168556.2168602"
    },
    {
      "text": "Howell Istance, Richard Bates, Aulikki Hyrskykari, and Stephen Vickers. 2008. Snap clutch, a moded approach to solving the Midas touch problem. In Proceedings of the 2008 symposium on Eye tracking research & applications (ETRA '08). ACM, NY, NY, USA, 221--228. DOI: https://doi.org/10.1145/1344471.1344523",
      "doi": "10.1145/1344471.1344523"
    },
    {
      "text": "Howell Istance, Aulikki Hyrskykari, Lauri Immonen, Santtu Mansikkamaa, and Stephen Vickers. 2010. Designing gaze gestures for gaming: an investigation of performance. In Proceedings of the 2010 Symposium on Eye-Tracking Research & Applications (ETRA '10). ACM, NY, NY, USA, 323--330. DOI=http://dx.doi.org/10.1145/1743666.1743740  ",
      "doi": "10.1145/1743666.1743740"
    },
    {
      "text": "Jari Kangas, Deepak Akkil, Jussi Rantala, Poika Isokoski, P\u00e4ivi Majaranta, and Roope Raisamo. 2014. Gaze gestures and haptic feedback in mobile devices. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '14). ACM, NY, NY, USA, 435--438. DOI: http://dx.doi.org/10.1145/2556288.2557040  ",
      "doi": "10.1145/2556288.2557040"
    },
    {
      "text": "Jari Kangas, Oleg Spakov, Poika Isokoski, Deepak Akkil, Jussi Rantala, and Roope Raisamo. 2016. Feedback for Smooth Pursuit Gaze Tracking Based Control. In Proceedings of the 7th Augmented Human International Conference 2016 (AH '16). ACM, NY, NY, USA, Article 6, 8 pages. DOI: http://dx.doi.org/10.1145/2875194.2875209  ",
      "doi": "10.1145/2875194.2875209"
    },
    {
      "text": "P\u00e4ivi Majaranta, & Andreas Bulling. 2014. Eye Tracking and Eye-Based Human-Computer Interaction. Advances in Phys. Comp., 39--65. Springer, London.",
      "doi": ""
    },
    {
      "text": "Diako Mardanbegi, Dan Witzner Hansen, and Thomas Pederson. 2012. Eye-based head gestures. In Proceedings of the Symposium on Eye Tracking Research and Applications (ETRA '12), Stephen N. Spencer (Ed.). ACM, NY, NY, USA, 139--146. DOI=http://dx.doi.org/10.1145/2168556.2168578  ",
      "doi": "10.1145/2168556.2168578"
    },
    {
      "text": "Emilie Mollenbach, John Paulin Hansen, and Martin Lillholm. 2013. Eye Movements in Gaze Interaction. Journal of Eye Movement Research, 6(2), 1--15.",
      "doi": ""
    },
    {
      "text": "Emilie Mollenbach, John Paulin Hansen, Martin Lillholm, and Alastair G. Gale. 2009. Single stroke gaze gestures. In CHI '09 Extended Abstracts on Human Factors in Computing Systems (CHI EA '09). ACM, NY, NY, USA, 4555--4560. DOI: https://doi.org/10.1145/1520340.1520699",
      "doi": "10.1145/1520340.1520699"
    },
    {
      "text": "Tomi Nukarinen, Jari Kangas, Oleg \u0160pakov, Poika Isokoski, Deepak Akkil, Jussi Rantala, and Roope Raisamo. 2016. Evaluation of HeadTurn: An Interaction Technique Using the Gaze and Head Turns. In Proceedings of the 9th Nordic Conference on HumanComputer Interaction (NordiCHI '16). ACM, NY, NY, USA, , Article 43 , 8 pages. DOI: https://doi.org/10.1145/2971485.2971490",
      "doi": "10.1145/2971485.2971490"
    },
    {
      "text": "Michael Posner, and Yoav Cohen. 1984. Components of visual orienting. Attention and Performance X: Control of Language Processes. H. Bouma and D. Bonwhuis. Hillsdale, N. J., Erlbaum: 551--556",
      "doi": ""
    },
    {
      "text": "Keith Rayner. 1998. Eye movements in reading and information processing: 20 years of research. Psychological Bulletin, 85, 618--660. ",
      "doi": ""
    },
    {
      "text": "Keith Rayner. 1995. Eye movements and cognitive processes in reading, visual search, and scene perception. In J. M. Findlay, R. Walker, & R.W. Kentridge (Eds.), Eye movement research: Mechanisms, processes and applications (pp. 3--22). Amsterdam: North Holland. ",
      "doi": ""
    },
    {
      "text": "David Rozado, Javier S. Agustin, Francisco B. Rodriguez, and Pablo Varona. 2012. Gliding and saccadic gaze gesture recognition in real time. ACM Trans. Interact. Intell. Syst. 1, 2, Article 10 (January 2012), 27 pages. DOI=http://dx.doi.org/10.1145/2070719.2070723  ",
      "doi": "10.1145/2070719.2070723"
    },
    {
      "text": "SensoMotoric Instruments. SMI High Performance eye tracking hmd based on HTC Vive. http://www.smivision.com/en/gaze-and-eye-trackingsystems/products/eye-tracking-htcvive.html?gclid=CMC9gzcndECFQcbaQoduK4Caw&cHash=bd18fc124ce9088b 07a26a494fd2bed7",
      "doi": ""
    },
    {
      "text": "Jing Tian, Howard Ying, and David Zee. 2013. Revisiting corrective saccades: role of visual feedback. Vision Research. August 30; 89: pp. 54--64",
      "doi": ""
    },
    {
      "text": "Jacob O. Wobbrock, James Rubinstein, Michael W. Sawyer, and Andrew T. Duchowski. 2008. Longitudinal evaluation of discrete consecutive gaze gestures for text entry. In Proceedings of the 2008 symposium on Eye tracking research & applications (ETRA '08). ACM, NY, NY, USA, 11--18. DOI: https://doi.org/10.1145/1344471.1344475",
      "doi": "10.1145/1344471.1344475"
    },
    {
      "text": "Xianjun Sam Zheng, Cedric Foucault, Patrik Matos da Silva, Siddharth Dasari, Tao Yang, and Stuart Goose. 2015. Eye-Wearable Technology for Machine Maintenance: Effects of Display Position and Handsfree Operation. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI '15). ACM, NY, NY, USA, 2125--2134. DOI: http://dx.doi.org/10.1145/2702123.2702305  ",
      "doi": "10.1145/2702123.2702305"
    }
  ]
}