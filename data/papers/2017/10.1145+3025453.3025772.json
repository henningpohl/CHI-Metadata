{
  "doi": "10.1145/3025453.3025772",
  "title": "Close to the Action: Eye-Tracking Evaluation of Speaker-Following Subtitles",
  "published": "2017-05-02",
  "proctitle": "CHI '17: Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems",
  "pages": "6559-6568",
  "year": 2017,
  "badges": [],
  "abstract": "The incorporation of subtitles in multimedia content plays an important role in communicating spoken content. For example, subtitles in the respective language are often preferred to expensive audio translation of foreign movies. The traditional representation of subtitles displays text centered at the bottom of the screen. This layout can lead to large distances between text and relevant image content, causing eye strain and even that we miss visual content. As a recent alternative, the technique of speaker-following subtitles places subtitle text in speech bubbles close to the current speaker. We conducted a controlled eye-tracking laboratory study (n = 40) to compare the regular approach (center-bottom subtitles) with content-sensitive, speaker-following subtitles. We compared different dialog-heavy video clips with the two layouts. Our results show that speaker-following subtitles lead to higher fixation counts on relevant image regions and reduce saccade length, which is an important factor for eye strain.",
  "authors": [
    {
      "name": "Kuno Kurzhals",
      "institution": "University of Stuttgart, Stuttgart, Germany",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "84459518557",
      "orcid": "missing"
    },
    {
      "name": "Emine Cetinkaya",
      "institution": "University of Stuttgart, Stuttgart, Germany",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659160850",
      "orcid": "missing"
    },
    {
      "name": "Yongtao Hu",
      "institution": "The University of Hong Kong, Hong Kong, China",
      "img": "/do/10.1145/contrib-99658682933/rel-imgonly/me.png",
      "acmid": "99658682933",
      "orcid": "missing"
    },
    {
      "name": "Wenping Wang",
      "institution": "The University of Hong Kong, Hong Kong, Hong Kong",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81451595839",
      "orcid": "missing"
    },
    {
      "name": "Daniel Weiskopf",
      "institution": "University of Stuttgart, Stuttgart, Germany",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81100214723",
      "orcid": "missing"
    }
  ],
  "references": [
    {
      "text": "W. Akahori, T. Hirai, S. Kawamura, and S. Morishima. Region-of-interest-based subtitle placement using eye-tracking data of multiple viewers. In Proceedings of the ACM International Conference on Interactive Experiences for TV and Online Video, pages 123--128, 2016.  ",
      "doi": "10.1145/2932206.2933558"
    },
    {
      "text": "A. T. Bahill and L. Stark. Overlapping saccades and glissades are produced by fatigue in the saccadic eye movement system. Experimental Neurology, 48(1):95--106, 1975.",
      "doi": ""
    },
    {
      "text": "P. Baudisch, D. DeCarlo, A. T. Duchowski, and W. S. Geisler. Focusing on the essential: Considering attention in display design. Communications of the ACM, 46(3):60--66, 2003.  ",
      "doi": "10.1145/636772.636799"
    },
    {
      "text": "R. Bednarik, H. Vrzakova, and M. Hradis. What do you want to do next: A novel approach for intent prediction in gaze-based interaction. In Proceedings of the Symposium on Eye Tracking Research and Applications, pages 83--90, 2012.  ",
      "doi": "10.1145/2168556.2168569"
    },
    {
      "text": "L. Bergen, T. Grimes, and D. Potter. How attention partitions itself during simultaneous message presentations. Human Communication Research, 31(3):311--336, 2005.",
      "doi": ""
    },
    {
      "text": "M.-J. Bisson, W. J. Van Heuven, K. Conklin, and R. J. Tunney. Processing of native and foreign language subtitles in films: An eye tracking study. Applied Psycholinguistics, 35(2):399--418, 2014.",
      "doi": ""
    },
    {
      "text": "A. Brown, R. Jones, M. Crabb, J. Sandford, M. Brooks, M. Armstrong, and C. Jay. Dynamic subtitles: The user experience. In Proceedings of the ACM International Conference on Interactive Experiences for TV and Online Video, pages 103--112, 2015.  ",
      "doi": "10.1145/2745197.2745204"
    },
    {
      "text": "D. Chiaro, C. Heiss, and C. Bucaria, editors. Between Text and Image: Updating Research in Screen Translation, volume 78. John Benjamins Publishing, 2008.",
      "doi": ""
    },
    {
      "text": "G. d'Ydewalle and W. De Bruycker. Eye movements of children and adults while reading television subtitles. European Psychologist, 12(3):196--205, 2007.",
      "doi": ""
    },
    {
      "text": "G. d'Ydewalle, C. Praet, K. Verfaillie, and J. Van Rensbergen. Watching subtitled television automatic reading behavior. Communication Research, 18(5):650--666, 1991.",
      "doi": ""
    },
    {
      "text": "G. d'Ydewalle, J. Van Rensbergen, and J. Pollet. Reading a message when the same message is available auditorily in another language: The case of subtitling. In J. O'Regan and A. L\u00e9vi-Schoen, editors, Eye Movements: From Psychology to Cognition, pages 313--321. Elsevier Science Publishers, 1987.",
      "doi": ""
    },
    {
      "text": "P. M. Fitts. The information capacity of the human motor system in controlling the amplitude of movement. Journal of Experimental Psychology, 47(6):381--391, 1954.",
      "doi": ""
    },
    {
      "text": "T. Foulsham and L. A. Sanderson. Look who's talking? sound changes gaze behaviour in a dynamic social scene. Visual Cognition, 21(7):922--944, 2013.",
      "doi": ""
    },
    {
      "text": "D. Holman, R. Vertegaal, C. Sohn, and D. Cheng. Attentive display: Paintings as attentive user interfaces. In CHI Extended Abstracts on Human Factors in Computing Systems, pages 1127--1130, 2004.  ",
      "doi": "10.1145/985921.986005"
    },
    {
      "text": "K. Holmqvist, M. Nystr\u00f6m, R. Andersson, R. Dewhurst, H. Jarodzka, and J. Van de Weijer. Eye Tracking: A Comprehensive Guide to Methods and Measures. Oxford University Press, 2011.",
      "doi": ""
    },
    {
      "text": "R. Hong, M. Wang, M. Xu, S. Yan, and T.-S. Chua. Dynamic captioning: Video accessibility enhancement for hearing impairment. In Proceedings of the ACM International Conference on Multimedia, pages 421--430, 2010.  ",
      "doi": "10.1145/1873951.1874013"
    },
    {
      "text": "Y. Hu, J. Kautz, Y. Yu, and W. Wang. Speaker-following video subtitles. ACM Transactions on Multimedia Computing, Communications, and Applications, 11(2):32:1--32:17, 2015.  ",
      "doi": "10.1145/2632111"
    },
    {
      "text": "F. Karamitroglou. A proposed set of subtitling standards in Europe. Translation Journal, 2(2):1--15, 1998.",
      "doi": ""
    },
    {
      "text": "C. M. Koolstra, A. L. Peeters, and H. Spinhof. The pros and cons of dubbing and subtitling. European Journal of Communication, 17(3):325--354, 2002.",
      "doi": ""
    },
    {
      "text": "B. Kothari, J. Takeda, A. Joshi, and A. Pandey. Same language subtitling: a butterfly for literacy? International Journal of Lifelong Education, 21(1):55--66, 2002.",
      "doi": ""
    },
    {
      "text": "I. Krejtz, A. Szarkowska, and K. Krejtz. The effects of shot changes on eye movements in subtitling. Journal of Eye Movement Research, 6(5):1--12, 2013.",
      "doi": ""
    },
    {
      "text": "J.-L. Kruger, E. Hefer, and G. Matthew. Measuring the impact of subtitles on cognitive load: Eye tracking and dynamic audiovisual texts. In Proceedings of the Conference on Eye Tracking South Africa, pages 62--66, 2013.  ",
      "doi": "10.1145/2509315.2509331"
    },
    {
      "text": "J.-L. Kruger and F. Steyn. Subtitles and eye tracking: Reading and performance. Reading Research Quarterly, 49(1):105--120, 2014.",
      "doi": ""
    },
    {
      "text": "K. Kurzhals, F. Heimerl, and D. Weiskopf. ISeeCube: Visual analysis of gaze data for video. In Proceedings of the Symposium on Eye Tracking Research and Applications, pages 43--50, 2014.  ",
      "doi": "10.1145/2578153.2578158"
    },
    {
      "text": "K. Kurzhals and D. Weiskopf. Space-time visual analytics of eye-tracking data for dynamic stimuli. IEEE Transactions on Visualization and Computer Graphics, 19(12):2129--2138, 2013.  ",
      "doi": "10.1109/TVCG.2013.194"
    },
    {
      "text": "D. Miniotas. Application of Fitts' law to eye gaze interaction. In CHI Extended Abstracts on Human Factors in Computing Systems, pages 339--340, 2000.  ",
      "doi": "10.1145/633292.633496"
    },
    {
      "text": "P. Mohr, B. Kerbl, M. Donoser, D. Schmalstieg, and D. Kalkofen. Retargeting technical documentation to augmented reality. In Proceedings of the ACM Conference on Human Factors in Computing Systems, pages 3337--3346, 2015.  ",
      "doi": "10.1145/2702123.2702490"
    },
    {
      "text": "E. Perego, F. Del Missier, M. Porta, and M. Mosconi. The cognitive effectiveness of subtitle processing. Media Psychology, 13(3):243--272, 2010.",
      "doi": ""
    },
    {
      "text": "A. Poole and L. Ball. Eye tracking in human-computer interaction and usability research: Current status and future prospects. In R. . D. Hyona, editor, The Mind's Eye: Cognitive and Applied Aspects of Eye Movement Research, pages 573--605. Elsevier Science, 2003.",
      "doi": ""
    },
    {
      "text": "D. J. Rajendran, A. T. Duchowski, P. Orero, J. Mart\u00ednez, and P. Romero-Fresco. Effects of text chunking on subtitling: A quantitative and qualitative examination. Perspectives, 21(1):5--21, 2013.",
      "doi": ""
    },
    {
      "text": "K. Rayner. The perceptual span and peripheral cues in reading. Cognitive Psychology, 7(1):65--81, 1975.",
      "doi": ""
    },
    {
      "text": "N. M. Ross and E. Kowler. Eye movements while viewing narrated, captioned, and silent videos. Journal of Vision, 13(4):1--1, 2013.",
      "doi": ""
    },
    {
      "text": "A. Szarkowska, I. Krejtz, Z. Klyszejko, and A. Wieczorek. Verbatim, standard, or edited?: Reading patterns of different captioning styles among deaf, hard of hearing, and hearing viewers. American Annals of the Deaf, 156(4):363--378, 2011.",
      "doi": ""
    },
    {
      "text": "E. E. Veas, E. Mendez, S. K. Feiner, and D. Schmalstieg. Directing attention and influencing memory with visual saliency modulation. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 1471--1480, 2011.  ",
      "doi": "10.1145/1978942.1979158"
    }
  ]
}