{
  "doi": "10.1145/3313831.3376615",
  "title": "COGAM: Measuring and Moderating Cognitive Load in Machine Learning Model Explanations",
  "published": "2020-04-23",
  "proctitle": "CHI '20: Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems",
  "pages": "1-14",
  "year": 2020,
  "badges": [],
  "abstract": "Interpretable machine learning models trade -off accuracy for simplicity to make explanations more readable and easier to comprehend. Drawing from cognitive psychology theories in graph comprehension, we formalize readability as visual cognitive chunks to measure and moderate the cognitive load in explanation visualizations. We present Cognitive-GAM (COGAM) to generate explanations with desired cognitive load and accuracy by combining the expressive nonlinear generalized additive models (GAM) with simpler sparse linear models. We calibrated visual cognitive chunks with reading time in a user study, characterized the trade-off between cognitive load and accuracy for four datasets in simulation studies, and evaluated COGAM against baselines with users. We found that COGAM can decrease cognitive load without decreasing accuracy and/or increase accuracy without increasing cognitive load. Our framework and empirical measurement instruments for cognitive load will enable more rigorous assessment of the human interpretability of explainable AI.",
  "authors": [
    {
      "name": "Ashraf Abdul",
      "institution": "National University of Singapore, Singapore, Singapore",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659146624",
      "orcid": "missing"
    },
    {
      "name": "Christian von der Weth",
      "institution": "National University of Singapore, Singapore, Singapore",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81453618445",
      "orcid": "missing"
    },
    {
      "name": "Mohan Kankanhalli",
      "institution": "National University of Singapore, Singapore, Singapore",
      "img": "/do/10.1145/contrib-81100562305/rel-imgonly/img1c_20_5_2013.jpg",
      "acmid": "81100562305",
      "orcid": "missing"
    },
    {
      "name": "Brian Y. Lim",
      "institution": "National University of Singapore, Singapore, Singapore",
      "img": "/do/10.1145/contrib-81416601689/rel-imgonly/brian_lim_2015_-_square.jpg",
      "acmid": "81416601689",
      "orcid": "0000-0002-0543-2414"
    }
  ],
  "references": [
    {
      "text": "Ashraf Abdul, Jo Vermeulen, Danding Wang, Brian Y. Lim, and Mohan Kankanhalli. 2018. Trends and trajectories for explainable, accountable and intelligible systems: An HCI research agenda. Conference on Human Factors in Computing Systems - Proceedings 2018-April. https://doi.org/10.1145/3173574.3174156",
      "doi": "10.1145/3173574.3174156"
    },
    {
      "text": "Nadia Ali and David Peebles. 2013. The effect of Gestalt laws of perceptual organization on the comprehension of three-variable bar and line graphs. Human Factors 55, 1: 183--203. https://doi.org/10.1177/0018720812452592",
      "doi": ""
    },
    {
      "text": "Saleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira Nushi, Penny Collisson, Jina Suh, Shamsi Iqbal, Paul N. Bennett, Kori Inkpen, Jaime Teevan, Ruth Kikin-Gil, and Eric Horvitz. 2019. Guidelines for human-AI interaction. In Conference on Human Factors in Computing Systems - Proceedings. https://doi.org/10.1145/3290605.3300233",
      "doi": "10.1145/3290605.3300233"
    },
    {
      "text": "Niels V A N Berkel, Jorge Goncalves, Danula Hettiachchi, Senuri Wijenayake, Ryan Kelly, and Vassilis Kostakos. 2019. Crowdsourcing Perceptions of Fair Predictors for Machine Learning?: A Recidivism Case Study. 1, 1.",
      "doi": ""
    },
    {
      "text": "Harald Binder and Gerhard Tutz. 2008. A comparison of methods for the fitting of generalized additive models. Statistics and Computing 18, 1: 87--99. https://doi.org/10.1007/s11222-007--9040-0",
      "doi": "10.1007/s11222-007-9040-0"
    },
    {
      "text": "Carrie J. Cai, Jonas Jongejan, and Jess Holbrook. 2019. The effects of example-based explanations in a machine learning interface. In International Conference on Intelligent User Interfaces, Proceedings IUI. https://doi.org/10.1145/3301275.3302289",
      "doi": "10.1145/3301275.3302289"
    },
    {
      "text": "C. Melody Carswell, Cathy Emery, and Andrea M. Lonon. 1993. Stimulus complexity and information integration in the spontaneous interpretations of line graphs. Applied Cognitive Psychology 7, 4: 341--357. https://doi.org/10.1002/acp.2350070407",
      "doi": ""
    },
    {
      "text": "Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie Elhadad. 2015. Intelligible Models for HealthCare. https://doi.org/10.1145/2783258.2788613",
      "doi": ""
    },
    {
      "text": "Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and No\u00e9mie Elhadad. 2015. Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission. In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. https://doi.org/10.1145/2783258.2788613",
      "doi": "10.1145/2783258.2788613"
    },
    {
      "text": "Alexandra Chouldechova and Trevor Hastie. 2015. Generalized Additive Model Selection. 1--24. Retrieved from http://arxiv.org/abs/1506.03850",
      "doi": ""
    },
    {
      "text": "Dean De Cock. 2011. Ames, Iowa?: Alternative to the Boston Housing Data as an End of Semester Regression Project. Journal of Statistics Education.",
      "doi": ""
    },
    {
      "text": "Paulo Cortez, Ant\u00f3nio Cerdeira, Fernando Almeida, Telmo Matos, and Jos\u00e9 Reis. 2009. Modeling wine preferences by data mining from physicochemical properties. Decision Support Systems. https://doi.org/10.1016/j.dss.2009.05.016",
      "doi": ""
    },
    {
      "text": "Andy Cotgreave. Lollipop Charts. Retrieved May 1, 2019 from https://gravyanecdote.com/andycotgreave/lollipop-charts/",
      "doi": ""
    },
    {
      "text": "Meredyth Daneman and Patricia A. Carpenter. 1980. Individual differences in working memory and reading. Journal of Verbal Learning and Verbal Behavior. https://doi.org/10.1016/s0022--5371(80)90312--6",
      "doi": ""
    },
    {
      "text": "Robyn M. Dawes. 1979. The robust beauty of improper linear models in decision making. American Psychologist. https://doi.org/10.1037/0003066X.34.7.571",
      "doi": ""
    },
    {
      "text": "Finale Doshi-velez and Been Kim. 2017. A Roadmap for a Rigorous Science of Interpretability. 1--13. https://doi.org/10.1016/j.intell.2013.05.008",
      "doi": ""
    },
    {
      "text": "Finale Doshi-Velez, Mason Kortz, Ryan Budish, Christopher Bavitz, Samuel J. Gershman, David O'Brien, Stuart Shieber, Jim Waldo, David Weinberger, and Alexandra Wood. 2017. Accountability of AI Under the Law: The Role of Explanation. SSRN Electronic Journal. https://doi.org/10.2139/ssrn.3064761",
      "doi": ""
    },
    {
      "text": "Upol Ehsan, Pradyumna Tambwekar, Larry Chan, Brent Harrison, and Mark O. Riedl. 2019. Automated rationale generation: A technique for explainable AI and its effects on human perceptions. In International Conference on Intelligent User Interfaces, Proceedings IUI. https://doi.org/10.1145/3301275.3302316",
      "doi": ""
    },
    {
      "text": "Charles R. Fletcher. 1981. Short-term memory processes in text comprehension. Journal of Verbal Learning and Verbal Behavior. https://doi.org/10.1016/S0022--5371(81)90183--3",
      "doi": ""
    },
    {
      "text": "Meadhbh I. Foster and Mark T. Keane. 2019. The Role of Surprise in Learning: Different Surprising Outcomes Affect Memorability Differentially. Topics in Cognitive Science. https://doi.org/10.1111/tops.12392",
      "doi": ""
    },
    {
      "text": "Jerome H. Friedman. 2001. Greedy function approximation: A gradient boosting machine. Annals of Statistics.",
      "doi": ""
    },
    {
      "text": "David Gunning and David Aha. 2019. DARPA's Explainable Artificial Intelligence (XAI) Program. AI Magazine. https://doi.org/10.1609/aimag.v40i2.2850",
      "doi": ""
    },
    {
      "text": "David Harrison and Daniel L. Rubinfeld. 1978. Hedonic housing prices and the demand for clean air. Journal of Environmental Economics and Management. https://doi.org/10.1016/00950696(78)90006--2",
      "doi": ""
    },
    {
      "text": "Trevor Hastie and Robert Tibshirani. 1986. Generalized additive models. Statistical Science 1, 3: 297--310. https://doi.org/10.1214/ss/1177013604",
      "doi": ""
    },
    {
      "text": "Trevor Hastie and Robert Tibshirani. 1986. Generalized additive models. Statistical Science. https://doi.org/10.1214/ss/1177013604",
      "doi": ""
    },
    {
      "text": "Fred Hohman, Andrew Head, Rich Caruana, Robert DeLine, and Steven M. Drucker. 2019. Gamut: A design probe to understand how data scientists understand machine learning models. Conference on Human Factors in Computing Systems - Proceedings. https://doi.org/10.1145/3290605.3300809",
      "doi": ""
    },
    {
      "text": "Fred Hohman, Arjun Srinivasan, and Steven M Drucker. 2019. TELEGAM: Combining Visualization and Verbalization for Interpretable Machine Learning. In IEEE Visualization Conference (VIS).",
      "doi": ""
    },
    {
      "text": "Weidong Huang, Peter Eades, and Seok Hee Hong. 2009. Measuring effectiveness of graph visualizations: A cognitive load perspective. Information Visualization 8, 3: 139--152. https://doi.org/10.1057/ivs.2009.10",
      "doi": "10.1057/ivs.2009.10"
    },
    {
      "text": "Minsuk Kahng, Pierre Y. Andrews, Aditya Kalro, and Duen Horng Polo Chau. 2018. ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models. IEEE Transactions on Visualization and Computer Graphics 24, 1: 88--97. https://doi.org/10.1109/TVCG.2017.2744718",
      "doi": ""
    },
    {
      "text": "Walter Kintsch and Teun A. van Dijk. 1978. Toward a model of text comprehension and production. Psychological Review. https://doi.org/10.1037/0033295X.85.5.363",
      "doi": ""
    },
    {
      "text": "Josua Krause, Adam Perer, and Kenney Ng. 2016. Interacting with predictions: Visual inspection of black-box machine learning models. In Conference on Human Factors in Computing Systems - Proceedings. https://doi.org/10.1145/2858036.2858529",
      "doi": "10.1145/2858036.2858529"
    },
    {
      "text": "Todd Kulesza, Margaret Burnett, Weng-Keen Wong, and and Simone Stumpf. Principles of explanatory debugging to personalize interactive machine learning. In Proceedings of the 20th International Conference on Intelligent User Interfaces. ACM, 126--137.",
      "doi": ""
    },
    {
      "text": "Sukwon Lee, Sung Hee Kim, and Bum Chul Kwon. 2017. VLAT: Development of a Visualization Literacy Assessment Test. IEEE Transactions on Visualization and Computer Graphics. https://doi.org/10.1109/TVCG.2016.2598920",
      "doi": ""
    },
    {
      "text": "Brian Y. Lim and Anind K. Dey. 2012. Weights of evidence for intelligible smart environments. In UbiComp'12 - Proceedings of the 2012 ACM Conference on Ubiquitous Computing.",
      "doi": ""
    },
    {
      "text": "Zachary C. Lipton. 2018. The mythos of model interpretability. Communications of the ACM 61, 35-- 43. https://doi.org/10.1145/3233231",
      "doi": "10.1145/3233231"
    },
    {
      "text": "Tania Lombrozo. The structure and function of explanations. Trends in cognitive sciences (Elsevier) 10, 10: 464--470.",
      "doi": ""
    },
    {
      "text": "Tania Lombrozo. 2007. Simplicity and probability in causal explanation. Cognitive Psychology. https://doi.org/10.1016/j.cogpsych.2006.09.006",
      "doi": ""
    },
    {
      "text": "Yin Lou, Jacob Bien, Rich Caruana, and Johannes Gehrke. 2016. Sparse Partially Linear Additive Models. Journal of Computational and Graphical Statistics 25, 4: 1126--1140. https://doi.org/10.1080/10618600.2015.1089775",
      "doi": ""
    },
    {
      "text": "Yin Lou, Rich Caruana, and Johannes Gehrke. 2012. Intelligible models for classification and regression. In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. https://doi.org/10.1145/2339530.2339556",
      "doi": "10.1145/2339530.2339556"
    },
    {
      "text": "Scott M. Lundberg and Su In Lee. 2017. A unified approach to interpreting model predictions. In Advances in Neural Information Processing Systems.",
      "doi": ""
    },
    {
      "text": "Tim Miller. 2019. Explanation in artificial intelligence: Insights from the social sciences. Artificial Intelligence 267: 1--38. https://doi.org/10.1016/j.artint.2018.07.007",
      "doi": ""
    },
    {
      "text": "Yao Ming, Huamin Qu, and Enrico Bertini. 2019. RuleMatrix: Visualizing and Understanding Classifiers with Rules. IEEE Transactions on Visualization and Computer Graphics. https://doi.org/10.1109/TVCG.2018.2864812",
      "doi": ""
    },
    {
      "text": "Menaka Narayanan, Emily Chen, Jeffrey He, Been Kim, Sam Gershman, and Finale Doshi-Velez. 2018. How do Humans Understand Explanations from Machine Learning Systems? An Evaluation of the Human-Interpretability of Explanation. 1--21. Retrieved from http://arxiv.org/abs/1802.00682",
      "doi": ""
    },
    {
      "text": "Rich Nori, Harsha and Jenkins, Samuel and Koch, Paul and Caruana. 2019. InterpretML: A Unified Framework for Machine Learning Interpretability. arXiv preprint arXiv:1909.09223.",
      "doi": ""
    },
    {
      "text": "Unaizah Hanum Binti Obaidellah. 2012. The role of chunking and schemas in learning and drawing. PQDT - UK & Ireland: 1. Retrieved from http://search.proquest.com/docview/1442498898?accountid=10673%5Cnhttp://openurl.ac.uk/athens:_edu?url_ver=Z39.882004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation& genre=dissertations+%26+theses&sid=ProQ:ProQuest +Dissertations+%26+Theses+Global&atitle=&ti",
      "doi": ""
    },
    {
      "text": "Steven Pinker. 1990. A theory of graph comprehension. Artificial intelligence and the future of testing. https://doi.org/10.1145/2046684.2046699",
      "doi": ""
    },
    {
      "text": "M T Ribeiro and S.Singh and C. Guestrin. Why Should I Trust You?: Explaining the Predictions of Any Classifier,. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.",
      "doi": ""
    },
    {
      "text": "Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2018. Anchors: High-precision modelagnostic explanations. 32nd AAAI Conference on Artificial Intelligence, AAAI 2018: 1527--1535.",
      "doi": ""
    },
    {
      "text": "Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2018. Anchors: High-precision modelagnostic explanations. In 32nd AAAI Conference on Artificial Intelligence, AAAI 2018.",
      "doi": ""
    },
    {
      "text": "Sergio Della Sala, Colin Gray, Alan Baddeley, Nadia Allamano, and Lindsey Wilson. 1999. Pattern span: A tool for unwelding visuo-spatial memory. Neuropsychologia. https://doi.org/10.1016/S00283932(98)00159--6",
      "doi": ""
    },
    {
      "text": "Annett Schmeck, Maria Opfermann, Tamara van Gog, Fred Paas, and Detlev Leutner. 2015. Measuring cognitive load with subjective rating scales during problem solving: differences between immediate and delayed ratings. Instructional Science 43, 1: 93--114. https://doi.org/10.1007/s11251-014--9328--3",
      "doi": ""
    },
    {
      "text": "Kilian G. Seeber. 2011. Cognitive load in simultaneous interpreting: Existing theories - new models. InterpretingInterpreting International Journal of Research and Practice in Interpreting. https://doi.org/10.1075/intp.13.2.02see",
      "doi": ""
    },
    {
      "text": "Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. 2017. Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization. In Proceedings of the IEEE International Conference on Computer Vision. https://doi.org/10.1109/ICCV.2017.74",
      "doi": ""
    },
    {
      "text": "Daniel Serv\u00e9n, Charlie Brummitt, and Hassan Abedi. 2018. pyGAM: Generalized Additive Models in Python. Zenodo. https://doi.org/10.5281/zenodo.1476122",
      "doi": ""
    },
    {
      "text": "Rita Sevastjanova, Fabian Beck, Basil Ell, Cagatay Turkay, Rafael Henkin, Miriam Butt, Daniel Keim, and Mennatallah El-Assady. 2018. Going beyond Visualization: Verbalization as Complementary Medium to Explain Machine Learning Models. In Proc. of IEEE VIS Workshop on Visualization for AI Explainability (VISxAI).",
      "doi": ""
    },
    {
      "text": "Priti Shah and Patricia A. Carpenter. 1995. Conceptual Limitations in Comprehending Line Graphs. Journal of Experimental Psychology: General 124, 1: 43--61. https://doi.org/10.1037/0096--3445.124.1.43",
      "doi": ""
    },
    {
      "text": "Priti Shah and Eric G. Freedman. 2011. Bar and line graph comprehension: An interaction of top-down and bottom-up processes. Topics in Cognitive Science 3, 3: 560--578. https://doi.org/10.1111/j.17568765.2009.01066.x",
      "doi": ""
    },
    {
      "text": "Shah Priti and Hoeffner James. 2002. Review of Graph Comprehension Research:\\nImplications for Instruction\\n. Educational Psychology Review 14, 1: 47--69. Retrieved from http://www.springerlink.com/content/v2581778612k5 432/?MUD=MP",
      "doi": ""
    },
    {
      "text": "Benjamin Strobel, Marlit Annalena Lindner, Steffani Sa\u00df, and Olaf K\u00f6ller. 2016. Do graph readers prefer the graph type most suited to a given task? Insights from eye tracking. Journal of Eye Movement Research 9, 4. https://doi.org/10.16910/jemr.9.4.4",
      "doi": ""
    },
    {
      "text": "Justin Talbot, Vidya Setlur, and Anushka Anand. 2014. Four experiments on the perception of bar charts. IEEE Transactions on Visualization and Computer Graphics. https://doi.org/10.1109/TVCG.2014.2346320",
      "doi": ""
    },
    {
      "text": "Sarah Tan, Rich Caruana, Giles Hooker, and Yin Lou. 2018. Distill-and-Compare: Auditing Black-Box Models Using Transparent Model Distillation. In AIES 2018 - Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society. https://doi.org/10.1145/3278721.3278725",
      "doi": ""
    },
    {
      "text": "Berk Ustun and Cynthia Rudin. 2016. Supersparse linear integer models for optimized medical scoring systems. In Machine Learning. 349--391. https://doi.org/10.1007/s10994-015--5528--6",
      "doi": ""
    },
    {
      "text": "Nadya Vasilyeva, Daniel Wilkenfeld, and Tania Lombrozo. 2017. Contextual utility affects the perceived quality of explanations. Psychonomic Bulletin and Review. https://doi.org/10.3758/s13423017--1275-y",
      "doi": ""
    },
    {
      "text": "Sandra Wachter, Brent Mittelstadt, and Chris Russell. 2017. Counterfactual Explanations Without Opening the Black Box: Automated Decisions and the GDPR. SSRN Electronic Journal: 1--52. https://doi.org/10.2139/ssrn.3063289",
      "doi": ""
    },
    {
      "text": "Danding Wang, Qian Yang, Ashraf Abdul, and Brian Y. Lim. 2019. Designing theory-driven user-centric explainable AI. In Conference on Human Factors in Computing Systems - Proceedings. https://doi.org/10.1145/3290605.3300831",
      "doi": ""
    },
    {
      "text": "Danding Wang, Qian Yang, Ashraf Abdul, and Brian Y. Lim. 2019. Designing Theory-Driven User-Centric Explainable AI. Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems - CHI '19: 1--15. https://doi.org/10.1145/3290605.3300831",
      "doi": ""
    },
    {
      "text": "Andrzej Wodecki, Gregory C Allen, Michael C Horowitz, Elsa B Kania, Paul Scharre, H James Wilson, Paul R Daugherty, Nicola Morini-Bianzino, Rishie Sharma, Johan Boye, Chris Reed, David Gunning, Darpa Io, A I System, Andrew J Fawkes, Jacques Bughin, Eric Hazan, Sree Ramaswamy, Michael Chui, Tera Allas, Peter Dahlstrom, Nicolas Henke, Monica Trench, Corinne Cath, Sandra Wachter, Brent Mittelstadt, Mariarosaria Taddeo, Luciano Floridi, Gregory C Allen, T. Chan, Bo-hu Li, Bao-cun Hou, Wen-tao Yu, Xiao-bing Lu, Chun-wei Yang, Derwin Suhartono Budihartono, widodo, and Jatin Borana. 2017. Explainable Artificial Intelligence ( XAI ) The Need for Explainable AI. Philosophical transactions. Series A, Mathematical, physical, and engineering sciences 58, 2: 4. https://doi.org/10.1111/fct.12208",
      "doi": ""
    },
    {
      "text": "Simon N. Wood. 2017. Generalized additive models: An introduction with R, second edition. https://doi.org/10.1201/9781315370279",
      "doi": ""
    },
    {
      "text": "Mike Wu, Michael C. Hughes, Sonali Parbhoo, Maurizio Zazzi, Volker Roth, and Finale Doshi-Velez. 2018. Beyond sparsity: Tree regularization of deep models for interpretability. In 32nd AAAI Conference on Artificial Intelligence, AAAI 2018.",
      "doi": ""
    },
    {
      "text": "Xiaoming Xi. 2010. Aspects of performance on line graph description tasks: Influenced by graph familiarity and different task features. Language Testing 27, 1: 73-- 100. https://doi.org/10.1177/0265532209346454",
      "doi": ""
    },
    {
      "text": "Hongyu Yang, Cynthia Rudin, and Margo Seltzer. 2017. Scalable Bayesian rule lists. In 34th International Conference on Machine Learning, ICML 2017.",
      "doi": "10.5555/3305890.3306086"
    },
    {
      "text": "I. C. Yeh. 1998. Modeling of strength of highperformance concrete using artificial neural networks. Cement and Concrete Research. https://doi.org/10.1016/S0008--8846(98)00165--3",
      "doi": ""
    },
    {
      "text": "Jiaming Zeng, Berk Ustun, and Cynthia Rudin. 2017. Interpretable classification models for recidivism prediction. Journal of the Royal Statistical Society. Series A: Statistics in Society. https://doi.org/10.1111/rssa.12227",
      "doi": ""
    }
  ]
}