{
  "doi": "10.1145/3313831.3376579",
  "title": "Leveraging Error Correction in Voice-based Text Entry by Talk-and-Gaze",
  "published": "2020-04-23",
  "proctitle": "CHI '20: Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems",
  "pages": "1-11",
  "year": 2020,
  "badges": [],
  "abstract": "We present the design and evaluation of Talk-and-Gaze (TaG), a method for selecting and correcting errors with voice and gaze. TaG uses eye gaze to overcome the inability of voice-only systems to provide spatial information. The user's point of gaze is used to select an erroneous word either by dwelling on the word for 800 ms (D-TaG) or by uttering a \"select\" voice command (V-TaG). A user study with 12 participants compared D-TaG, V-TaG, and a voice-only method for selecting and correcting words. Corrections were performed more than 20% faster with D-TaG compared to the V-TaG or voice-only methods. As well, D-TaG was observed to require 24% less selection effort than V-TaG and 11% less selection effort than voice-only error correction. D-TaG was well received in a subjective assessment with 66% of users choosing it as their preferred choice for error correction in voice-based text entry.",
  "tags": [
    "voice",
    "eye tracking",
    "interaction design",
    "multimodal",
    "text entry",
    "usability"
  ],
  "authors": [
    {
      "name": "Korok Sengupta",
      "institution": "University of Koblenz-Landau, Koblenz, Germany",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659081734",
      "orcid": "missing"
    },
    {
      "name": "Sabin Bhattarai",
      "institution": "University of Koblenz-Landau, Koblenz, Germany",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659526546",
      "orcid": "missing"
    },
    {
      "name": "Sayan Sarcar",
      "institution": "University of Tsukuba, Tsukuba, Ibaraki, Japan",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81550967756",
      "orcid": "missing"
    },
    {
      "name": "I. Scott MacKenzie",
      "institution": "York University, Toronto, ON, Canada",
      "img": "/do/10.1145/contrib-81100367406/rel-imgonly/me8.jpg",
      "acmid": "81100367406",
      "orcid": "missing"
    },
    {
      "name": "Steffen Staab",
      "institution": "Universit\u00e4t Stuttgart & University of Southampton, Koblenz, Germany",
      "img": "/do/10.1145/contrib-81409593685/rel-imgonly/akademiepreis-portrait-staab.png",
      "acmid": "81409593685",
      "orcid": "missing"
    }
  ],
  "references": [
    {
      "text": "Dario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Qiang Cheng, Guoliang Chen, and others. 2016. Deep speech 2: End-to-end speech recognition in english and mandarin. In Proceedings of the 33rd International Conference on Machine Learning. PMLR, New York, 173--182.",
      "doi": ""
    },
    {
      "text": "T. R. Beelders and P. J. Blignaut. 2010. Using vision and voice to create a multimodal interface for Microsoft Word 2007. In Proceedings of the 2010 Symposium on Eye Tracking Research & Applications (ETRA '10). ACM, New York, 173--176. DOI: http://dx.doi.org/10.1145/1743666.1743709",
      "doi": ""
    },
    {
      "text": "John Brooke and others. 1996. SUS-A quick and dirty usability scale. Usability evaluation in industry 189, 194 (1996), 4--7.",
      "doi": ""
    },
    {
      "text": "Emiliano Castellina, Fulvio Corno, and Paolo Pellegrino. 2008. Integrated speech and gaze control for realistic desktop environments. In Proceedings of the 2008 Symposium on Eye Tracking Research & Applications (ETRA '08). ACM, New York, 79--82. DOI: http://dx.doi.org/10.1145/1344471.1344492",
      "doi": "10.1145/1344471.1344492"
    },
    {
      "text": "Kevin Christian, Bill Kules, Ben Shneiderman, and Adel Youssef. 2000. A comparison of voice controlled and mouse controlled web browsing. Technical Report TR_2005--11. College Park, MD.",
      "doi": ""
    },
    {
      "text": "Catalina Danis, Liam Comerford, Eric Janke, Ken Davies, Jackie De Vries, and Alex Bertrand. 1994. Storywriter: A speech oriented editor. In Proceedings of the ACM SIGCHI Conference Companion on Human 9http://www.mamem.eu Factors in Computing Systems (CHI '94). ACM, New York, 277--278. DOI: http://dx.doi.org/10.1145/259963.260490",
      "doi": "10.1145/259963.260490"
    },
    {
      "text": "C De Mauro, M Gori, M Maggini, and E Martinelli. 2001. Easy access to graphical interfaces by voice mouse. Technical Report. Universit\u00e0 di Siena. Available from the author.",
      "doi": ""
    },
    {
      "text": "Mark Dunlop, Emma Nicol, Andreas Komninos, Prima Dona, and Naveen Durga. 2016. Measuring inviscid text entry using image description tasks. In CHI'16 Workshop on Inviscid Text Entry and Beyond. ACM, New York. http://www.textentry.org/chi2016/9%20-% 20Dunlop%20-%20Image%20Description%20Tasks.pdf",
      "doi": ""
    },
    {
      "text": "Sandra G Hart and Lowell E Staveland. 1988. Development of NASA-TLX (Task Load Index): Results of empirical and theoretical research. In Advances in psychology. Vol. 52. Elsevier, 139--183.",
      "doi": ""
    },
    {
      "text": "Lode Hoste and Beat Signer. 2013. SpeeG2: A speechand gesture-based interface for effcient controller-free text input. In Proceedings of the 15th ACM on International Conference on Multimodal Interaction (ICMI '13). Association for Computing Machinery, New York, 213--220. DOI: http://dx.doi.org/10.1145/2522848.2522861",
      "doi": "10.1145/2522848.2522861"
    },
    {
      "text": "R Jakob. 1998. The use of eye movements in human-computer interaction techniques: What you look at is what you get. In Readings in Intelligent User Interfaces, W. Wahlster M. T. Maybury (Ed.). Morgan Kaufmann, San Francisco, 65--83.",
      "doi": ""
    },
    {
      "text": "Clare-Marie Karat, Christine Halverson, Daniel Horn, and John Karat. 1999. Patterns of entry and correction in large vocabulary continuous speech recognition systems. In Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems (CHI '99). ACM, New York, 568--575. DOI: http://dx.doi.org/10.1145/302979.303160",
      "doi": "10.1145/302979.303160"
    },
    {
      "text": "Reo Kishi and Takahiro Hayashi. 2015. Effective gazewriting with support of text copy and paste. In 2015 IEEE/ACIS 14th International Conference on Computer and Information Science (ICIS). 125--130. DOI: http://dx.doi.org/10.1109/ICIS.2015.7166581",
      "doi": ""
    },
    {
      "text": "Chandan Kumar, Raphael Menges, and Steffen Staab. 2016. Eye-controlled interfaces for multimedia interaction. In IEEE MultiMedia, Vol. 23. IEEE, New York, 6--13.",
      "doi": "10.1109/MMUL.2016.52"
    },
    {
      "text": "Manu Kumar, Andreas Paepcke, Terry Winograd, and Terry Winograd. 2007. EyePoint: Practical pointing and selection using gaze and keyboard. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '07). ACM, New York, 421--430. DOI: http://dx.doi.org/10.1145/1240624.1240692",
      "doi": "10.1145/1240624.1240692"
    },
    {
      "text": "P\u00e4ivi Majaranta. 2009. Text entry by eye gaze. PhD dissertation. http://urn.fi/urn:isbn:978--951--44--7787--4",
      "doi": ""
    },
    {
      "text": "Bill Manaris and Alan Harkreader. 1998. SUITEKeys: A speech understanding interface for the motor-control challenged. In Proceedings of the Third International ACM Conference on Assistive Technologies (Assets '98). ACM, New York, 108--115. DOI: http://dx.doi.org/10.1145/274497.274517",
      "doi": "10.1145/274497.274517"
    },
    {
      "text": "Chandra Sekhar Mantravadi. 2009. Adaptive multimodal integration of speech and gaze. Ph.D. Dissertation. Rutgers University, New Brunswick, NJ.",
      "doi": ""
    },
    {
      "text": "Arthur E McNair and Alex Waibel. 1994. Improving recognizer acceptance through robust, natural speech repair. In Third International Conference on Spoken Language Processing (ICSLP '94). International Speech Communication Organization, Baixas, France, 1299--1302.",
      "doi": ""
    },
    {
      "text": "Raphael Menges, Chandan Kumar, Daniel M\u00fcller, and Korok Sengupta. 2017. GazeTheWeb: A Gaze-Controlled Web Browser. In Proceedings of the 14th Web for All Conference on The Future of Accessible Work (W4A '17). ACM, NY, NY, USA, Article 25, 2 pages. DOI: http://dx.doi.org/10.1145/3058555.3058582",
      "doi": "10.1145/3058555.3058582"
    },
    {
      "text": "Raphael Menges, Chandan Kumar, and Steffen Staab. 2019. Improving User Experience of Eye Tracking-Based Interaction: Introspecting and Adapting Interfaces. ACM Trans. Comput.-Hum. Interact. 26, 6, Article Article 37 (Nov. 2019), 46 pages. DOI: http://dx.doi.org/10.1145/3338844",
      "doi": "10.1145/3338844"
    },
    {
      "text": "Yoshiyuki Mihara, Etsuya Shibayama, and Shin Takahashi. 2005. The migratory cursor: Accurate speech-based cursor movement by moving multiple ghost cursors using non-verbal vocalizations. In Proceedings of the 7th International ACM Conference on Computers and Accessibility (Assets '05). ACM, New York, 76--83. DOI: http://dx.doi.org/10.1145/1090785.1090801",
      "doi": "10.1145/1090785.1090801"
    },
    {
      "text": "Sharon Oviatt. 1997. Multimodal interactive maps: Designing for human performance. Human-Computer Interaction 12, 1 (1997), 93--129.",
      "doi": "10.1207/s15327051hci1201%25262_4"
    },
    {
      "text": "Sharon Oviatt. 2000. Taming recognition errors with a multimodal interface. Commun. ACM 43, 9 (2000), 45--45.",
      "doi": "10.1145/348941.348979"
    },
    {
      "text": "Sharon Oviatt. 2003. Multimodal interfaces. In The Human-Computer Interaction Handbook: Fundamentals, Evolving Technologies and Emerging Applications (2 ed.), J. A. Jacko A. Sears (Ed.). Vol. 14. Erlbaum, Mahwah, NJ, 286--304.",
      "doi": ""
    },
    {
      "text": "Sharon Oviatt, Phil Cohen, Lizhong Wu, Lisbeth Duncan, Bernhard Suhm, Josh Bers, Thomas Holzman, Terry Winograd, James Landay, Jim Larson, and others. 2000. Designing the user interface for multimodal speech and pen-based gesture applications: state-of-the-art systems and future research directions. Human-Computer Interaction 15, 4 (2000), 263--322. DOI: http://dx.doi.org/10.1207/S15327051HCI1504_1",
      "doi": "10.1207/S15327051HCI1504_1"
    },
    {
      "text": "Soujanya Poria, Erik Cambria, Rajiv Bajpai, and Amir Hussain. 2017. A review of affective computing: From unimodal analysis to multimodal fusion. Information Fusion 37 (2017), 98 -- 125. DOI: http://dx.doi.org/https://doi.org/10.1016/j.inffus.2017.02.003",
      "doi": "10.1016/j.inffus.2017.02.003"
    },
    {
      "text": "Marco Porta and Alessia Ravelli. 2009. WeyeB, an eye-controlled Web browser for hands-free navigation. In 2009 2nd Conference on Human System Interactions. 210--215. DOI: http://dx.doi.org/10.1109/HSI.2009.5090980",
      "doi": ""
    },
    {
      "text": "Matheus Vieira Portela and David Rozado. 2014. Gaze enhanced speech recognition for truly hands-free and effcient text input during HCI. In Proceedings of the 26th Australian Computer-Human Interaction Conference on Designing Futures: the Future of Design (OzCHI '14). ACM, New York, 426--429. DOI: http://dx.doi.org/10.1145/2686612.2686679",
      "doi": "10.1145/2686612.2686679"
    },
    {
      "text": "Kari-Jouko R\u00e4ih\u00e4 and Saila Ovaska. 2012. An exploratory study of eye typing fundamentals: Dwell time, text entry rate, errors, and workload. In Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems (CHI '12). ACM, New York, 3001--3010. DOI: http://dx.doi.org/10.1145/2207676.2208711",
      "doi": "10.1145/2207676.2208711"
    },
    {
      "text": "David B Roe, Jay G Wilpon, and others (Eds.). 1994. Voice communication between humans and machines. National Academies Press, Washington, DC.",
      "doi": ""
    },
    {
      "text": "Sherry Ruan, Jacob O. Wobbrock, Kenny Liou, Andrew Ng, and James A. Landay. 2018. Comparing speech and keyboard text entry for short messages in two languages on touchscreen phones. In Proceedings of the ACM Conference on Interactive, Mobile, Wearable and Ubiquitous Technologies. ACM, New York, 159:1--159:23. DOI: http://dx.doi.org/10.1145/3161187",
      "doi": ""
    },
    {
      "text": "Johan Schalkwyk, Doug Beeferman, Fran\u00e7oise Beaufays, Bill Byrne, Ciprian Chelba, Mike Cohen, Maryam Kamvar, and Brian Strope. 2010. \"Your word is my command\": Google search by voice: A case study. In Advances in speech recognition, A. Neustein (Ed.). Springer, Boston, 61--90. DOI: http://dx.doi.org/10.1007/978--1--4419--5951--5",
      "doi": ""
    },
    {
      "text": "Simon Schenk, Marc Dreiser, Gerhard Rigoll, and Michael Dorr. 2017. GazeEverywhere: Enabling Gaze-only User Interaction on an Unmodifed Desktop PC in Everyday Scenarios. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (CHI '17). ACM, NY, NY, USA, 3034--3044. DOI: http://dx.doi.org/10.1145/3025453.3025455",
      "doi": "10.1145/3025453.3025455"
    },
    {
      "text": "Andrew Sears, Jinhuan Feng, Kwesi Oseitutu, and Claire-Marie Karat. 2003. Hands-free, speech-based navigation during dictation: diffculties, consequences, and solutions. Human-Computer Interaction 18, 3 (2003), 229--257. DOI: http://dx.doi.org/10.1207/S15327051HCI1803_2",
      "doi": "10.1207/S15327051HCI1803_2"
    },
    {
      "text": "Korok Sengupta, Min Ke, Raphael Menges, Chandan Kumar, and Steffen Staab. 2018. Hands-free web browsing: enriching the user experience with gaze and voice modality. In Proceedings of the 2018 ACM Symposium on Eye Tracking Research & Applications (ETRA '18). ACM, New York, 88. DOI: http://dx.doi.org/10.1145/3204493.3208338",
      "doi": "10.1145/3204493.3208338"
    },
    {
      "text": "Korok Sengupta, Raphael Menges, Chandan Kumar, and Steffen Staab. 2017. GazeTheKey: Interactive keys to integrate word predictions for gaze-based text entry. In Proceedings of the 22nd International Conference on Intelligent User Interfaces Companion (IUI '17 Companion). ACM, New York, 121--124. DOI: http://dx.doi.org/10.1145/3030024.3038259",
      "doi": "10.1145/3030024.3038259"
    },
    {
      "text": "Korok Sengupta, Raphael Menges, Chandan Kumar, and Steffen Staab. 2019. Impact of Variable Positioning of Text Prediction in Gaze-Based Text Entry. In Proceedings of the 11th ACM Symposium on Eye Tracking Research Applications (ETRA '19). Association for Computing Machinery, New York, NY, USA, Article Article 74, 9 pages. DOI: http://dx.doi.org/10.1145/3317956.3318152",
      "doi": "10.1145/3317956.3318152"
    },
    {
      "text": "Shyamli Sindhwani, Christof Lutteroth, and Gerald Weber. 2019. ReType: Quick text editing with keyboard and gaze. In Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems (CHI '19). ACM, New York, 203. DOI: http://dx.doi.org/10.1145/3290605.3300433",
      "doi": "10.1145/3290605.3300433"
    },
    {
      "text": "Bernhard Suhm, Brad Myers, and Alex Waibel. 2001. Multimodal error correction for speech user interfaces. ACM Transactions on Computer-Human Interaction (TOCHI) 8, 1 (2001), 60--98. DOI: http://dx.doi.org/10.1145/371127.371166",
      "doi": "10.1145/371127.371166"
    },
    {
      "text": "Oleg \"pakov and Darius Miniotas. 2005. Gaze-based selection of standard-size menu items. In Proceedings of the 7th International Conference on Multimodal Interfaces (ICMI '05). ACM, New York, 124--128. DOI: http://dx.doi.org/10.1145/1088463.1088486",
      "doi": ""
    },
    {
      "text": "Nicole Yankelovich, Gina-Anne Levow, and Matt Marx. 1995. Designing SpeechActs: Issues in speech user interfaces. In Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems (CHI '95). ACM, New York, 369--376.",
      "doi": "10.1145/223904.223952"
    }
  ]
}