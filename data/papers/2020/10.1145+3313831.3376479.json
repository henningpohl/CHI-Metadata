{
  "doi": "10.1145/3313831.3376479",
  "title": "Enhancing Mobile Voice Assistants with WorldGaze",
  "published": "2020-04-23",
  "proctitle": "CHI '20: Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems",
  "pages": "1-10",
  "year": 2020,
  "badges": [],
  "abstract": "Contemporary voice assistants require that objects of inter-est be specified in spoken commands. Of course, users are often looking directly at the object or place of interest ? fine-grained, contextual information that is currently unused. We present WorldGaze, a software-only method for smartphones that provides the real-world gaze location of a user that voice agents can utilize for rapid, natural, and precise interactions. We achieve this by simultaneously opening the front and rear cameras of a smartphone. The front-facing camera is used to track the head in 3D, including estimating its direction vector. As the geometry of the front and back cameras are fixed and known, we can raycast the head vector into the 3D world scene as captured by the rear-facing camera. This allows the user to intuitively define an object or region of interest using their head gaze. We started our investigations with a qualitative exploration of competing methods, before developing a functional, real-time implementation. We conclude with an evaluation that shows WorldGaze can be quick and accurate, opening new multimodal gaze+voice interactions for mobile voice agents.",
  "tags": [
    "worldgaze",
    "mobile interaction",
    "interaction techniques"
  ],
  "authors": [
    {
      "name": "Sven Mayer",
      "institution": "Carnegie Mellon University, Pittsburgh, PA, USA",
      "img": "/do/10.1145/contrib-99658696530/rel-imgonly/p_mayersven.jpg",
      "acmid": "99658696530",
      "orcid": "0000-0001-5462-8782"
    },
    {
      "name": "Gierad Laput",
      "institution": "Apple Inc. & Carnegie Mellon University, Cupertino, CA, USA",
      "img": "/do/10.1145/contrib-81502797271/rel-imgonly/gierad_laput_c2_425x425_gradient2_shift.jpg",
      "acmid": "81502797271",
      "orcid": "missing"
    },
    {
      "name": "Chris Harrison",
      "institution": "Carnegie Mellon University, Pittsburgh, PA, USA",
      "img": "/do/10.1145/contrib-81350573130/rel-imgonly/pastedgraphic-11.png",
      "acmid": "81350573130",
      "orcid": "0000-0001-5312-3619"
    }
  ],
  "references": [
    {
      "text": "Cengiz Acart\u00fcrk, Jo\u00e3o Freitas, Mehmetcal Fal, and Miguel Sales Dias. 2015. Elderly Speech-Gaze Interaction. In International Conference on Universal Access in Human-Computer Interaction. Springer, Cham, 3--12. DOI: https://doi.org/10.1007/978--3--31920678--3_1",
      "doi": ""
    },
    {
      "text": "Lisa Anthony, Jie Yang, and Kenneth R. Koedinger. 2005. Evaluation of multimodal input for entering mathematical equations on the computer. In CHI '05 Extended Abstracts on Human Factors in Computing Systems (CHI EA '05). ACM, NY, NY, USA, 1184--1187. DOI: http://dx.doi.org/10.1145/1056808.1056872",
      "doi": ""
    },
    {
      "text": "Apple Vision Framework. 2019. URL: https://developer.apple.com/documentation/vision",
      "doi": ""
    },
    {
      "text": "Apple Speech Framework. 2019. URL: https://developer.apple.com/documentation/speech",
      "doi": ""
    },
    {
      "text": "Vijay Badrinarayanan, Kendall Alex, and Cipolla Roberto. 2017. Segnet: A deep convolutional encoderdecoder architecture for image segmentation. IEEE transactions on pattern analysis and machine intelligence 39.12: 2481--2495. DOI: http://dx.doi.org/10.1109/TPAMI.2016.2644615",
      "doi": ""
    },
    {
      "text": "Tadas Baltruaitis, Peter Robinson, and Louis-Philippe Morency. 2016. OpenFace: An open source facial behavior analysis toolkit. In IEEE Winter Conference on Applications of Computer Vision (WACV '16). IEEE, 1--10. DOI: http://dx.doi.org/10.1109/WACV.2016.7477553",
      "doi": ""
    },
    {
      "text": "Tanya R. Beelders, and Pieter J. Blignaut. 2011. The Usability of Speech and Eye Gaze as a Multimodal Interface for a Word Processor. Speech Technologies, 386--404. DOI: http://dx.doi.org/10.5772/16604",
      "doi": ""
    },
    {
      "text": "Tim Bailey, and Hugh Durrant-Whyte. 2006. Simultaneous localization and mapping (SLAM): Part II. IEEE robotics & automation magazine 13, no. 3, 108--117. IEEE. DOI: http://dx.doi.org/10.1109/MRA.2006.1678144",
      "doi": ""
    },
    {
      "text": "Ann Blandford, Dominic Furniss, and Stephann Makri. 2016. Qualitative HCI research: Going behind the scenes. Synthesis lectures on human-centered informatics, 9(1), 1--115. DOI: https://doi.org/2200/S00706ED1V01Y201602HCI034",
      "doi": ""
    },
    {
      "text": "Richard A. Bolt. 1980. Put-that-there: Voice and gesture at the graphics interface. In Proceedings of the 7th annual conference on Computer graphics and interactive techniques (SIGGRAPH '80). ACM, NY, NY, USA, 262--270. DOI: http://dx.doi.org/10.1145/800250.807503",
      "doi": "10.1145/800250.807503"
    },
    {
      "text": "John Brooke. 1996. SUS-A quick and dirty usability scale. Usability evaluation in industry, 189(194), 4--7.",
      "doi": ""
    },
    {
      "text": "Drini Cami, Fabrice Matulic, Richard G. Calland, Brian Vogel, and Daniel Vogel. 2018. Unimanual Pen+Touch Input Using Variations of Precision Grip Postures. In Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology (UIST '18). ACM, NY, NY, USA, 825--837. DOI: https://doi.org/10.1145/3242587.3242652",
      "doi": "10.1145/3242587.3242652"
    },
    {
      "text": "Ishan Chatterjee, Robert Xiao, and Chris Harrison. 2015. Gaze+Gesture: Expressive, Precise and Targeted Free-Space Interactions. In Proceedings of the 2015 ACM on International Conference on Multimodal Interaction (ICMI '15). ACM, NY, NY, USA. DOI: https://doi.org/10.1145/2818346.2820752",
      "doi": "10.1145/2818346.2820752"
    },
    {
      "text": "Leigh Clark, Phillip Doyle, Diego Garaialde, Emer Gilmartin, Stephan Schl\u00f6gl, Jens Edlund, Matthew Aylett, Jo\u00e3o Cabral, Cosmin Munteanu, and Benjamin Cowan. 2018. The State of Speech in HCI: Trends, Themes and Challenges. In Proceedings of the Interacting with Computers. DOI: https://doi.org/10.1093/iwc/iwz016",
      "doi": ""
    },
    {
      "text": "Heiko Drewes, Alexander De Luca, and Albrecht Schmidt. 2007. Eye-gaze interaction for mobile phones. In Proceedings of the 4th international conference on mobile technology, applications, and systems and the 1st international symposium on Computer human interaction in mobile technology (Mobility '07). ACM, NY, NY, USA, 364--371. DOI: http://dx.doi.org/10.1145/1378063.1378122",
      "doi": "10.1145/1378063.1378122"
    },
    {
      "text": "Augusto Esteves, Eduardo Velloso, Andreas Bulling, and Hans Gellersen. 2015. Orbits: Gaze Interaction for Smart Watches using Smooth Pursuit Eye Movements. In Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology (UIST '15). ACM, NY, NY, USA, 457--466. DOI: https://doi.org/10.1145/2807442.2807499",
      "doi": "10.1145/2807442.2807499"
    },
    {
      "text": "Jorge Fuentes-Pacheco, Jos\u00e9 Ruiz-Ascencio, and Juan Manuel Rend\u00f3n-Mancha. 2015. Visual simultaneous localization and mapping: a survey. Artificial Intelligence Review 43, no. 1, 55--81. DOI: https://doi.org/10.1007/s10462-012--9365--8",
      "doi": "10.1007/s10462-012-9365-8"
    },
    {
      "text": "Alastair G. Gale. 1997. Human response to visual stimuli. In The perception of visual information. Springer, New York, NY, 127--147. DOI: https://doi.org/10.1007/978--1--4612--1836--4_5",
      "doi": ""
    },
    {
      "text": "Google Cloud Vision AI. 2019. https://cloud.google.com/vision/automl/objectdetection/docs/",
      "doi": ""
    },
    {
      "text": "Floyd A. Glenn III, Helene P. Iavecchia, Lorna V. Ross, James M. Stokes, William J. Weiland, Daniel Weiss, and Allen L. Zaklad. 1986. Eyevoicecontrolled interface. In Proceedings of the Human Factors Society, 322--326. DOI: https://doi.org/10.1177/154193128603000402",
      "doi": ""
    },
    {
      "text": "Gunnar Harboe, and Elaine M. Huang. 2015. RealWorld Affinity Diagramming Practices: Bridging the Paper-Digital Gap. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI '15). ACM, NY, NY, USA, 95-- 104. DOI: http://dx.doi.org/10.1145/2702123.2702561",
      "doi": ""
    },
    {
      "text": "Sandra G. Hart. 2006. NASA-task load index (NASATLX); 20 years later. In Proceedings of the human factors and ergonomics society annual meeting, Vol. 50, No. 9, 904--908, Los Angeles, CA, Sage publications. DOI: https://doi.org/10.1037/e577632012-009",
      "doi": ""
    },
    {
      "text": "Kaiming He, Gkioxari Georgia, Doll\u00e1r Piotr, and Girshick Ross. 2017. Mask R-CNN. In Proceedings of the IEEE international conference on computer vision. IEEE, 2961--2969. DOI: http://dx.doi.org/10.1109/TPAMI.2018.2844175",
      "doi": ""
    },
    {
      "text": "Ken Hinckley, Koji Yatani, Michel Pahud, Nicole Coddington, Jenny Rodenhouse, Andy Wilson, Hrvoje Benko, and Bill Buxton. 2010. Pen + touch = new tools. In Proceedings of the 23nd annual ACM symposium on User interface software and technology (UIST '10). ACM, NY, NY, USA, 27--36. DOI: https://doi.org/10.1145/1866029.1866036",
      "doi": "10.1145/1866029.1866036"
    },
    {
      "text": "Ron Jacob. 1995. Eye tracking in advanced interface design. In Virtual Environments and Advanced Interface Design. New York: Oxford University Press, 258--288.",
      "doi": ""
    },
    {
      "text": "Justin Johnson, Andrej Karpathy, and Li Fei-Fei. 2016. Densecap: Fully convolutional localization networks for dense captioning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR '16). IEEE 4565--4574. DOI: https://doi.org/10.1109/CVPR.2016.494",
      "doi": ""
    },
    {
      "text": "David B. Koons, Carlton J. Sparrell, and Kristinn R. Thorisson. 1993. Integrating simultaneous input from speech, gaze, and hand gestures. MIT Press: Menlo Park, CA, 257--276.",
      "doi": ""
    },
    {
      "text": "Kyle Krafka, Aditya Khosla, Petr Kellnhofer, Harini Kannan, Suchendra Bhandarkar, Wojciech Matusik, and Antonio Torralba. 2016. Eye tracking for everyone. In Proceedings of the IEEE conference on computer vision and pattern recognition 2016 (CVPR '16). IEEE, 2176--2184. DOI: https://doi.org/10.1109/CVPR.2016.239",
      "doi": ""
    },
    {
      "text": "Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. 2017. Feature pyramid networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR '17). IEEE, 2117--2125. DOI: https://doi.org/10.1109/CVPR.2017.106",
      "doi": ""
    },
    {
      "text": "Diako Mardanbegi, and Dan Witzner Hansen. 2011. Mobile gaze-based screen interaction in 3D environments. In Proceedings of the 1st Conference on Novel Gaze-Controlled Applications (NGCA '11). ACM, NY, NY, USA, Article 2, 4 pages. DOI: http://dx.doi.org/10.1145/1983302.1983304",
      "doi": "10.1145/1983302.1983304"
    },
    {
      "text": "Sven Mayer, Katrin Wolf, Stefan Schneegass, and Niels Henze. 2015. Modeling Distant Pointing for Compensating Systematic Displacements. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI '15). ACM, NY, NY, USA, 4165--4168. DOI: https://doi.org/10.1145/2702123.2702332",
      "doi": "10.1145/2702123.2702332"
    },
    {
      "text": "Sven Mayer, Valentin Schwind, Robin Schweigert, and Niels Henze. 2018. The Effect of Offset Correction and Cursor on Mid-Air Pointing in Real and Virtual Environments. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (CHI '18). ACM, NY, NY, USA, Paper 653, 13 pages. DOI: https://doi.org/10.1145/3173574.3174227",
      "doi": "10.1145/3173574.3174227"
    },
    {
      "text": "Darius Miniotas, Ivan Tugoy, and I. Scott MacKenzie. 2006. Speech-augmented eye gaze interaction with small closely spaced targets. In Proceedings of the 2006 symposium on Eye tracking research & applications (ETRA '06). ACM, NY, NY, USA, 67--72. DOI: http://dx.doi.org/10.1145/1117309.1117345",
      "doi": "10.1145/1117309.1117345"
    },
    {
      "text": "Robert Ne\u00dfelrath, Mohammad Mehdi Moniri, and Michael Feld. 2016. Combining speech, gaze, and micro-gestures for the multimodal control of in-car functions. In Proceedings of the 12th International Conference on Intelligent Environments (IE '16). IEEE. DOI: http://dx.doi.org/10.1109/IE.2016.42",
      "doi": ""
    },
    {
      "text": "Alexandra Papoutsaki, Patsorn Sangkloy, James Laskey, Nediyana Daskalova, Jeff Huang, and James Hays. 2016. Webgazer: Scalable webcam eye tracking using user interactions. In Proceedings of the TwentyFifth International Joint Conference on Artificial Intelligence-IJCAI 2016.",
      "doi": ""
    },
    {
      "text": "Ken Pfeuffer, Jason Alexander, Ming Ki Chong, and Hans Gellersen. 2014. Gaze-touch: combining gaze with multi-touch for interaction on the same surface. In Proceedings of the 27th annual ACM symposium on User interface software and technology (UIST '14). ACM, NY, NY, USA, 509--518. DOI: https://doi.org/10.1145/2642918.2647397",
      "doi": "10.1145/2642918.2647397"
    },
    {
      "text": "Bastian Pfleging, Stefan Schneegass, and Albrecht Schmidt. 2012. Multimodal interaction in the car: combining speech and gestures on the steering wheel. In Proceedings of the 4th International Conference on Automotive User Interfaces and Interactive Vehicular Applications (AutomotiveUI '12). ACM, NY, NY, USA, 155--162. DOI: http://dx.doi.org/10.1145/2390256.2390282",
      "doi": "10.1145/2390256.2390282"
    },
    {
      "text": "Katrin Plaumann, Matthias Weing, Christian Winkler, Michael M\u00fcller, and Enrico Rukzio. 2018. Towards accurate cursorless pointing: the effects of ocular dominance and handedness. Personal Ubiquitous Comput. 22, 4 (August 2018), 633--646. DOI: https://doi.org/10.1007/s00779-017--1100--7",
      "doi": "10.1007/s00779-017-1100-7"
    },
    {
      "text": "Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. 2016. You only look once: Unified, realtime object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR '16). IEEE, 779--788. DOI: https://doi.org/10.1109/CVPR.2016.91",
      "doi": ""
    },
    {
      "text": "Florian Roider, Lars Reisig, and Tom Gross. 2018. Just Look: The Benefits of Gaze-Activated Voice Input in the Car. In Adjunct Proceedings of the 10th International Conference on Automotive User Interfaces and Interactive Vehicular Applications (AutomotiveUI '18). ACM, NY, NY, USA, 210--214. DOI: https://doi.org/10.1145/3239092.3265968",
      "doi": ""
    },
    {
      "text": "David Rozado, Alexander McNeill, and Daniel Mazur. 2016. Voxvisio -- Combining Gaze And Speech For Accessible Hci. In Proceedings of RESNA/NCART 2016.",
      "doi": ""
    },
    {
      "text": "Albrecht Schmidt, Michael Beigl, and Hans Gellersen. 1999. There is more to context than location. Computers & Graphics 23.6, 893--901. DOI: https://doi.org/10.1016/S0097--8493(99)00120-X",
      "doi": ""
    },
    {
      "text": "Julia Schwarz, Scott Hudson, Jennifer Mankoff, and Andrew D. Wilson. 2010. A framework for robust and flexible handling of inputs with uncertainty. In Proceedings of the 23nd annual ACM symposium on User interface software and technology (UIST '10). ACM, NY, NY, USA, 47--56. DOI: https://doi.org/10.1145/1866029.1866039",
      "doi": ""
    },
    {
      "text": "Robin Schweigert, Valentin Schwind, and Sven Mayer. 2019. EyePointing: A Gaze-Based Selection Technique. In Proceedings of Mensch und Computer 2019 (MuC '19). ACM, NY, NY, USA, 719723. DOI: https://doi.org/10.1145/3340764.3344897",
      "doi": "10.1145/3340764.3344897"
    },
    {
      "text": "Valentin Schwind, Sven Mayer, Alexandre Comeau Vermeersch, Robin Schweigert, and Niels Henze. 2018. Up to the Finger Tip: The Effect of Avatars on Mid-Air Pointing Accuracy in Virtual Reality. In Proceedings of the 2018 Annual Symposium on Computer-Human Interaction in Play (CHI PLAY '18). ACM, NY, NY, USA, 477--488. DOI: https://doi.org/10.1145/3242671.3242675",
      "doi": "10.1145/3242671.3242675"
    },
    {
      "text": "Ke Sun, Chun Yu, Weinan Shi, Lan Liu, and Yuanchun Shi. 2018. Lip-Interact: Improving Mobile Device Interaction with Silent Speech Commands. In Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology (UIST '18). ACM, NY, NY, USA, 581--593. DOI: https://doi.org/10.1145/3242587.3242599",
      "doi": "10.1145/3242587.3242599"
    },
    {
      "text": "Daniel Vogel, and Ravin Balakrishnan. 2005. Distant freehand pointing and clicking on very large, high resolution displays. In Proceedings of the 18th annual ACM symposium on User interface software and technology (UIST '05). ACM, NY, NY, USA, 33--42. DOI: http://dx.doi.org/10.1145/1095034.1095041",
      "doi": "10.1145/1095034.1095041"
    },
    {
      "text": "Vuforia. URL: https://developer.vuforia.com",
      "doi": ""
    },
    {
      "text": "Jacob O. Wobbrock, Leah Findlater, Darren Gergle, and James J. Higgins. 2011. The aligned rank transform for nonparametric factorial analyses using only anova procedures. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '11). ACM, NY, NY, USA, 143--146. DOI: https://doi.org/10.1145/1978942.1978963",
      "doi": "10.1145/1978942.1978963"
    },
    {
      "text": "Shumin Zhai, Carlos Morimoto, and Steven Ihde. 1999. Manual and gaze input cascaded (MAGIC) pointing. In Proceedings of the SIGCHI conference on Human Factors in Computing Systems (CHI '99). ACM, 246--253. DOI: http://dx.doi.org/10.1145/302979.303053",
      "doi": "10.1145/302979.303053"
    },
    {
      "text": "Qiaohui Zhang, Atsumi Imamiya, Kentaro Go, and Xiaoyang Mao. 2004. Resolving ambiguities of a gaze and speech interface. In Proceedings of the 2004 symposium on Eye tracking research & applications (ETRA '04). ACM, NY, NY, USA, 85--92. DOI: https://doi.org/10.1145/968363.968383",
      "doi": "10.1145/968363.968383"
    },
    {
      "text": "Xucong Zhang, Yusuke Sugano, M. Fritz, and Andreas Bulling. 2015. Appearance-based gaze estimation in the wild. In Proceedings of the IEEE conference on computer vision and pattern recognition 2015 (CVPR '15). IEEE, 4511--4520. DOI: https://doi.org/10.1109/CVPR.2015.7299081",
      "doi": ""
    }
  ]
}