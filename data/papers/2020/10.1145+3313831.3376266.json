{
  "doi": "10.1145/3313831.3376266",
  "title": "A View on the Viewer: Gaze-Adaptive Captions for Videos",
  "published": "2020-04-23",
  "proctitle": "CHI '20: Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems",
  "pages": "1-12",
  "year": 2020,
  "badges": [
    "Honorable Mention"
  ],
  "abstract": "Subtitles play a crucial role in cross-lingual distribution of multimedia content and help communicate information where auditory content is not feasible (loud environments, hearing impairments, unknown languages). Established methods utilize text at the bottom of the screen, which may distract from the video. Alternative techniques place captions closer to related content (e.g., faces) but are not applicable to arbitrary videos such as documentations. Hence, we propose to leverage live gaze as indirect input method to adapt captions to individual viewing behavior. We implemented two gaze-adaptive methods and compared them in a user study (n=54) to traditional captions and audio-only videos. The results show that viewers with less experience with captions prefer our gaze-adaptive methods as they assist them in reading. Furthermore, gaze distributions resulting from our methods are closer to natural viewing behavior compared to the traditional approach. Based on these results, we provide design implications for gaze-adaptive captions.",
  "tags": [
    "video captions",
    "gaze input",
    "multimedia",
    "subtitles",
    "eye tracking",
    "gaze-responsive display"
  ],
  "authors": [
    {
      "name": "Kuno Kurzhals",
      "institution": "ETH Z\u00fcrich, Z\u00fcrich, Switzerland",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "84459518557",
      "orcid": "missing"
    },
    {
      "name": "Fabian G\u00f6bel",
      "institution": "ETH Z\u00fcrich, Z\u00fcrich, Switzerland",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81502651171",
      "orcid": "missing"
    },
    {
      "name": "Katrin Angerbauer",
      "institution": "University of Stuttgart, Stuttgart, Germany",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99658703665",
      "orcid": "missing"
    },
    {
      "name": "Michael Sedlmair",
      "institution": "University of Stuttgart, Stuttgart, Germany",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81375621529",
      "orcid": "missing"
    },
    {
      "name": "Martin Raubal",
      "institution": "ETH Z\u00fcrich, Z\u00fcrich, Switzerland",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81100573131",
      "orcid": "missing"
    }
  ],
  "references": [
    {
      "text": "Wataru Akahori, Tatsunori Hirai, Shunya Kawamura, and Shigeo Morishima. 2016. Region-of-interest-based subtitle placement using eye-tracking data of multiple viewers. In Proceedings of the ACM International Conference on Interactive Experiences for TV and Online Video. 123--128.",
      "doi": "10.1145/2932206.2933558"
    },
    {
      "text": "Wataru Akahori, Tatsunori Hirai, and Shigeo Morishima. 2017. Dynamic subtitle placement considering the region of interest and speaker location. In Proceedings of the International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications. 102--109.",
      "doi": ""
    },
    {
      "text": "Katrin Angerbauer, Heike Adel, and Ngoc T Vu. 2019. Automatic compression of subtitles with neural networks and its effect on user experience. In Proceedings of the 20th Annual Conference of the International Speech Communication Association. 594--598.",
      "doi": ""
    },
    {
      "text": "Mike Armstrong and Matthew Brooks. 2014. Enhancing subtitles. In Adjunct Publication of the ACM International Conference on Interactive Experiences for Television and Online Video. 1--2.",
      "doi": ""
    },
    {
      "text": "Ronald Azuma and Chris Furmanski. 2003. Evaluating label placement for augmented reality view management. In Proceedings of the Second IEEE and ACM International Symposium on Mixed and Augmented Reality. 66--75.",
      "doi": ""
    },
    {
      "text": "Marie-Jos\u00e9e Bisson, Walter J. B. Van Heuven, Kathy Conklin, and Richard J. Tunney. 2014. Processing of native and foreign language subtitles in films: An eye tracking study. Applied Psycholinguistics 35, 2 (2014), 399--418.",
      "doi": ""
    },
    {
      "text": "Andy Brown, Rhia Jones, Mike Crabb, James Sandford, Matthew Brooks, Mike Armstrong, and Caroline Jay. 2015. Dynamic subtitles: The user experience. In Proceedings of the ACM International Conference on Interactive Experiences for TV and Online Video. 103--112.",
      "doi": "10.1145/2745197.2745204"
    },
    {
      "text": "Andy Brown, Jayson Turner, Jake Patterson, Anastasia Schmitz, Mike Armstrong, and Maxine Glancy. 2017. Subtitles in 360-degree video. In Adjunct Publication of the ACM International Conference on Interactive Experiences for TV and Online Video. 3--8.",
      "doi": "10.1145/3084289.3089915"
    },
    {
      "text": "Stephen Doherty and Jan-Louis Kruger. 2018. The development of eye tracking in empirical research on subtitling and captioning. In Seeing into screens -- Eye tracking and the moving image, Tessa Dwyer, Claire Perkins, Sean Remond, and Jodi Sita (Eds.). Bloomsbury, London, 46--64.",
      "doi": ""
    },
    {
      "text": "Andrew T. Duchowski. 2017. Serious gaze. In Proceedings of the 9th International Conference on Virtual Worlds and Games for Serious Applications. 276--283.",
      "doi": ""
    },
    {
      "text": "G\u00e9ry d'Ydewalle and Wim De Bruycker. 2007. Eye movements of children and adults while reading television subtitles. European Psychologist 12, 3 (2007), 196--205.",
      "doi": ""
    },
    {
      "text": "G\u00e9ry d'Ydewalle, Caroline Praet, Karl Verfaillie, and Johan Van Rensbergen. 1991. Watching subtitled television automatic reading behavior. Communication Research 18, 5 (1991), 650--666.",
      "doi": ""
    },
    {
      "text": "Olivia Gerber-Mor\u00f3n, Agnieszka Szarkowska, and Bencie Woll. 2018. The impact of text segmentation on subtitle reading. Journal of Eye Movement Research 6, 5 (2018), 1--12.",
      "doi": ""
    },
    {
      "text": "Fabian G\u00f6bel, Peter Kiefer, Ioannis Giannopoulos, Andrew T. Duchowski, and Martin Raubal. 2018. Improving map reading with gaze-adaptive legends. In Proceedings of the ACM Symposium on Eye Tracking Research & Applications. 29:1--29:9.",
      "doi": "10.1145/3204493.3204544"
    },
    {
      "text": "Sandra G. Hart. 2006. Nasa-Task Load Index (NASA-TLX); 20 years later. Proceedings of the Human Factors and Ergonomics Society Annual Meeting 50, 9 (2006), 904--908.",
      "doi": ""
    },
    {
      "text": "Richang Hong, Meng Wang, Xiao-Tong Yuan, Mengdi Xu, Jianguo Jiang, Shuicheng Yan, and Tat-Seng Chua. 2011. Video accessibility enhancement for hearing-impaired users. ACM Transactions on Multimedia Computing, Communications, and Applications 7S, 1 (2011), 24:1--24:19.",
      "doi": "10.1145/2037676.2037681"
    },
    {
      "text": "Yongtao Hu, Jan Kautz, Yizhou Yu, and Wenping Wang. 2014. Speaker-following video subtitles. ACM Transactions on Multimedia Computing, Communications, and Applications 11, 2 (2014), 32:1--32:17.",
      "doi": ""
    },
    {
      "text": "Robert J. K. Jacob. 1990. What you look at is what you get: Eye movement-based interaction techniques. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 11--18.",
      "doi": "10.1145/97243.97246"
    },
    {
      "text": "Bo Jiang, Sijiang Liu, Liping He, Weimin Wu, Hongli Chen, and Yunfei Shen. 2017. Subtitle positioning for e-learning videos based on rough gaze estimation and saliency detection. In SIGGRAPH Asia Posters. 15--16.",
      "doi": ""
    },
    {
      "text": "Marcel A. Just and Patricia A. Carpenter. 1976. Eye fixations and cognitive processes. Cognitive Psychology 8, 4 (1976), 441--480.",
      "doi": ""
    },
    {
      "text": "Harish Katti, Anoop Kolar Rajagopal, Mohan Kankanhalli, and Ramakrishnan Kalpathi. 2014. Online estimation of evolving human visual interest. ACM Transactions on Multimedia Computing Communication Applications 11, 1 (2014), 8:1--8:21.",
      "doi": "10.1145/2632284"
    },
    {
      "text": "Izabela Krejtz, Agnieszka Szarkowska, and Krzysztof Krejtz. 2013. The effects of shot changes on eye movements in subtitling. Journal of Eye Movement Research 6, 5 (2013), 1--12.",
      "doi": ""
    },
    {
      "text": "Jan-Louis Kruger, Agnieszka Szarkowska, and Izabela Krejtz. 2015. Subtitles on the moving image: An overview of eye tracking studies. Refractory : A Journal of Entertainment Media 25 (2015), 1--14.",
      "doi": ""
    },
    {
      "text": "Kuno Kurzhals, Emine Cetinkaya, Yongtao Hu, Wenping Wang, and Daniel Weiskopf. 2017. Close to the action: Eye-tracking evaluation of speaker-following subtitles. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 6559--6568.",
      "doi": "10.1145/3025453.3025772"
    },
    {
      "text": "Tiffany C. K. Kwok, Peter Kiefer, Victor R. Schinazi, Benjamin Adams, and Martin Raubal. 2019. Gaze-guided narratives: Adapting audio guide content to gaze in virtual and real environments. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 491:1--491:12.",
      "doi": "10.1145/3290605.3300721"
    },
    {
      "text": "Michael Land, Neil Mennie, and Jennifer Rusted. 1999. The roles of vision and eye movements in the control of activities of daily living. Perception 28, 11 (1999), 1311--1328.",
      "doi": ""
    },
    {
      "text": "Bettina Laugwitz, Theo Held, and Martin Schrepp. 2008. Construction and evaluation of a user experience questionnaire. In HCI and Usability for Education and Work, Andreas Holzinger (Ed.). Berlin, Heidelberg, 63--76.",
      "doi": ""
    },
    {
      "text": "Maryam Sadat Mirzaei, Kourosh Meshgi, Yuya Akita, and Tatsuya Kawahara. 2017. Partial and synchronized captioning: A new tool to assist learners in developing second language listening skill. ReCALL - The Journal of the European Association for Computer Assisted Language Learning 29, 2 (2017), 178--199.",
      "doi": ""
    },
    {
      "text": "Jason Orlosky, Kiyoshi Kiyokawa, and Haruo Takemura. 2014. Managing mobile text in head mounted displays: Studies on visual preference and text placement. ACM SIGMOBILE Mobile Computing and Communications 18, 2 (2014), 20--31.",
      "doi": "10.1145/2636242.2636246"
    },
    {
      "text": "Yi-Hao Peng, Ming-Wei Hsi, Paul Taele, Ting-Yu Lin, Po-En Lai, Leon Hsu, Tzu-chuan Chen, Te-Yen Wu, Yu-An Chen, Hsien-Hui Tang, and others. 2018. Speechbubbles: Enhancing captioning experiences for deaf and hard-of-hearing people in group conversations. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 293--302.",
      "doi": "10.1145/3173574.3173867"
    },
    {
      "text": "Elisa Perego, Fabio Del Missier, Marco Porta, and Mauro Mosconi. 2010. The cognitive effectiveness of subtitle processing. Media Psychology 13, 3 (2010), 243--272.",
      "doi": ""
    },
    {
      "text": "Dhevi J. Rajendran, Andrew T. Duchowski, Pilar Orero, Juan Mart\u00ednez, and Pablo Romero-Fresco. 2013. Effects of text chunking on subtitling: A quantitative and qualitative examination. Perspectives 21, 1 (2013), 5--21.",
      "doi": ""
    },
    {
      "text": "Vasili Ramanishka, Abir Das, Jianming Zhang, and Kate Saenko. 2017. Top-down visual saliency guided by captions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 7206--7215.",
      "doi": ""
    },
    {
      "text": "Raisa Rashid, Quoc Vy, Richard Hunt, and Deborah I. Fels. 2008. Dancing with words: Using animated text for captioning. International Journal of Human--Computer Interaction 24, 5 (2008), 505--519.",
      "doi": ""
    },
    {
      "text": "Sylvia Rothe, Kim Tran, and Heinrich Hu\u00dfmann. 2018. Dynamic subtitles in cinematic virtual reality. In Proceedings of the ACM International Conference on Interactive Experiences for TV and Online Video. 209--214.",
      "doi": "10.1145/3210825.3213556"
    },
    {
      "text": "Rufat Rzayev, Pawe\u0142 W. Wo\u017aniak, Tilman Dingler, and Niels Henze. 2018. Reading on smart glasses: The Effect of text position, presentation type and walking. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 45:1--45:9.",
      "doi": "10.1145/3173574.3173619"
    },
    {
      "text": "Dario D. Salvucci and Joseph H. Goldberg. 2000. Identifying fixations and saccades in eye-tracking protocols. In Proceedings of the ACM Symposium on Eye Tracking Research & Applications. 71--78.",
      "doi": ""
    },
    {
      "text": "Agnieszka Szarkowska, Izabela Krejtz, Zuzanna Klyszejko, and Anna Wieczorek. 2011. Verbatim, standard, or edited?: Reading patterns of different captioning styles among deaf, hard of hearing, and hearing viewers. American Annals of the Deaf 156, 4 (2011), 363--378.",
      "doi": ""
    },
    {
      "text": "Ruxandra Tapu, Bogdan Mocanu, and Titus Zaharia. 2019. DEEP-HEAR: A multimodal subtitle positioning system dedicated to deaf and hearing-impaired people. IEEE Access 7 (2019), 88:150--88:162.",
      "doi": ""
    },
    {
      "text": "Roel Vertegaal. 2002. Designing attentive interfaces. In Proceedings of the ACM Symposium on Eye tracking Research & Applications. 23--30.",
      "doi": "10.1145/507072.507077"
    },
    {
      "text": "Roel Vertegaal. 2003. Attentive user interfaces. Communications of the ACM 46, 3 (2003), 30--33.",
      "doi": "10.1145/636772.636794"
    },
    {
      "text": "Toinon Vigier, Yoann Baveye, Josselin Rousseau, and Patrick Le Callet. 2016. Visual attention as a dimension of QoE: Subtitles in UHD videos. In Proceedings of the Eighth International Conference on Quality of Multimedia Experience. 1--6.",
      "doi": ""
    },
    {
      "text": "Colin Ware and Harutune H. Mikaelian. 1986. An evaluation of an eye tracker as a device for computer input. ACM SIGCHI Bulletin 17, SI (1986), 183--188.",
      "doi": ""
    },
    {
      "text": "Jacob O. Wobbrock, Leah Findlater, Darren Gergle, and James J. Higgins. 2011. The aligned rank transform for nonparametric factorial analyses using only anova procedures. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 143--146.",
      "doi": ""
    }
  ]
}