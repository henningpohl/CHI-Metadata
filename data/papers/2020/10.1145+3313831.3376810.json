{
  "doi": "10.1145/3313831.3376810",
  "title": "FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions",
  "published": "2020-04-23",
  "proctitle": "CHI '20: Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems",
  "pages": "1-14",
  "year": 2020,
  "badges": [
    "Honorable Mention"
  ],
  "abstract": "In the conversations with smart speakers, misunderstandings of users' requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a first user study (N=12) to understand users' intuitive reactions to the correct and incorrect responses. Our results reveal the significant difference in the frequency of occurrence and intensity of users' facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efficiency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4%, recall: 97.6%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.",
  "authors": [
    {
      "name": "Yukang Yan",
      "institution": "Tsinghua University, Beijing, China",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659161020",
      "orcid": "0000-0001-7515-3755"
    },
    {
      "name": "Chun Yu",
      "institution": "Tsinghua University; Ministry of Education, Beijing, China",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81467660483",
      "orcid": "0000-0003-2591-7993"
    },
    {
      "name": "Wengrui Zheng",
      "institution": "Tsinghua University, Beijing, China",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659527761",
      "orcid": "missing"
    },
    {
      "name": "Ruining Tang",
      "institution": "Tsinghua University, Beijing, China",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659526642",
      "orcid": "missing"
    },
    {
      "name": "Xuhai Xu",
      "institution": "Tsinghua University, Beijing, China",
      "img": "/do/10.1145/contrib-99659534445/rel-imgonly/orson-uw-talk-head-small.png",
      "acmid": "99659534445",
      "orcid": "0000-0001-5930-3899"
    },
    {
      "name": "Yuanchun Shi",
      "institution": "Tsinghua University; Ministry of Education, Beijing, China",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81100647319",
      "orcid": "0000-0003-2273-6927"
    }
  ],
  "references": [
    {
      "text": "2019a. Amazon Echo. Website. (2019). Retrieved September 3, 2019 from https://www.amazon.com/gp/ help/customer/display.html?nodeId=201399130.",
      "doi": ""
    },
    {
      "text": "2019b. Baidu Text-To-Speech online tool. Website. (2019). Retrieved December 27th, 2019 from https://ai.baidu.com/tech/speech/tts.",
      "doi": ""
    },
    {
      "text": "2019c. Google Home. Website. (2019). Retrieved September 3, 2019 from https://store.google.com/product/google_home.",
      "doi": ""
    },
    {
      "text": "2019d. Online tutorial of fcal action coding systme. Website. (2019). Retrieved December 27th, 2019 from https://imotions.com/blog/facial-action-coding-system/.",
      "doi": ""
    },
    {
      "text": "Swapna Agarwal and Saiyed Umer. 2015. Mp-feg: media player controlled by facial expressions and gestures. In 2015 Fifth National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics (NCVPRIPG). IEEE, 1--4.",
      "doi": ""
    },
    {
      "text": "Tadas Baltru?aitis, Peter Robinson, and Louis-Philippe Morency. 2016. Openface: an open source facial behavior analysis toolkit. In 2016 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 1--10.",
      "doi": ""
    },
    {
      "text": "Erin Beneteau, Olivia K. Richards, Mingrui Zhang, Julie A. Kientz, Jason Yip, and Alexis Hiniker. 2019. Communication Breakdowns Between Families and Alexa. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (CHI '19). ACM, New York, NY, USA, Article 243, 13 pages. DOI: http://dx.doi.org/10.1145/3290605.3300473",
      "doi": "10.1145/3290605.3300473"
    },
    {
      "text": "Regina Bernhaupt. 2010. Evaluating user experience in games: Concepts and methods. Springer.",
      "doi": "10.5555/1830456"
    },
    {
      "text": "Dan Bohus. 2007. Error awareness and recovery in conversational spoken language interfaces. Technical Report. CARNEGIE-MELLON UNIV PITTSBURGH PA DEPT OF COMPUTER SCIENCE.",
      "doi": ""
    },
    {
      "text": "Dan Bohus and Alexander I Rudnicky. 2005. Sorry, I didn't catch that!-An investigation of non-understanding errors and recovery strategies. In 6th SIGdial workshop on discourse and dialogue.",
      "doi": ""
    },
    {
      "text": "Dan Bohus and Alexander I. Rudnicky. 2008. Sorry, I Didn't Catch That! Springer Netherlands, Dordrecht, 123--154. DOI: http://dx.doi.org/10.1007/978--1--4020--6821--8_6",
      "doi": ""
    },
    {
      "text": "Michael Braun, Anja Mainz, Ronee Chadowitz, Bastian Pfeging, and Florian Alt. 2019. At your service: Designing voice assistant personalities to improve automotive user interfaces. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. ACM, 40.",
      "doi": "10.1145/3290605.3300270"
    },
    {
      "text": "Yi Cheng, Kate Yen, Yeqi Chen, Sijin Chen, and Alexis Hiniker. 2018. Why Doesn't It Work?: Voice-driven Interfaces and Young Children's Communication Repair Strategies. In Proceedings of the 17th ACM Conference on Interaction Design and Children (IDC '18). ACM, New York, NY, USA, 337--348. DOI: http://dx.doi.org/10.1145/3202185.3202749",
      "doi": "10.1145/3202185.3202749"
    },
    {
      "text": "Leigh Clark, Phillip Doyle, Diego Garaialde, Emer Gilmartin, Stephan Schl\u00f6gl, Jens Edlund, Matthew Aylett, Jo\u00e3o Cabral, Cosmin Munteanu, and Benjamin Cowan. 2018. The State of Speech in HCI: Trends, Themes and Challenges. arXiv preprint arXiv:1810.06828 (2018).",
      "doi": ""
    },
    {
      "text": "Philip R Cohen and Sharon L Oviatt. 1995. The role of voice input for human-machine communication. proceedings of the National Academy of Sciences 92, 22 (1995), 9921--9927.",
      "doi": ""
    },
    {
      "text": "Benjamin R. Cowan, Nadia Pantidi, David Coyle, Kellie Morrissey, Peter Clarke, Sara Al-Shehri, David Earley, and Natasha Bandeira. 2017. \"What Can I Help You with?\": Infrequent Users' Experiences of Intelligent Personal Assistants. In Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services (MobileHCI '17). ACM, New York, NY, USA, Article 43, 12 pages. DOI: http://dx.doi.org/10.1145/3098279.3098539",
      "doi": "10.1145/3098279.3098539"
    },
    {
      "text": "Paul Ekman. 1993. Facial expression and emotion. American psychologist 48, 4 (1993), 384.",
      "doi": ""
    },
    {
      "text": "Paul Ekman and Wallace V. Friesen. 1978. Facial action coding system: a technique for the measurement of facial movement.",
      "doi": ""
    },
    {
      "text": "Paul Ekman and Dacher Keltner. 1997. Universal facial expressions of emotion. Segerstrale U, P. Molnar P, eds. Nonverbal communication: Where nature meets culture (1997), 27--46.",
      "doi": ""
    },
    {
      "text": "Rafael Mateo Ferr\u00fas and Manuel Dom\u00ednguez Somonte. 2016. Design in robotics based in the voice of the customer of household robots. Robotics and Autonomous Systems 79 (2016), 99--107.",
      "doi": "10.1016/j.robot.2016.01.010"
    },
    {
      "text": "Masaaki Fukumoto. 2018. SilentVoice: Unnoticeable Voice Input by Ingressive Speech. In Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology (UIST '18). ACM, New York, NY, USA, 237--246. DOI: http://dx.doi.org/10.1145/3242587.3242603",
      "doi": "10.1145/3242587.3242603"
    },
    {
      "text": "Anna Gruebler and Kenji Suzuki. 2014. Design of a wearable device for reading positive expressions from facial emg signals. IEEE Transactions on affective computing 5, 3 (2014), 227--237.",
      "doi": ""
    },
    {
      "text": "Richard Hazlett. 2003. Measurement of User Frustration: A Biologic Approach. In CHI '03 Extended Abstracts on Human Factors in Computing Systems (CHI EA '03). ACM, New York, NY, USA, 734--735. DOI: http://dx.doi.org/10.1145/765891.765958",
      "doi": "10.1145/765891.765958"
    },
    {
      "text": "Ryuichiro Higashinaka, Kotaro Funakoshi, Masahiro Araki, Hiroshi Tsukahara, Yuka Kobayashi, and Masahiro Mizukami. 2015. Towards Taxonomy of Errors in Chat-oriented Dialogue Systems. In Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue. Association for Computational Linguistics, Prague, Czech Republic, 87--95. DOI:http://dx.doi.org/10.18653/v1/W15--4611",
      "doi": ""
    },
    {
      "text": "Mirja Ilves, Yulia Gizatdinova, Veikko Surakka, and Esko Vankka. 2014. Head movement and facial expressions as game input. Entertainment Computing 5, 3 (2014), 147--156.",
      "doi": ""
    },
    {
      "text": "Julia Kiseleva, Kyle Williams, Ahmed Hassan Awadallah, Aidan C Crook, Imed Zitouni, and Tasos Anastasakos. 2016. Predicting user satisfaction with intelligent assistants. In Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval. ACM, 45--54.",
      "doi": "10.1145/2911451.2911521"
    },
    {
      "text": "Andrea Kleinsmith and Azin Semsar. 2019. Perception of Emotion in Body Expressions from Gaze Behavior. In Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems. ACM, LBW2416.",
      "doi": "10.1145/3290607.3313062"
    },
    {
      "text": "Margarita Kotti, Alexandros Papangelis, and Yannis Stylianou. 2017. Will this dialogue be unsuccessful? prediction using audio features. (2017).",
      "doi": ""
    },
    {
      "text": "Emiel Krahmer, Marc Swerts, Mariet Theune, and Mieke Weegels. 2001. Error detection in spoken human-machine interaction. International journal of speech technology 4, 1 (2001), 19--30.",
      "doi": ""
    },
    {
      "text": "Christian Lang, Sven Wachsmuth, Heiko Wersing, and Marc Hanheide. 2010. Facial expressions as feedback cue in human-robot interaction: comparison between human and automatic recognition performances. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition-Workshops. IEEE, 79--85.",
      "doi": ""
    },
    {
      "text": "Michael Lankes, Stefan Riegler, Astrid Weiss, Thomas Mirlacher, Michael Pirker, and Manfred Tscheligi. 2008. Facial expressions as game input with different emotional feedback conditions. In Proceedings of the 2008 International Conference on Advances in Computer Entertainment Technology. ACM, 253--256.",
      "doi": "10.1145/1501750.1501809"
    },
    {
      "text": "Josephine Lau, Benjamin Zimmerman, and Florian Schaub. 2018. Alexa, Are You Listening?: Privacy Perceptions, Concerns and Privacy-seeking Behaviors with Smart Speakers. Proc. ACM Hum.-Comput. Interact. 2, CSCW, Article 102 (Nov. 2018), 31 pages. DOI:http://dx.doi.org/10.1145/3274371",
      "doi": "10.1145/3274371"
    },
    {
      "text": "Key Jung Lee, Yeon Kyoung Joo, and Clifford Nass. 2014. Partially intelligent automobiles and driving experience at the moment of system transition. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 3631--3634.",
      "doi": "10.1145/2556288.2557370"
    },
    {
      "text": "Seungchul Lee, Chulhong Min, Alessandro Montanari, Akhil Mathur, Youngjae Chang, Junehwa Song, and Fahim Kawsar. 2019. Automatic Smile and Frown Recognition with Kinetic Earables. In Proceedings of the 10th Augmented Human International Conference 2019. ACM, 25.",
      "doi": "10.1145/3311823.3311869"
    },
    {
      "text": "Q Vera Liao, Mas-ud Hussain, Praveen Chandar, Matthew Davis, Yasaman Khazaeni, Marco Patricio Crasso, Dakuo Wang, Michael Muller, N Sadat Shami, Werner Geyer, and others. 2018. All Work and No Play?. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems. ACM, 3.",
      "doi": "10.1145/3173574.3173577"
    },
    {
      "text": "Weiyuan Liu. 2010. Natural user interface-next mainstream product user interface. In 2010 IEEE 11th International Conference on Computer-Aided Industrial Design & Conceptual Design 1, Vol. 1. IEEE, 203--205.",
      "doi": ""
    },
    {
      "text": "Athanasios Lykartsis, Margarita Kotti, Alexandros Papangelis, and Yannis Stylianou. 2018. Prediction of Dialogue Success with Spectral and Rhythm Acoustic Features Using DNNS and SVMS. In 2018 IEEE Spoken Language Technology Workshop (SLT). IEEE, 838--845.",
      "doi": ""
    },
    {
      "text": "David Maulsby, Saul Greenberg, and Richard Mander. 1993. Prototyping an intelligent agent through Wizard of Oz. In Proceedings of the INTERACT'93 and CHI'93 conference on Human factors in computing systems. ACM, 277--284.",
      "doi": "10.1145/169059.169215"
    },
    {
      "text": "Daniel McDuff, Abdelrahman Mahmoud, Mohammad Mavadati, May Amr, Jay Turcot, and Rana el Kaliouby. 2016. AFFDEX SDK: a cross-platform real-time multi-face expression recognition toolkit. In Proceedings of the 2016 CHI conference extended abstracts on human factors in computing systems. ACM, 3723--3726.",
      "doi": "10.1145/2851581.2890247"
    },
    {
      "text": "Raveesh Meena, Jos\u00e9 Lopes, Gabriel Skantze, and Joakim Gustafson. 2015. Automatic detection of miscommunication in spoken dialogue systems. In Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue. 354--363.",
      "doi": ""
    },
    {
      "text": "Rishabh Mehrotra, Ahmed Hassan Awadallah, Milad Shokouhi, Emine Yilmaz, Imed Zitouni, Ahmed El Kholy, and Madian Khabsa. 2017. Deep sequential models for task satisfaction prediction. In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management. ACM, 737--746.",
      "doi": "10.1145/3132847.3133001"
    },
    {
      "text": "A. S. A. Mohamed, M. N. Ab Wahab, S. S. Suhaily, and D. B. L. Arasu. 2018. Smart Mirror Design Powered by Raspberry PI. In Proceedings of the 2018 Artifcial Intelligence and Cloud Computing Conference (AICCC '18). ACM, New York, NY, USA, 166--173. DOI: http://dx.doi.org/10.1145/3299819.3299840",
      "doi": "10.1145/3299819.3299840"
    },
    {
      "text": "Narichika Nomoto, Hirokazu Masataki, Osamu Yoshioka, and Satoshi Takahashi. 2010. Detection of anger emotion in dialog speech using prosody feature and temporal relation of utterances. In Eleventh Annual Conference of the International Speech Communication Association.",
      "doi": ""
    },
    {
      "text": "Marianna Obrist, Judith Igelsb\u00f6ck, Elke Beck, Christiane Moser, Stefan Riegler, and Manfred Tscheligi. 2009. \"Now You Need to Laugh!\": Investigating Fun in Games with Children. In Proceedings of the International Conference on Advances in Computer Enterntainment Technology (ACE '09). ACM, New York, NY, USA, 81--88. DOI: http://dx.doi.org/10.1145/1690388.1690403",
      "doi": "10.1145/1690388.1690403"
    },
    {
      "text": "Alice Oh, Harold Fox, Max Van Kleek, Aaron Adler, Krzysztof Gajos, Louis-Philippe Morency, and Trevor Darrell. 2002a. Evaluating Look-to-talk: A Gaze-aware Interface in a Collaborative Environment. In CHI '02 Extended Abstracts on Human Factors in Computing Systems (CHI EA '02). ACM, New York, NY, USA, 650--651. DOI: http://dx.doi.org/10.1145/506443.506528",
      "doi": "10.1145/506443.506528"
    },
    {
      "text": "Alice Oh, Harold Fox, Max Van Kleek, Aaron Adler, Krzysztof Z Gajos, Louis-Philippe Morency, and Trevor Darrell. 2002b. Evaluating look-to-talk. (2002).",
      "doi": ""
    },
    {
      "text": "Alexandros Papangelis, Margarita Kotti, and Yannis Stylianou. 2017. Predicting dialogue success, naturalness, and length with acoustic features. In 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 5010--5014.",
      "doi": "10.1109/ICASSP.2017.7953110"
    },
    {
      "text": "Martin Porcheron, Joel E. Fischer, Stuart Reeves, and Sarah Sharples. 2018. Voice Interfaces in Everyday Life. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (CHI '18). ACM, New York, NY, USA, Article 640, 12 pages. DOI: http://dx.doi.org/10.1145/3173574.3174214",
      "doi": "10.1145/3173574.3174214"
    },
    {
      "text": "Fran\u00e7ois Portet, Michel Vacher, Caroline Golanski, Camille Roux, and Brigitte Meillon. 2013. Design and evaluation of a smart home voice interface for the elderly: acceptability and objection aspects. Personal and Ubiquitous Computing 17, 1 (2013), 127--144.",
      "doi": "10.1007/s00779-011-0470-5"
    },
    {
      "text": "David Reitter and Johanna D Moore. 2007. Predicting success in dialogue. (2007).",
      "doi": ""
    },
    {
      "text": "Paul Rozin and Adam B Cohen. 2003. High frequency of facial expressions corresponding to confusion, concentration, and worry in an analysis of naturally occurring facial expressions of Americans. Emotion 3, 1 (2003), 68.",
      "doi": ""
    },
    {
      "text": "Gabriel Skantze. 2005. Exploring human error recovery strategies: Implications for spoken dialogue systems. Speech Communication 45, 3 (2005), 325 -- 341. DOI: http://dx.doi.org/https: //doi.org/10.1016/j.specom.2004.11.005 Special Issue on Error Handling in Spoken Dialogue Systems.",
      "doi": ""
    },
    {
      "text": "Gabriel Skantze and Jens Edlund. 2004. Early error detection on word level. In COST278 and ISCA Tutorial and Research Workshop (ITRW) on Robustness Issues in Conversational Interaction.",
      "doi": ""
    },
    {
      "text": "Misha Sra, Xuhai Xu, and Pattie Maes. 2018. BreathVR: Leveraging Breathing As a Directly Controlled Interface for Virtual Reality Games. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (CHI '18). ACM, New York, NY, USA, Article 340, 12 pages. DOI: http://dx.doi.org/10.1145/3173574.3173914",
      "doi": "10.1145/3173574.3173914"
    },
    {
      "text": "Jacopo Staiano, Mar\u00eda Men\u00e9ndez, Alberto Battocchi, Antonella De Angeli, and Nicu Sebe. 2012. UX_Mate: from facial expressions to UX evaluation. In Proceedings of the Designing Interactive Systems Conference. ACM, 741--750.",
      "doi": "10.1145/2317956.2318068"
    },
    {
      "text": "Stefan Steidl, Christian Hacker, Christine Ruff, Anton Batliner, Elmar N\u00f6th, and J\u00fcrgen Haas. 2004. Looking at the last two turns, I'd say this dialogue is doomed--measuring dialogue success. In International Conference on Text, Speech and Dialogue. Springer, 629--636.",
      "doi": ""
    },
    {
      "text": "Ke Sun, Chun Yu, Weinan Shi, Lan Liu, and Yuanchun Shi. 2018. Lip-Interact: Improving Mobile Device Interaction with Silent Speech Commands. In Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology (UIST '18). ACM, New York, NY, USA, 581--593. DOI: http://dx.doi.org/10.1145/3242587.3242599",
      "doi": "10.1145/3242587.3242599"
    },
    {
      "text": "Joseph Tepperman, David Traum, and Shrikanth Narayanan. 2006. \" Yeah Right\": Sarcasm Recognition for Spoken Dialogue Systems. In Ninth International Conference on Spoken Language Processing.",
      "doi": ""
    },
    {
      "text": "Leimin Tian, Johanna Moore, and Catherine Lai. 2016. Recognizing emotions in spoken dialogue with hierarchically fused acoustic and lexical features. In 2016 IEEE Spoken Language Technology Workshop (SLT). IEEE, 565--572.",
      "doi": ""
    },
    {
      "text": "Alan Transon, Adrien Verhulst, Jean-Marie Normand, Guillaume Moreau, and Maki Sugimoto. 2017. Evaluation of Facial Expressions as an Interaction Mechanism and their Impact on Affect, Workload and Usability in an AR game. In 2017 23rd International Conference on Virtual System & Multimedia (VSMM). IEEE, 1--8.",
      "doi": ""
    },
    {
      "text": "Hanna Venesvirta, Oleg ?pakov, Yulia Gizatdinova, Outi Tuisku, Ville Rantanen, Jarmo Verho, Akos Vetek, Jukka Lekkala, and Veikko Surakka. 2017. Smile to save it: facial expressions for lifelogging. In Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia. ACM, 441--448.",
      "doi": "10.1145/3152832.3156628"
    },
    {
      "text": "Marilyn Walker, Irene Langkilde, Jerry Wright, Allen Gorin, and Diane Litman. 2000. Learning to predict problematic situations in a spoken dialogue system: experiments with how may i help you?. In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference. Association for Computational Linguistics, 210--217.",
      "doi": ""
    },
    {
      "text": "Xuhai Xu, Alexandru Dancu, Pattie Maes, and Suranga Nanayakkara. 2018. Hand Range Interface: Information Always at Hand with a Body-centric Mid-air Input Surface. In Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services (MobileHCI '18). ACM, New York, NY, USA, Article 5, 12 pages. DOI: http://dx.doi.org/10.1145/3229434.3229449",
      "doi": "10.1145/3229434.3229449"
    },
    {
      "text": "Xuhai Xu, Haitian Shi, Xin Yi, Wenjia Liu, Yukang Yan, Yuanchun Shi, Alex Mariakakis, Jennifer Mankoff, and Anind K. Dey. 2020. EarBuddy: Enabling On-Face Interaction via Wireless Earbuds. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (CHI '20). Association for Computing Machinery, New York, NY, USA, 14. DOI: http://dx.doi.org/10.1145/3313831.3376836",
      "doi": "10.1145/3313831.3376836"
    },
    {
      "text": "Xuhai Xu, Chun Yu, Anind K. Dey, and Jennifer Mankoff. 2019. Clench Interface: Novel Biting Input Techniques. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (CHI '19). ACM, New York, NY, USA, Article 275, 12 pages. DOI: http://dx.doi.org/10.1145/3290605.3300505",
      "doi": "10.1145/3290605.3300505"
    },
    {
      "text": "Yukang Yan, Chun Yu, Yingtian Shi, and Minxing Xie. 2019. PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones. In Proceedings of the 32Nd Annual ACM Symposium on User Interface Software and Technology (UIST '19). ACM, New York, NY, USA, 1013--1020. DOI:http://dx.doi.org/10.1145/3332165.3347950",
      "doi": "10.1145/3332165.3347950"
    },
    {
      "text": "Changeun Yang, Yuxuan Jiang, Pujana Paliyawan, Tomohiro Harada, and Ruck Thawonmas. 2018. Smile with Angry Birds: Two Smile-Interface Implementations. In 2018 Nicograph International (NicoInt). IEEE, 80--80.",
      "doi": ""
    },
    {
      "text": "Eric Zeng, Shrirang Mare, and Franziska Roesner. 2017. End user security and privacy concerns with smart homes. In Thirteenth Symposium on Usable Privacy and Security ({SOUPS} 2017). 65--80.",
      "doi": ""
    },
    {
      "text": "Teresa Zollo. 1999. A study of human dialogue strategies in the presence of speech recognition errors. In Proceedings of AAAI Fall Symposium on Psychological Models of Communication in Collaborative Systems. 132--139.",
      "doi": ""
    }
  ]
}