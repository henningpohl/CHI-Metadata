{
  "doi": "10.1145/3313831.3376638",
  "title": "A Case for Humans-in-the-Loop: Decisions in the Presence of Erroneous Algorithmic Scores",
  "published": "2020-04-23",
  "proctitle": "CHI '20: Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems",
  "pages": "1-12",
  "year": 2020,
  "badges": [],
  "abstract": "The increased use of algorithmic predictions in sensitive domains has been accompanied by both enthusiasm and concern. To understand the opportunities and risks of these technologies, it is key to study how experts alter their decisions when using such tools. In this paper, we study the adoption of an algorithmic tool used to assist child maltreatment hotline screening decisions. We focus on the question: Are humans capable of identifying cases in which the machine is wrong, and of overriding those recommendations? We first show that humans do alter their behavior when the tool is deployed. Then, we show that humans are less likely to adhere to the machine's recommendation when the score displayed is an incorrect estimate of risk, even when overriding the recommendation requires supervisory approval. These results highlight the risks of full automation and the importance of designing decision pipelines that provide humans with autonomy.",
  "tags": [
    "decision support",
    "algorithm aversion",
    "automation bias",
    "human-in-the-loop",
    "child welfare",
    "algorithm assisted decision making"
  ],
  "authors": [
    {
      "name": "Maria De-Arteaga",
      "institution": "Carnegie Mellon University, Pittsburgh, PA, USA",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99658746336",
      "orcid": "missing"
    },
    {
      "name": "Riccardo Fogliato",
      "institution": "Carnegie Mellon University, Pittsburgh, PA, USA",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659529116",
      "orcid": "missing"
    },
    {
      "name": "Alexandra Chouldechova",
      "institution": "Carnegie Mellon University, Pittsburgh, PA, USA",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81553917956",
      "orcid": "0000-0002-2337-9610"
    }
  ],
  "references": [
    {
      "text": "Stefan\u00eda \u00c6gisd\u00f3ttir, Michael J White, Paul M Spengler, Alan S Maugherman, Linda A Anderson, Robert S Cook, Cassandra N Nichols, Georgios K Lampropoulos, Blain S Walker, Genna Cohen, and others. 2006. The meta-analysis of clinical judgment project: Fifty-six years of accumulated research on clinical versus statistical prediction. The Counseling Psychologist 34, 3 (2006), 341--382.",
      "doi": ""
    },
    {
      "text": "Alex Albright. 2019. If You Give a Judge a Risk Score: Evidence from Kentucky Bail Decisions. Technical Report. Working paper.",
      "doi": ""
    },
    {
      "text": "Gagan Bansal, Besmira Nushi, Ece Kamar, Daniel S Weld, Walter S Lasecki, and Eric Horvitz. 2019. Updates in human-ai teams: Understanding and addressing the performance/compatibility tradeoff. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 33. 2429--2437.",
      "doi": "10.1609/aaai.v33i01.33012429"
    },
    {
      "text": "Reuben Binns, Max Van Kleek, Michael Veale, Ulrik Lyngs, Jun Zhao, and Nigel Shadbolt. 2018. 'It's Reducing a Human Being to a Percentage': Perceptions of Justice in Algorithmic Decisions. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems. ACM, 377.",
      "doi": "10.1145/3173574.3173951"
    },
    {
      "text": "Anna Brown, Alexandra Chouldechova, Emily Putnam-Hornstein, Andrew Tobin, and Rhema Vaithianathan. 2019. Toward Algorithmic Accountability in Public Services: A Qualitative Study of Affected Community Perspectives on Algorithmic Decision-making in Child Welfare Services. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. ACM, 41.",
      "doi": "10.1145/3290605.3300271"
    },
    {
      "text": "Shawn D Bushway, Emily G Owens, and Anne Morrison Piehl. 2012. Sentencing guidelines and judicial discretion: Quasi-experimental evidence from human calculation errors. Journal of Empirical Legal Studies 9, 2 (2012), 291--319.",
      "doi": ""
    },
    {
      "text": "Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie Elhadad. 2015. Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 1721--1730.",
      "doi": "10.1145/2783258.2788613"
    },
    {
      "text": "Alexandra Chouldechova, Diana Benavides-Prado, Oleksandr Fialko, and Rhema Vaithianathan. 2018. A case study of algorithm-assisted decision making in child maltreatment hotline screening decisions. In Conference on Fairness, Accountability and Transparency. 134--148.",
      "doi": ""
    },
    {
      "text": "Alma Cohen and Crystal S Yang. 2019. Judicial politics and sentencing decisions. American Economic Journal: Economic Policy 11, 1 (2019), 160--91.",
      "doi": ""
    },
    {
      "text": "Mary Cummings. 2004. Automation bias in intelligent time critical decision support systems. In AIAA 1st Intelligent Systems Technical Conference. 6313.",
      "doi": ""
    },
    {
      "text": "Robyn M Dawes, David Faust, and Paul E Meehl. 1989. Clinical versus actuarial judgment. Science 243, 4899 (1989), 1668--1674.",
      "doi": ""
    },
    {
      "text": "Matthew DeMichele, Peter Baumgartner, Kelle Barrick, Megan Comfort, Samuel Scaggs, and Shilpi Misra. 2018. What do criminal justice professionals think about risk assessment at pretrial? Available at SSRN 3168490 (2018).",
      "doi": ""
    },
    {
      "text": "Berkeley J Dietvorst, Joseph P Simmons, and Cade Massey. 2015. Algorithm aversion: People erroneously avoid algorithms after seeing them err. Journal of Experimental Psychology: General 144, 1 (2015), 114.",
      "doi": ""
    },
    {
      "text": "Berkeley J Dietvorst, Joseph P Simmons, and Cade Massey. 2016. Overcoming algorithm aversion: People will use imperfect algorithms if they can (even slightly) modify them. Management Science 64, 3 (2016), 1155--1170.",
      "doi": "10.1287/mnsc.2016.2643"
    },
    {
      "text": "Jennif Doleac and Megan Stevenson. 2018. The roadblock to reform. Technical Report. American Constitution Society Research Report.",
      "doi": ""
    },
    {
      "text": "Virginia Eubanks. 2018. Automating inequality: How high-tech tools profile, police, and punish the poor. St. Martin's Press.",
      "doi": "10.5555/3208509"
    },
    {
      "text": "Kate Goddard, Abdul Roudsari, and Jeremy C Wyatt. 2011. Automation bias: a systematic review of frequency, effect mediators, and mitigators. Journal of the American Medical Informatics Association 19, 1 (2011), 121--127.",
      "doi": ""
    },
    {
      "text": "Paul Goodwin and Robert Fildes. 1999. Judgmental forecasts of time series affected by special events: Does providing a statistical forecast improve accuracy? Journal of Behavioral Decision Making 12, 1 (1999), 37--53.",
      "doi": ""
    },
    {
      "text": "Ben Green and Yiling Chen. 2019. Disparate interactions: An algorithm-in-the-loop analysis of fairness in risk assessments. In Proceedings of the Conference on Fairness, Accountability, and Transparency. ACM, 90--99.",
      "doi": "10.1145/3287560.3287563"
    },
    {
      "text": "William M Grove, David H Zald, Boyd S Lebow, Beth E Snitz, and Chad Nelson. 2000. Clinical versus mechanical prediction: a meta-analysis. Psychological assessment 12, 1 (2000), 19.",
      "doi": ""
    },
    {
      "text": "Sophie Hilgard, Nir Rosenfeld, Mahzarin R Banaji, Jack Cao, and David C Parkes. 2019. Learning Representations by Humans, for Humans. arXiv preprint arXiv:1905.12686 (2019).",
      "doi": ""
    },
    {
      "text": "Shagun Jhaver, Amy Bruckman, and Eric Gilbert. 2019. Does transparency in moderation really matter? User behavior after content removal explanations on reddit. Proceedings of the ACM on Human-Computer Interaction 3, CSCW (2019), 1--27.",
      "doi": "10.1145/3359252"
    },
    {
      "text": "Danielle Leah Kehl and Samuel Ari Kessler. 2017. Algorithms in the criminal justice system: Assessing the use of risk assessments in sentencing. (2017).",
      "doi": ""
    },
    {
      "text": "Ren\u00e9 F Kizilcec. 2016. How much information?: Effects of transparency on trust in an algorithmic interface. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems. ACM, 2390--2395.",
      "doi": "10.1145/2858036.2858402"
    },
    {
      "text": "Jon Kleinberg, Himabindu Lakkaraju, Jure Leskovec, Jens Ludwig, and Sendhil Mullainathan. 2017. Human decisions and machine predictions. The quarterly journal of economics 133, 1 (2017), 237--293.",
      "doi": ""
    },
    {
      "text": "Amanda Kube, Sanmay Das, and Patrick J Fowler. 2019. Allocating interventions based on predicted outcomes: A case study on homelessness services. In Proceedings of the AAAI Conference on Artificial Intelligence.",
      "doi": "10.1609/aaai.v33i01.3301622"
    },
    {
      "text": "Himabindu Lakkaraju and Osbert Bastani. 2019. \" How do I fool you?\": Manipulating User Trust via Misleading Black Box Explanations. arXiv preprint arXiv:1911.06473 (2019).",
      "doi": ""
    },
    {
      "text": "John D Lee and Katrina A See. 2004. Trust in automation: Designing for appropriate reliance. Human factors 46, 1 (2004), 50--80.",
      "doi": ""
    },
    {
      "text": "Joa Sang Lim and Marcus O'Connor. 1995. Judgemental adjustment of initial forecasts: Its effectiveness and biases. Journal of Behavioral Decision Making 8, 3 (1995), 149--168.",
      "doi": ""
    },
    {
      "text": "David Madras, Toni Pitassi, and Richard Zemel. 2018. Predict responsibly: improving fairness and accuracy by learning to defer. In Advances in Neural Information Processing Systems. 6147--6157.",
      "doi": ""
    },
    {
      "text": "Katharina Marten, Tobias Seyfarth, Florian Auer, Edzard Wiener, Andreas Grillh\u00f6sl, Silvia Obenauer, Ernst J Rummeny, and Christoph Engelke. 2004. Computer-assisted detection of pulmonary nodules: performance evaluation of an expert knowledge-based detection system in consensus reading with experienced and inexperienced chest radiologists. European radiology 14, 10 (2004), 1930--1938.",
      "doi": ""
    },
    {
      "text": "Paul E Meehl. 1954. Clinical versus statistical prediction: A theoretical analysis and a review of the evidence. In In Proceedings of the 1955 Invitational Conference on Testing Problems. University of Minnesota Press, 136--141.",
      "doi": ""
    },
    {
      "text": "Neville Moray, Toshiyuki Inagaki, and Makoto Itoh. 2000. Adaptive automation, trust, and self-confidence in fault management of time-critical tasks. Journal of experimental psychology: Applied 6, 1 (2000), 44.",
      "doi": ""
    },
    {
      "text": "Kathleen L Mosier, Melisa Dunbar, Lori McDonnell, Linda J Skitka, Mark Burdick, and Bonnie Rosenblatt. 1998a. Automation bias and errors: Are teams better than individuals?. In Proceedings of the Human Factors and Ergonomics Society Annual Meeting, Vol. 42. SAGE Publications Sage CA: Los Angeles, CA, 201--205.",
      "doi": ""
    },
    {
      "text": "Kathleen L. Mosier, Linda J. Skitka, Susan Heers, and Mark Burdick. 1998b. Automation Bias: Decision Making and Performance in High-Tech Cockpits. The International Journal of Aviation Psychology 8, 1 (1998), 47--63. DOI: http://dx.doi.org/10.1207/s15327108ijap0801_3",
      "doi": ""
    },
    {
      "text": "Daniel S. Nagin and Robert J. Sampson. 2019. The Real Gold Standard: Measuring Counterfactual Worlds That Matter Most to Social Science and Policy. Annual Review of Criminology 2, 1 (2019), 123--145. DOI: http://dx.doi.org/10.1146/annurev-criminol-011518-024838",
      "doi": ""
    },
    {
      "text": "Mahsan Nourani, Samia Kabir, Sina Mohseni, and Eric D Ragan. 2019. The Effects of Meaningful and Meaningless Explanations on Trust and Perceived System Accuracy in Intelligent Systems. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, Vol. 7. 97--105.",
      "doi": ""
    },
    {
      "text": "Allegheny County Department of Health and Human Services. (????). https://www.alleghenycounty.us/Human-Services/News-Events/ Accomplishments/Allegheny-Family-Screening-Tool.aspx",
      "doi": ""
    },
    {
      "text": "Emilee Rader, Kelley Cotter, and Janghee Cho. 2018. Explanations as mechanisms for supporting algorithmic transparency. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems. ACM, 103.",
      "doi": "10.1145/3173574.3173677"
    },
    {
      "text": "Maithra Raghu, Katy Blumer, Greg Corrado, Jon Kleinberg, Ziad Obermeyer, and Sendhil Mullainathan. 2019. The algorithmic automation problem: Prediction, triage, and human effort. arXiv preprint arXiv:1903.12220 (2019).",
      "doi": ""
    },
    {
      "text": "Nadine B. Sarter and Beth Schroeder. 2001. Supporting decision making and action selection under time pressure and uncertainty: The case of in-flight icing. Human factors 43, 4 (2001), 573--583.",
      "doi": ""
    },
    {
      "text": "Jennifer L. Skeem, Nicholas Scurich, and John Monahan. 2019. Impact of Risk Assessment on Judges. Fairness in Sentencing Relatively Poor Defendants. Virginia Public Law and Legal Theory Research Paper 2019-02 (2019).",
      "doi": ""
    },
    {
      "text": "Linda J. Skitka, Kathleen L. Mosier, Mark Burdick, and Bonnie Rosenblatt. 2000. Automation bias and errors: are crews better than individuals? The International journal of aviation psychology 10, 1 (2000), 85--97.",
      "doi": ""
    },
    {
      "text": "CarlyWill Sloan, George Naufal, and Heather Caspers. 2018. The Effect of Risk Assessment Scores on Judicial Behavior and Defendant Outcomes. (2018).",
      "doi": ""
    },
    {
      "text": "Vernon C Smith, Adam Lange, and Daniel R Huston. 2012. Predictive modeling to forecast student outcomes and drive effective interventions in online community college courses. Journal of Asynchronous Learning Networks 16, 3 (2012), 51--61.",
      "doi": ""
    },
    {
      "text": "Megan Stevenson. 2018. Assessing risk assessment in action. Minn. L. Rev. 103 (2018), 303.",
      "doi": ""
    },
    {
      "text": "Sarah Tan, Julius Adebayo, Kori Inkpen, and Ece Kamar. 2018. Investigating Human+ Machine Complementarity for Recidivism Predictions. arXiv preprint arXiv:1808.09123 (2018).",
      "doi": ""
    },
    {
      "text": "Niels van Berkel, Jorge Goncalves, Danula Hettiachchi, Senuri Wijenayake, Ryan M Kelly, and Vassilis Kostakos. 2019. Crowdsourcing Perceptions of Fair Predictors for Machine Learning: A Recidivism Case Study. Proceedings of the ACM on Human-Computer Interaction 3, CSCW (2019), 1--21.",
      "doi": "10.1145/3359130"
    },
    {
      "text": "Michael Yeomans, Anuj Shah, Sendhil Mullainathan, and Jon Kleinberg. 2017. Making sense of recommendations. Journal of Behavioral Decision Making (2017).",
      "doi": ""
    },
    {
      "text": "Ming Yin, Jennifer Wortman Vaughan, and Hanna Wallach. 2019. Understanding the Effect of Accuracy on Trust in Machine Learning Models. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. ACM, 279.",
      "doi": "10.1145/3290605.3300509"
    },
    {
      "text": "Kun Yu, Shlomo Berkovsky, Dan Conway, Ronnie Taib, Jianlong Zhou, and Fang Chen. 2016. Trust and reliance based on system accuracy. In Proceedings of the 2016 Conference on User Modeling Adaptation and Personalization. ACM, 223--227.",
      "doi": "10.1145/2930238.2930290"
    },
    {
      "text": "Kun Yu, Shlomo Berkovsky, Ronnie Taib, Dan Conway, Jianlong Zhou, and Fang Chen. 2017. User trust dynamics: An investigation driven by differences in system performance. In Proceedings of the 22nd International Conference on Intelligent User Interfaces. ACM, 307--317.",
      "doi": "10.1145/3025171.3025219"
    }
  ]
}