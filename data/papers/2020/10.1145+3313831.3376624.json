{
  "doi": "10.1145/3313831.3376624",
  "title": "No Explainability without Accountability: An Empirical Study of Explanations and Feedback in Interactive ML",
  "published": "2020-04-23",
  "proctitle": "CHI '20: Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems",
  "pages": "1-13",
  "year": 2020,
  "badges": [],
  "abstract": "Automatically generated explanations of how machine learning (ML) models reason can help users understand and accept them. However, explanations can have unintended consequences: promoting over-reliance or undermining trust. This paper investigates how explanations shape users' perceptions of ML models with or without the ability to provide feedback to them: (1) does revealing model flaws increase users' desire to \"fix\" them; (2) does providing explanations cause users to believe - wrongly - that models are introspective, and will thus improve over time. Through two controlled experiments - varying model quality - we show how the combination of explanations and user feedback impacted perceptions, such as frustration and expectations of model improvement. Explanations without opportunity for feedback were frustrating with a lower quality model, while interactions between explanation and feedback for the higher quality model suggest that detailed feedback should not be requested without explanation. Users expected model correction, regardless of whether they provided feedback or received explanations.",
  "authors": [
    {
      "name": "Alison Smith-Renner",
      "institution": "University of Maryland, College Park, MD, USA",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659323801",
      "orcid": "missing"
    },
    {
      "name": "Ron Fan",
      "institution": "University of Washington, Seattle, WA, USA",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659528539",
      "orcid": "missing"
    },
    {
      "name": "Melissa Birchfield",
      "institution": "University of Washington, Seattle, WA, USA",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659528316",
      "orcid": "missing"
    },
    {
      "name": "Tongshuang Wu",
      "institution": "University of Washington, Seattle, WA, USA",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659439447",
      "orcid": "missing"
    },
    {
      "name": "Jordan Boyd-Graber",
      "institution": "University of Maryland, College Park, MD, USA",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81436600158",
      "orcid": "missing"
    },
    {
      "name": "Daniel S. Weld",
      "institution": "University of Washington, Seattle, WA, USA",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659376950",
      "orcid": "0000-0002-3255-0109"
    },
    {
      "name": "Leah Findlater",
      "institution": "University of Washington, Seattle, WA, USA",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81100170743",
      "orcid": "0000-0002-5619-4452"
    }
  ],
  "references": [
    {
      "text": "Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim. 2018. Sanity Checks for Saliency Maps. In Proceedings of Advances in Neural Information Processing Systems.",
      "doi": ""
    },
    {
      "text": "Jae Wook Ahn, Peter Brusilovsky, Jonathan Grady, Daqing He, and Sue Yeon Syn. 2007. Open User Profiles for Adaptive News Systems: Help or Harm?. In Proceedings of the World Wide Web Conference.",
      "doi": "10.1145/1242572.1242575"
    },
    {
      "text": "David Alvarez-Melis and Tommi S. Jaakkola. 2018. Towards Robust Interpretability with Self-explaining Neural Networks. In Proceedings of Advances in Neural Information Processing Systems.",
      "doi": ""
    },
    {
      "text": "Saleema Amershi, Maya Cakmak, William Bradley Knox, and Todd Kulesza. 2014. Power to the People: The Role of Humans in Interactive Machine Learning. AI Magazine (2014), 105--120.",
      "doi": ""
    },
    {
      "text": "Saleema Amershi, James Fogarty, Ashish Kapoor, and Desney Tan. 2010. Examining Multiple Potential Models in End-user Interactive Concept Learning. In International Conference on Human Factors in Computing Systems.",
      "doi": ""
    },
    {
      "text": "Saleema Amershi, James Fogarty, and Daniel Weld. 2012. ReGroup: Interactive Machine learning for On-demand Group Creation in Social Networks. In International Conference on Human Factors in Computing Systems.",
      "doi": "10.1145/2207676.2207680"
    },
    {
      "text": "Maria-Florina Balcan and Avrim Blum. 2008. Clustering with Interactive Feedback. In International Conference on Algorithmic Learning Theory.",
      "doi": ""
    },
    {
      "text": "Gagan Bansal, Besmira Nushi, Ece Kamar, Daniel S Weld, Walter S Lasecki, and Eric Horvitz. 2019. Updates in Human-AI Teams: Understanding and Addressing the Performance/Compatibility Tradeoff. Association for the Advancement of Artificial Intelligence (2019).",
      "doi": ""
    },
    {
      "text": "Mustafa Bilgic and Raymond J Mooney. 2005. Explaining Recommendations: Satisfaction vs. Promotion. In Proceedings of Beyond Personalization 2005: A Workshop on the Next Stage of Recommender Systems Research at IUI.",
      "doi": ""
    },
    {
      "text": "Or Biran and Kathleen McKeown. 2017. Human-centric Justification of Machine Learning Predictions. In International Joint Conference on Artificial Intelligence.",
      "doi": ""
    },
    {
      "text": "Andrea Bunt, Joanna McGrenere, and Cristina Conati. 2007. Understanding the Utility of Rationale in a Mixed-Initiative System for GUI Customization. In International Conference on User Modeling.",
      "doi": "10.1007/978-3-540-73078-1_18"
    },
    {
      "text": "Carrie J. Cai, Jonas Jongejan, and Jess Holbrook. 2019. The Effects of Example-based Explanations in a Machine Learning Interface. In International Conference on Intelligent User Interfaces.",
      "doi": "10.1145/3301275.3302289"
    },
    {
      "text": "Oana Maria Camburu, Tim Rockt\u00e4schel, Thomas Lukasiewicz, and Phil Blunsom. 2018. E-SNLI: Natural Language Inference with Natural Language Explanations. In Proceedings of Advances in Neural Information Processing Systems.",
      "doi": ""
    },
    {
      "text": "Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and No\u00e9mie Elhadad. 2015. Intelligible Models for Healthcare: Predicting Pneumonia Risk and Hospital 30-day Readmission. In Knowledge Discovery and Data Mining.",
      "doi": ""
    },
    {
      "text": "Arjun Chandrasekaran, Viraj Prabhu, Deshraj Yadav, Prithvijit Chattopadhyay, and Devi Parikh. 2018. Do Explanations make VQA Models more Predictable to a Human?. In Proceedings of Empirical Methods in Natural Language Processing.",
      "doi": ""
    },
    {
      "text": "Henriette Cramer, Vanessa Evers, Satyan Ramlal, Maarten Van Someren, Lloyd Rutledge, Natalia Stash, Lora Aroyo, and Bob Wielinga. 2008. The Effects of Transparency on Trust in and Acceptance of a Content-based Art Recommender. User Modeling and User-Adapted Interaction (2008).",
      "doi": ""
    },
    {
      "text": "Aron Culotta, Trausti Kristjansson, Andrew McCallum, and Paul Viola. 2006. Corrective Feedback and Persistent Learning for Information Extraction. Artificial Intelligence (2006).",
      "doi": ""
    },
    {
      "text": "Jonathan Dodge, Q. Vera Liao, Yunfeng Zhang, Rachel K.E. Bellamy, and Casey Dugan. 2019. Explaining Models: An Empirical Study of How Explanations Impact Fairness Judgment. In International Conference on Intelligent User Interfaces.",
      "doi": "10.1145/3301275.3302310"
    },
    {
      "text": "Upol Ehsan, Pradyumna Tambwekar, Larry Chan, Brent Harrison, and Mark O. Riedl. 2019. Automated Rationale Generation: A Technique for Explainable AI and its Effects on Human Perceptions. In International Conference on Intelligent User Interfaces.",
      "doi": ""
    },
    {
      "text": "Jerry Alan Fails and Dan R Olsen. 2003. Interactive Machine Learning. In International Conference on Intelligent User Interfaces.",
      "doi": ""
    },
    {
      "text": "Shi Feng and Jordan Boyd-Graber. 2019. What can AI do for me? Evaluating Machine Learning Interpretations in Cooperative Play. In International Conference on Intelligent User Interfaces.",
      "doi": "10.1145/3301275.3302265"
    },
    {
      "text": "Rebecca Fiebrink, Dan Trueman, and Perry R Cook. 2009. A Metainstrument for Interactive, On-the-fly Machine Learning. In Proceedings of New Interfaces for Musical Expression.",
      "doi": ""
    },
    {
      "text": "Dimitra Gkatzia, Oliver Lemon, and Verena Rieser. 2016. Natural Language Generation Enhances Human Decision-making with Uncertain Information. In Proceedings of the Association for Computational Linguistics.",
      "doi": ""
    },
    {
      "text": "Dorota Glowacka, Tuukka Ruotsalo, Ksenia Konyushkova, Kumaripaba Athukorala, Samuel Kaski, and Giulio Jacucci. 2013. Directing Exploratory Search: Reinforcement Learning from User Interactions with Keywords. In International Conference on Intelligent User Interfaces.",
      "doi": "10.1145/2449396.2449413"
    },
    {
      "text": "Bryce Goodman and Seth Flaxman. 2017. European Union Regulations on Algorithmic Decision Making and a -Right to Explanation\". AI Magazine (2017).",
      "doi": ""
    },
    {
      "text": "Dave Gunning. 2016. Explainable Artificial Intelligence (XAI). (2016). https://www.darpa.mil/program/ explainable-artificial-intelligence",
      "doi": ""
    },
    {
      "text": "Jonathan L. Herlocker, Joseph A. Konstan, and John Riedl. 2000. Explaining Collaborative Filtering Recommendations. In Conference on Computer Supported Cooperative Work and Social Computing.",
      "doi": ""
    },
    {
      "text": "Kristina H\u00f6\u00f6k. 2000. Steps to Take before Intelligent User Interfaces become Real. Interacting With Computers 12, 4 (2000), 409--426.",
      "doi": "10.1016/s0953-5438%2899%2900006-5"
    },
    {
      "text": "Yuening Hu, Jordan Boyd-Graber, Brianna Satinoff, and Alison Smith. 2014. Interactive Topic Modeling. Machine Learning 95, 3 (6 2014), 423--469.",
      "doi": ""
    },
    {
      "text": "Anna Huang. 2008. Similarity Measures for Text Document Clustering. In New Zealand Computer Science Research Student Conference.",
      "doi": ""
    },
    {
      "text": "W. Bradley Knox and Peter Stone. 2012. Reinforcement Learning from Human Reward: Discounting in Episodic Tasks. In IEEE International Workshop on Robot and Human Interactive Communication.",
      "doi": ""
    },
    {
      "text": "Rafal Kocielnik, Saleema Amershi, and Paul N Bennett. 2019. Will you Accept an Imperfect AI? Exploring Designs for Adjusting End-user Expectations of AI systems. In International Conference on Human Factors in Computing Systems.",
      "doi": ""
    },
    {
      "text": "Todd Kulesza, Margaret Burnett, Weng-Keen Wong, and Simone Stumpf. 2015. Principles of Explanatory Debugging to Personalize Interactive Machine Learning. In International Conference on Intelligent User Interfaces.",
      "doi": "10.1145/2678025.2701399"
    },
    {
      "text": "Todd Kulesza, Simone Stumpf, Margaret Burnett, and Irwin Kwan. 2012. Tell me More? The Effects of Mental Model Soundness on Personalizing an Intelligent Agent. In International Conference on Human Factors in Computing Systems.",
      "doi": ""
    },
    {
      "text": "Todd Kulesza, Simone Stumpf, Margaret Burnett, Sherry Yang, Irwin Kwan, and Weng Keen Wong. 2013. Too Much, too Little, or Just Right? Ways Explanations Impact End Users' Mental Models. In IEEE Symposium on Visual Languages and Human-Centric Computing.",
      "doi": ""
    },
    {
      "text": "Isaac Lage, Andrew Slavin Ross, Been Kim, Samuel J. Gershman, and Finale Doshi-Velez. 2018. Human-in-the-loop Interpretability Prior. In Proceedings of Advances in Neural Information Processing Systems.",
      "doi": ""
    },
    {
      "text": "Himabindu Lakkaraju, Rich Caruana, Ece Kamar, and Jure Leskovec. 2019. Faithful and Customizable Explanations of Black Box Models. In Conference on AI, Ethics, and Society.",
      "doi": ""
    },
    {
      "text": "Ken Lang. 1995. NewsWeeder: Learning to Filter Netnews. In Machine Learning Proceedings.",
      "doi": ""
    },
    {
      "text": "Hanseung Lee, Jaeyeon Kihm, Jaegul Choo, John Stasko, and Haesun Park. 2012. iVisClustering: An Interactive Visual Document Clustering via Topic Modeling. Computer Graphics Forum 31 (2012), 1155--1164.",
      "doi": "10.1111/j.1467-8659.2012.03108.x"
    },
    {
      "text": "Tao Lei, Regina Barzilay, and Tommi Jaakkola. 2016. Rationalizing Neural Predictions. In Proceedings of Empirical Methods in Natural Language Processing.",
      "doi": ""
    },
    {
      "text": "David D Lewis. 1998. Naive (Bayes) at Forty: The Independence Assumption in Information Retrieval. In Proceedings of European Conference of Machine Learning.",
      "doi": "10.1007/BFb0026666"
    },
    {
      "text": "Brian Lim, Anind Dey, and Daniel Avrahami. 2009. Why and Why Not Explanations Improve the Intelligibility of Context-aware Intelligent Systems. (2009).",
      "doi": ""
    },
    {
      "text": "Brian Y. Lim and Anind K. Dey. 2011. Investigating Intelligibility for Uncertain Context-aware Applications. In Proceedings of the international conference on Ubiquitous computing.",
      "doi": ""
    },
    {
      "text": "Menaka Narayanan, Emily Chen, Jeffrey He, Been Kim, Sam Gershman, and Finale Doshi-Velez. 2018. How do Humans Understand Explanations from Machine Learning Systems? An Evaluation of the Human-interpretability of Explanation. (2018). http://arxiv.org/abs/1802.00682",
      "doi": ""
    },
    {
      "text": "Dong Huk Park, Lisa Anne Hendricks, Zeynep Akata, Anna Rohrbach, Bernt Schiele, Trevor Darrell, and Marcus Rohrbach. 2017. Attentive Explanations: Justifying Decisions and Pointing to the Evidence. In Computer Vision and Pattern Recognition.",
      "doi": ""
    },
    {
      "text": "Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and \u00c9douard Duchesnay. 2011. scikit-learn: Machine Learning in Python. Journal of Machine Learning Research 12 (2011), 2825--2830.",
      "doi": "10.5555/1953048.2078195"
    },
    {
      "text": "Forough Poursabzi-Sangdeh, Daniel G Goldstein, Jake M Hofman, Jennifer Wortman Vaughan, and Hanna Wallach. 2018. Manipulating and Measuring Model Interpretability. (2018). http://arxiv.org/abs/1802.07810",
      "doi": ""
    },
    {
      "text": "Pearl Pu and Li Chen. 2006. Trust Building with Explanation Interfaces. In International Conference on Intelligent User Interfaces.",
      "doi": ""
    },
    {
      "text": "Hema Raghavan, Omid Madani, and Rosie Jones. 2006. Active Learning with Feedback on both Features and Instances. Journal of Machine Learning Research (2006).",
      "doi": ""
    },
    {
      "text": "Paul Resnick, Neophytos Iacovou, Mitesh Suchak, Peter Bergstrom, and John T. Riedl. 1994. GroupLens: An Open Architecture for Collaborative Filtering of Netnews. In Conference on Computer Supported Cooperative Work and Social Computing.",
      "doi": ""
    },
    {
      "text": "Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. Why Should I Trust You? Explaining the Predictions of any Classifier. In Knowledge Discovery and Data Mining.",
      "doi": ""
    },
    {
      "text": "Stephanie Rosenthal and Anind K Dey. 2010. Towards Maximizing the Accuracy of Human-labeled Sensor Data. In International Conference on Intelligent User Interfaces.",
      "doi": ""
    },
    {
      "text": "William Saunders, Andreas Stuhlm\u00fcller, Girish Sastry, and Owain Evans. 2018. Trial without Error: Towards Safe Reinforcement Learning via Human Intervention. In International Joint Conference on Autonomous Agents and Multiagent Systems.",
      "doi": ""
    },
    {
      "text": "Philipp Schmidt and Felix Biessmann. 2019. Quantifying Interpretability and Trust in Machine Learning Systems. (2019). http://arxiv.org/abs/1901.08558",
      "doi": ""
    },
    {
      "text": "Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. 2017. Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization. In International Conference on Computer Vision.",
      "doi": ""
    },
    {
      "text": "Burr Settles. 2010. Active Learning Literature Survey. Machine Learning 15, 2 (2010), 201--221.",
      "doi": ""
    },
    {
      "text": "Burr Settles. 2011. Closing the Loop: Fast, Interactive Semi-supervised Annotation with Queries on Features and Instances. In Proceedings of Empirical Methods in Natural Language Processing.",
      "doi": ""
    },
    {
      "text": "Zhangzhang Si and Song Chun Zhu. 2013. Learning and-or Templates for Object Recognition and Detection. IEEE Transactions on Pattern Analysis and Machine Intelligence (2013).",
      "doi": "10.1109/TPAMI.2013.35"
    },
    {
      "text": "Robert S. Siegler. 2002. Microgenetic Studies of Self-explanation. Cambridge University Press, 31--58.",
      "doi": ""
    },
    {
      "text": "Alison Smith, Varun Kumar, Jordan Boyd-Graber, Kevin Seppi, and Leah Findlater. 2018. Closing the Loop: User-centered Design and Evaluation of a Human-in-the-loop Topic Modeling System. In International Conference on Intelligent User Interfaces.",
      "doi": "10.1145/3172944.3172965"
    },
    {
      "text": "Kimberly Stowers, Nicholas Kasdaglis, Michael Rupp, Jessie Chen, Daniel Barber, and Michael Barnes. 2017. Insights into Human-agent Teaming: Intelligent Agent Transparency and Uncertainty. In Advances in Intelligent Systems and Computing.",
      "doi": ""
    },
    {
      "text": "Simone Stumpf. 2016. Explanations Considered Harmful? User Interactions with Machine Learning Systems. In ACM SIGCHI Workshop on Human-Centered Machine Learning.",
      "doi": ""
    },
    {
      "text": "Emily Wall, Soroush Ghorashi, and Gonzalo Ramos. 2019. Using Expert Patterns in Assisted Interactive Machine Learning: A Study in Machine Teaching. In IFIP Conference on Human-Computer Interaction.",
      "doi": ""
    },
    {
      "text": "Jacob O. Wobbrock, Leah Findlater, Darren Gergle, and James J. Higgins. 2011. The Aligned Rank Transform for Nonparametric Factorial Analyses Using only ANOVA Procedures. In International Conference on Human Factors in Computing Systems.",
      "doi": ""
    },
    {
      "text": "Tongshuang Wu, Daniel S. Weld, and Jeffrey Heer. 2019. Local Decision Pitfalls in Interactive Machine Learning: An Investigation into Feature Selection in Sentiment Analysis. ACM Transactions on Computer-Human Interaction (2019).",
      "doi": "10.1145/3319616"
    },
    {
      "text": "Ming Yin, Jennifer Wortman Vaughan, and Hanna Wallach. 2019. Understanding the Effect of Accuracy on Trust in Machine Learning Models. In International Conference on Human Factors in Computing Systems.",
      "doi": ""
    }
  ]
}