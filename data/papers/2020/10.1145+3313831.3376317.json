{
  "doi": "10.1145/3313831.3376317",
  "title": "TAGSwipe: Touch Assisted Gaze Swipe for Text Entry",
  "published": "2020-04-23",
  "proctitle": "CHI '20: Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems",
  "pages": "1-12",
  "year": 2020,
  "badges": [],
  "abstract": "The conventional dwell-based methods for text entry by gaze are typically slow and uncomfortable. A swipe-based method that maps gaze path into words offers an alternative. However, it requires the user to explicitly indicate the beginning and ending of a word, which is typically achieved by tedious gaze-only selection. This paper introduces TAGSwipe, a bi-modal method that combines the simplicity of touch with the speed of gaze for swiping through a word. The result is an efficient and comfortable dwell-free text entry method. In the lab study TAGSwipe achieved an average text entry rate of 15.46 wpm and significantly outperformed conventional swipe-based and dwell-based methods in efficacy and user satisfaction.",
  "tags": [
    "dwell-free typing",
    "swipe",
    "eye tracking",
    "multimodal interaction",
    "touch input",
    "eye typing",
    "word-level text entry"
  ],
  "authors": [
    {
      "name": "Chandan Kumar",
      "institution": "University of Koblenz-Landau, Koblenz, Germany",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659135081",
      "orcid": "missing"
    },
    {
      "name": "Ramin Hedeshy",
      "institution": "University of Koblenz-Landau, Koblenz, Germany",
      "img": "/do/10.1145/contrib-99659526501/rel-imgonly/raminhedeshy__2_.jpg",
      "acmid": "99659526501",
      "orcid": "missing"
    },
    {
      "name": "I. Scott MacKenzie",
      "institution": "York University, Toronto, ON, Canada",
      "img": "/do/10.1145/contrib-81100367406/rel-imgonly/me8.jpg",
      "acmid": "81100367406",
      "orcid": "missing"
    },
    {
      "name": "Steffen Staab",
      "institution": "University of Stuttgart & University of Southampton, Stuttgart, Germany",
      "img": "/do/10.1145/contrib-81409593685/rel-imgonly/akademiepreis-portrait-staab.png",
      "acmid": "81409593685",
      "orcid": "missing"
    }
  ],
  "references": [
    {
      "text": "Sunggeun Ahn and Geehyuk Lee. 2019. Gaze-assisted typing for smart glasses. In Proceedings of the ACM Symposium on User Interface Software and Technology (UIST '19). ACM, New York, 857--869. DOI: http://dx.doi.org/10.1145/3332165.3347883",
      "doi": "10.1145/3332165.3347883"
    },
    {
      "text": "Tanya Ren\u00e9 Beelders and Pieter J Blignaut. 2012. Measuring the performance of gaze and speech for text input. In Proceedings of the ACM Symposium on Eye Tracking Research and Applications (ETRA '12). ACM, New York, 337--340.",
      "doi": "10.1145/2168556.2168631"
    },
    {
      "text": "Alexander De Luca, Roman Weiss, and Heiko Drewes. 2007. Evaluation of eye-gaze interaction methods for security enhanced PIN-entry. In Proceedings of the 19th Australasian Conference on Computer-Human Interaction (OzCHI '07). ACM, New York, 199--202. DOI: http://dx.doi.org/10.1145/1324892.1324932",
      "doi": ""
    },
    {
      "text": "Antonio Diaz-Tula and Carlos H. Morimoto. 2016. AugKey: Increasing foveal throughput in eye typing with augmented keys. In Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems (CHI '16). ACM, New York, 3533--3544. DOI: http://dx.doi.org/10.1145/2858036.2858517",
      "doi": ""
    },
    {
      "text": "Heiko Drewes and Albrecht Schmidt. 2009. The MAGIC touch: Combining MAGIC-pointing with a touch-sensitive mouse. In IFIP Conference on Human-Computer Interaction. Springer, Berlin, 415--428.",
      "doi": "10.1007/978-3-642-03658-3_46"
    },
    {
      "text": "Anna Maria Feit, Shane Williams, Arturo Toledo, Ann Paradiso, Harish Kulkarni, Shaun Kane, and 5https://www.tobiidynavox.com/products Meredith Ringel Morris. 2017. Toward everyday gaze input: Accuracy and precision of eye tracking and implications for design. In Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems (CHI '17). ACM, New York, 1118--1130. DOI: http://dx.doi.org/10.1145/3025453.3025599",
      "doi": "10.1145/3025453.3025599"
    },
    {
      "text": "M Maurice Fr\u00e9chet. 1906. Sur quelques points du calcul fonctionnel. Rendiconti del Circolo Matematico di Palermo (1884--1940) 22, 1 (1906), 1--72.",
      "doi": ""
    },
    {
      "text": "John Paulin Hansen, Anders Sewerin Johansen, Dan Witzner Hansen, Kenji Itoh, and Satoru Mashino. 2003. Command without a click: Dwell time typing by mouse and gaze selections. In Proceedings of Human-Computer Interaction--INTERACT. Springer, Berlin, 121--128.",
      "doi": ""
    },
    {
      "text": "Anke Huckauf and Mario Urbina. 2007. Gazing with pEYE: New concepts in eye typing. In Proceedings of the 4th Symposium on Applied Perception in Graphics and Visualization (APGV '07). ACM, New York, 141--141. DOI: http://dx.doi.org/10.1145/1272582.1272618",
      "doi": "10.1145/1272582.1272618"
    },
    {
      "text": "Josh Kaufman. 2015. Google 10000 english. (2015).",
      "doi": ""
    },
    {
      "text": "Per Ola Kristensson and Keith Vertanen. 2012. The potential of dwell-free eye-typing for fast assistive gaze communication. In Proceedings of the ACM Symposium on Eye Tracking Research and Applications (ETRA '12). ACM, New York, 241--244.",
      "doi": "10.1145/2168556.2168605"
    },
    {
      "text": "Per-Ola Kristensson and Shumin Zhai. 2004. SHARK 2: a large vocabulary shorthand writing system for pen-based computers. In Proceedings of the ACM Symposium on User Interface Software and Technology (UIST '04). ACM, New York, 43--52.",
      "doi": "10.1145/1029632.1029640"
    },
    {
      "text": "Chandan Kumar, Daniyal Akbari, Raphael Menges, Scott MacKenzie, and Steffen Staab. 2019. TouchGazePath: Multimodal interaction with touch and gaze path for secure yet efficient PIN entry. In 2019 International Conference on Multimodal Interaction (ICMI '19). ACM, New York, 329--338. DOI: http://dx.doi.org/10.1145/3340555.3353734",
      "doi": "10.1145/3340555.3353734"
    },
    {
      "text": "Chandan Kumar, Raphael Menges, and Steffen Staab. 2016. Eye-controlled interfaces for multimedia interaction. IEEE MultiMedia 23, 4 (Oct 2016), 6--13. DOI: http://dx.doi.org/10.1109/MMUL.2016.52",
      "doi": "10.1109/MMUL.2016.52"
    },
    {
      "text": "Manu Kumar. 2007. USER INTERFACE DESIGN. May (2007).",
      "doi": ""
    },
    {
      "text": "Manu Kumar, Andreas Paepcke, Terry Winograd, and Terry Winograd. 2007. EyePoint: practical pointing and selection using gaze and keyboard. In Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems (CHI '07). ACM, New York, 421--430.",
      "doi": "10.1145/1240624.1240692"
    },
    {
      "text": "Andrew Kurauchi, Wenxin Feng, Ajjen Joshi, Carlos Morimoto, and Margrit Betke. 2016. EyeSwipe: Dwell-free text entry using gaze paths. In Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems (CHI '16). ACM, New York, 1952--1956. DOI: http://dx.doi.org/10.1145/2858036.2858335",
      "doi": "10.1145/2858036.2858335"
    },
    {
      "text": "I Scott MacKenzie and R William Soukoreff. 2003. Phrase sets for evaluating text entry techniques. In Extended Abstracts of the ACM SIGCHI Conference on Human Factors in Computing Systems (CHI '03). ACM, New York, 754--755.",
      "doi": "10.1145/765891.765971"
    },
    {
      "text": "P\u00e4ivi Majaranta. 2012. Communication and text entry by gaze. In Gaze interaction and applications of eye tracking: Advances in assistive technologies. IGI Global, 63--77.",
      "doi": ""
    },
    {
      "text": "P\u00e4ivi Majaranta, Ulla-Kaija Ahola, and Oleg ?pakov. 2009. Fast gaze typing with an adjustable dwell time. In Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems (CHI '09). ACM, New York, 357--360. DOI: http://dx.doi.org/10.1145/1518701.1518758",
      "doi": "10.1145/1518701.1518758"
    },
    {
      "text": "Yogesh Kumar Meena, Hubert Cecotti, K Wong-Lin, and Girijesh Prasad. 2016. A novel multimodal gaze-controlled hindi virtual keyboard for disabled users. In 2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC '16). IEEE, New York, 3688--3693.",
      "doi": "10.1109/SMC.2016.7844807"
    },
    {
      "text": "Raphael Menges, Chandan Kumar, and Steffen Staab. 2019. Improving user experience of eye tracking-based interaction: Introspecting and adapting interfaces. ACM Transactions on Computer-Human Interaction 26, 6, Article 37 (Nov. 2019), 46 pages. DOI: http://dx.doi.org/10.1145/3338844",
      "doi": "10.1145/3338844"
    },
    {
      "text": "Carlos H. Morimoto and Arnon Amir. 2010. Context Switching for Fast Key Selection in Text Entry Applications. In Proceedings of the 2010 Symposium on Eye-Tracking Research Applications (ETRA '10). Association for Computing Machinery, New York, NY, USA, 271--274. DOI: http://dx.doi.org/10.1145/1743666.1743730",
      "doi": ""
    },
    {
      "text": "Carlos H. Morimoto, Jose A. T. Leyva, and Antonio Diaz-Tula. 2018. Context switching eye typing using dynamic expanding targets. In Proceedings of the Workshop on Communication by Gaze Interaction (COGAIN '18). ACM, New York, Article 6, 9 pages. DOI: http://dx.doi.org/10.1145/3206343.3206347",
      "doi": "10.1145/3206343.3206347"
    },
    {
      "text": "Martez E Mott, Shane Williams, Jacob O Wobbrock, and Meredith Ringel Morris. 2017. Improving dwell-based gaze typing with dynamic, cascading dwell times. In Proceedings of the ACM CHI Conference on Human Factors in Computing Systems (CHI '17). ACM, New York, 2558--2570.",
      "doi": "10.1145/3025453.3025517"
    },
    {
      "text": "Diogo Pedrosa, Maria da Gra\u00e7a Pimentel, and Khai N. Truong. 2015. Filteryedping: A dwell-free eye typing technique. In Extended Abstracts of the ACM SIGCHI Conference on Human Factors in Computing Systems (CHI '15). ACM, New York, 303--306. DOI: http://dx.doi.org/10.1145/2702613.2725458",
      "doi": ""
    },
    {
      "text": "Thies Pfeiffer. 2018. Gaze-based assistive technologies. In Smart Technologies: Breakthroughs in Research and Practice. IGI Global, 44--66.",
      "doi": ""
    },
    {
      "text": "Ken Pfeuffer, Jason Alexander, and Hans Gellersen. 2015. Gaze + touch vs. touch: What's the trade-off when using gaze to extend touch to remote displays?. In Proceedings of the IFIP Conference on Human-Computer Interaction (INTERACT '15). Springer, Berlin, 349--367. DOI: http://dx.doi.org/10.1007/978--3--319--22668--2_27",
      "doi": "10.1007/978-3-319-22668-2_27"
    },
    {
      "text": "Ken Pfeuffer, Jason Alexander, and Hans Gellersen. 2016. Partially-indirect bimanual input with gaze, pen, and touch for pan, zoom, and ink interaction. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI '16). ACM, New York, 2845--2856. DOI: http://dx.doi.org/10.1145/2858036.2858201",
      "doi": "10.1145/2858036.2858201"
    },
    {
      "text": "Ken Pfeuffer and Hans Gellersen. 2016. Gaze and touch interaction on tablets. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST '16). ACM, New York, 301--311. DOI: http://dx.doi.org/10.1145/2984511.2984514",
      "doi": "10.1145/2984511.2984514"
    },
    {
      "text": "Ondrej Polacek, Adam J Sporka, and Pavel Slavik. 2017. Text input for motor-impaired people. Universal Access in the Information Society 16, 1 (2017), 51--72.",
      "doi": "10.1007/s10209-015-0433-0"
    },
    {
      "text": "Alex Poole and Linden J Ball. 2006. Eye tracking in HCI and usability research. In Encyclopedia of human computer interaction. IGI Global, 211--219.",
      "doi": ""
    },
    {
      "text": "Amy Roman. 2013. Maintain ability to type, swipe point with hand weakness in ALS. (Nov 2013).",
      "doi": ""
    },
    {
      "text": "Daniel Rough, Keith Vertanen, and Per Ola Kristensson. 2014. An evaluation of Dasher with a high-performance language model as a gaze communication method. In Proceedings of the 2014 International Working Conference on Advanced Visual Interfaces. ACM, New York, 169--176.",
      "doi": "10.1145/2598153.2598157"
    },
    {
      "text": "H. Sakoe and S. Chiba. 1978. Dynamic programming algorithm optimization for spoken word recognition. IEEE Transactions on Acoustics, Speech, and Signal Processing 26, 1 (February 1978), 43--49. DOI: http://dx.doi.org/10.1109/TASSP.1978.1163055",
      "doi": ""
    },
    {
      "text": "Sayan Sarcar, Prateek Panwar, and Tuhin Chakraborty. 2013. EyeK: An efficient dwell-free eye gaze-based text entry system. In Proceedings of the 11th Asia Pacific Conference on Computer-Human Interaction. ACM, New York, 215--220.",
      "doi": "10.1145/2525194.2525288"
    },
    {
      "text": "Korok Sengupta, Raphael Menges, Chandan Kumar, and Steffen Staab. 2017. GazeTheKey: Interactive keys to integrate word predictions for gaze-based text entry. In IUI Companion, George A. Papadopoulos, Tsvi Kuflik, Fang Chen, Carlos Duarte, and Wai-Tat Fu (Eds.). ACM, New York, 121--124. http://dblp.uni-trier.de/db/conf/ iui/iui2017c.html#SenguptaMKS17",
      "doi": "10.1145/3030024.3038259"
    },
    {
      "text": "Korok Sengupta, Raphael Menges, Chandan Kumar, and Steffen Staab. 2019. Impact of variable positioning of text prediction in gaze-based text entry. In Proceedings of the 11th ACM Symposium on Eye Tracking Research & Applications (ETRA '19). ACM, New York, Article 74, 9 pages. DOI: http://dx.doi.org/10.1145/3317956.3318152",
      "doi": "10.1145/3317956.3318152"
    },
    {
      "text": "R. William Soukoreff and I. Scott MacKenzie. 2003. Metrics for text entry research: An evaluation of MSD and KSPC, and a new unified error Mmtric. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '03). ACM, New York, 113--120. DOI: http://dx.doi.org/10.1145/642611.642632",
      "doi": "10.1145/642611.642632"
    },
    {
      "text": "Keith Trnka, John McCaw, Debra Yarrington, Kathleen F McCoy, and Christopher Pennington. 2009. User interaction with word prediction: The effects of prediction quality. ACM Transactions on Accessible Computing (TACCESS) 1, 3 (2009), 17.",
      "doi": ""
    },
    {
      "text": "Outi Tuisku, P\u00e4ivi Majaranta, Poika Isokoski, and Kari-Jouko R\u00e4ih\u00e4. 2008. Now Dasher! Dash away!: longitudinal study of fast text entry by eye gaze. In Proceedings of the 2008 Symposium on Eye Tracking Research & Applications (ETRA '08). ACM, New York, 19--26.",
      "doi": "10.1145/1344471.1344476"
    },
    {
      "text": "Mario H Urbina and Anke Huckauf. 2010. Alternatives to single character entry and dwell time selection on eye typing. In Proceedings of the 2010 Symposium on Eye-Tracking Research Applications (ETRA '10). Association for Computing Machinery, New York, 315--322. DOI: http://dx.doi.org/10.1145/1743666.1743738",
      "doi": "10.1145/1743666.1743738"
    },
    {
      "text": "Keith Vertanen and David J C MacKay. 2010. Speech dasher: Fast writing using speech and gaze. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, New York, 595--598.",
      "doi": "10.1145/1753326.1753415"
    },
    {
      "text": "Roel Vertegaal. 2008. A Fitts' law comparison of eye tracking and manual input in the selection of visual targets. In Proceedings of the 10th international conference on Multimodal interfaces. ACM, New York, 241--248.",
      "doi": "10.1145/1452392.1452443"
    },
    {
      "text": "Alex Waibel and Kai-Fu Lee (Eds.). 1990. Readings in Speech Recognition. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA.",
      "doi": ""
    },
    {
      "text": "Jacob O Wobbrock, James Rubinstein, Michael W Sawyer, and Andrew T Duchowski. 2008. Longitudinal evaluation of discrete consecutive gaze gestures for text entry. In Proceedings of the 2008 ACM Symposium on Eye Tracking Research & Applications (ETRA '08). ACM, New York, 11--18.",
      "doi": "10.1145/1344471.1344475"
    },
    {
      "text": "Shumin Zhai and Per Ola Kristensson. 2012. The word-gesture keyboard: Reimagining keyboard interaction. Commun. ACM 55, 9 (2012), 91--101.",
      "doi": "10.1145/2330667.2330689"
    }
  ]
}