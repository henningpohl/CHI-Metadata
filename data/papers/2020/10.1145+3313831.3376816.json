{
  "doi": "10.1145/3313831.3376816",
  "title": "Non-Verbal Auditory Input for Controlling Binary, Discrete, and Continuous Input in Automotive User Interfaces",
  "published": "2020-04-23",
  "proctitle": "CHI '20: Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems",
  "pages": "1-13",
  "year": 2020,
  "badges": [],
  "abstract": "Using auditory input while driving is becoming increasingly popular for making distraction-free inputs while driving. However, we argue that auditory input is more than just using speech. Thus, in this work, we explore using Non-Verbal Auditory Input (NVAI) for interacting with smart assistants while driving. Through an online study with 100 participants, we initially investigated users' input preferences for binary, discrete, and continuous data types. After identifying the top three modalities for NVAI, we subsequently conducted an in-person study with 16 participants. In our study, the participants tested these input modalities for three different input data types regarding their accuracy, driver-distraction, and social acceptability, while operating a driving simulator. The results reveal that, although clapping hands for making input was initially preferred in our online survey, it is snapping fingers for binary input and discrete input and humming for making continuous input that is the preferred NVAI modality while driving.",
  "tags": [
    "non-verbal auditory interaction",
    "automotive user interfaces",
    "voice-user interface",
    "speech input"
  ],
  "authors": [
    {
      "name": "Markus Funk",
      "institution": "Cerence GmbH, Ulm, Germany",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "83458955757",
      "orcid": "missing"
    },
    {
      "name": "Vanessa Tobisch",
      "institution": "Cerence GmbH, Ulm, Germany",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "87259551557",
      "orcid": "missing"
    },
    {
      "name": "Adam Emfield",
      "institution": "Cerence Inc., Burlington, MA, USA",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81488669830",
      "orcid": "missing"
    }
  ],
  "references": [
    {
      "text": "S Al-Hashimi. 2006. Blowtter: A voice-controlled plotter. In Proceedings of HCI. 41--44.",
      "doi": ""
    },
    {
      "text": "Sama'a Al Hashimi. 2009. Vocal Telekinesis: towards the development of voice-physical installations. Universal Access in the Information Society 8 (2009), 65--75. DOI: http://dx.doi.org/10.1007/s10209-008-0135-y",
      "doi": "10.1007/s10209-008-0135-y"
    },
    {
      "text": "Sama' Al Hashimi and Gordon Davies. 2006. Vocal telekinesis: physical control of inanimate objects with minimal paralinguistic voice input. In Proceedings of the 14th ACM international conference on Multimedia. ACM, 813--814. DOI: http://dx.doi.org/10.1145/1180639.1180822",
      "doi": "10.1145/1180639.1180822"
    },
    {
      "text": "Ignacio Alvarez, Aqueasha Martin, Jerone Dunbar, Joachim Taiber, Dale-Marie Wilson, and Juan E Gilbert. 2011. Designing driver-centric natural voice user interfaces. Adj. Proc. AutomotiveUI 11 (2011), 42--49.",
      "doi": ""
    },
    {
      "text": "Jerome R Bellegarda. 2014. Spoken language understanding for natural interaction: The siri experience. In Natural Interaction with Robots, Knowbots and Smartphones. Springer, 3--14. DOI: http://dx.doi.org/10.1007/978--1--4614--8280--2_1",
      "doi": ""
    },
    {
      "text": "Jeff A Bilmes, Xiao Li, Jonathan Malkin, Kelley Kilanski, Richard Wright, Katrin Kirchhoff, Amarnag Subramanya, Susumu Harada, James A Landay, Patricia Dowden, and others. 2005. The Vocal Joystick: A voice-based human-computer interface for individuals with motor impairments. In Proceedings of the conference on human language technology and empirical methods in natural language processing. Association for Computational Linguistics, 995--1002. DOI: http://dx.doi.org/10.3115/1220575.1220700",
      "doi": "10.3115/1220575.1220700"
    },
    {
      "text": "Michael Braun, Nora Broy, Bastian Pfleging, and Florian Alt. 2017. A design space for conversational in-vehicle information systems. In Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services. ACM, 79. DOI: http://dx.doi.org/10.1145/3098279.3122122",
      "doi": "10.1145/3098279.3122122"
    },
    {
      "text": "Michael Braun, Nora Broy, Bastian Pfleging, and Florian Alt. 2019a. Visualizing natural language interaction for conversational in-vehicle information systems to minimize driver distraction. Journal on Multimodal User Interfaces 13, 2 (2019), 71--88. DOI: http://dx.doi.org/10.1007/s12193-019-00301--2",
      "doi": ""
    },
    {
      "text": "Michael Braun, Anja Mainz, Ronee Chadowitz, Bastian Pfleging, and Florian Alt. 2019b. At your service: Designing voice assistant personalities to improve automotive user interfaces. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. ACM, 40. DOI: http://dx.doi.org/10.1145/3290605.3300270",
      "doi": "10.1145/3290605.3300270"
    },
    {
      "text": "William Buxton, William Gaver, and Sara Bly. 1994. Auditory interfaces: The use of non-speech audio at the interface. Unpublished manuscript (1994).",
      "doi": ""
    },
    {
      "text": "Liwei Dai, Rich Goldman, Andrew Sears, and Jeremy Lozier. 2004. Speech-based cursor control: a study of grid-based solutions. In ACM SIGACCESS Accessibility and Computing. ACM, 94--101. DOI: http://dx.doi.org/10.1145/1029014.1028648",
      "doi": ""
    },
    {
      "text": "C De Mauro, M Gori, M Maggini, and E Martinelli. 2001. Easy access to graphical interfaces by voice mouse. Technical Report. Technical report, Universit\u00e0 di Siena. Available from the author at: maggini \u00e2 ? A? e.",
      "doi": ""
    },
    {
      "text": "Tilman Dingler, Jeffrey Lindsay, and Bruce N Walker. 2008. Learnabiltiy of sound cues for environmental features: Auditory icons, earcons, spearcons, and speech. International Community for Auditory Display.",
      "doi": ""
    },
    {
      "text": "David Fleer and Christian Leichsenring. 2012. MISO: a context-sensitive multimodal interface for smart objects based on hand gestures and finger snaps. In Adjunct proceedings of the 25th annual ACM symposium on User interface software and technology. ACM, 93--94. DOI: http://dx.doi.org/10.1145/2380296.2380338",
      "doi": "10.1145/2380296.2380338"
    },
    {
      "text": "G. Geiser. 1985. Man Machine Interaction in Vehicles. ATZ 87 (1985), 74--77.",
      "doi": ""
    },
    {
      "text": "Linn Hackenberg, Sara Bongartz, Christian H\u00e4rtle, Paul Leiber, Thorb Baumgarten, and Jo Ann Sison. 2013. International evaluation of NLU benefits in the domain of in-vehicle speech dialog systems. In Proceedings of the 5th International Conference on Automotive User Interfaces and Interactive Vehicular Applications. ACM, 114--120. DOI: http://dx.doi.org/10.1145/2516540.2516553",
      "doi": "10.1145/2516540.2516553"
    },
    {
      "text": "Perttu H\u00e4m\u00e4l\u00e4inen, Teemu M\u00e4ki-Patola, Ville Pulkki, and Matti Airas. 2004. Musical computer games played by singing. In Proc. 7th Int. Conf. on Digital Audio Effects, Naples.",
      "doi": ""
    },
    {
      "text": "Susumu Harada, James A Landay, Jonathan Malkin, Xiao Li, and Jeff A Bilmes. 2006. The vocal joystick:: evaluation of voice-based cursor control techniques. In Proceedings of the 8th international ACM SIGACCESS conference on Computers and accessibility. ACM, 197--204. DOI: http://dx.doi.org/10.1145/1168987.1169021",
      "doi": "10.1145/1168987.1169021"
    },
    {
      "text": "Susumu Harada, Jacob O Wobbrock, and James A Landay. 2007. Voicedraw: a hands-free voice-driven drawing application for people with motor impairments. In Proceedings of the 9th international ACM SIGACCESS conference on Computers and accessibility. ACM, 27--34. DOI: http://dx.doi.org/10.1145/1296843.1296850",
      "doi": "10.1145/1296843.1296850"
    },
    {
      "text": "Sandra G Hart. 2006. NASA-task load index (NASA-TLX); 20 years later. In Proceedings of the human factors and ergonomics society annual meeting, Vol. 50. Sage publications Sage CA: Los Angeles, CA, 904--908. DOI: http://dx.doi.org/10.1177/154193120605000909",
      "doi": ""
    },
    {
      "text": "Paul Heisterkamp. 2001. Linguatronic product-level speech system for Mercedes-Benz cars. In Proceedings of the first international conference on Human language technology research. Association for Computational Linguistics, 1--2. DOI: http://dx.doi.org/10.3115/1072133.1072199",
      "doi": "10.3115/1072133.1072199"
    },
    {
      "text": "Brandi House, Jonathan Malkin, and Jeff Bilmes. 2009. The VoiceBot: a voice controlled robot arm. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 183--192. DOI: http://dx.doi.org/10.1145/1518701.1518731",
      "doi": "10.1145/1518701.1518731"
    },
    {
      "text": "Zhang Hua and Wei Lieh Ng. 2010. Speech recognition interface design for in-vehicle system. In Proceedings of the 2nd international conference on automotive user interfaces and interactive vehicular applications. ACM, 29--33. DOI: http://dx.doi.org/10.1145/1969773.1969780",
      "doi": "10.1145/1969773.1969780"
    },
    {
      "text": "Takeo Igarashi and John F Hughes. 2001. Voice as sound: using non-verbal voice input for interactive control. In Proceedings of the 14th annual ACM symposium on User interface software and technology. ACM, 155--156. DOI: http://dx.doi.org/10.1145/502348.502372",
      "doi": "10.1145/502348.502372"
    },
    {
      "text": "SAE International. 2014. Automated driving: levels of driving automation are defined in new SAE international standard J3016. (2014).",
      "doi": ""
    },
    {
      "text": "Mads Gregers J\u00e6ger, Mikael B Skov, Nils Gram Thomassen, and others. 2008. You can touch, but you can't look: interacting with in-vehicle systems. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 1139--1148. DOI: http://dx.doi.org/10.1145/1357054.1357233",
      "doi": ""
    },
    {
      "text": "Antti Jylh\u00e4 and Cumhur Erkut. 2009. A hand clap interface for sonic interaction with the computer. In CHI'09 Extended Abstracts on Human Factors in Computing Systems. ACM, 3175--3180. DOI: http://dx.doi.org/10.1145/1520340.1520452",
      "doi": "10.1145/1520340.1520452"
    },
    {
      "text": "Dagmar Kern and Albrecht Schmidt. 2009. Design space for driver-based automotive user interfaces. In Proceedings of the 1st International Conference on Automotive User Interfaces and Interactive Vehicular Applications. ACM, 3--10. DOI: http://dx.doi.org/10.1145/1620509.1620511",
      "doi": "10.1145/1620509.1620511"
    },
    {
      "text": "Georgios K Kountouriotis, Richard M Wilkie, Peter H Gardner, and Natasha Merat. 2015. Looking and thinking when driving: The impact of gaze and cognitive load on steering. Transportation research part F: traffic psychology and behaviour 34 (2015), 108--121. DOI: http://dx.doi.org/10.1016/j.trf.2015.07.012",
      "doi": ""
    },
    {
      "text": "Andrew L Kun, Tim Paek, ?eljko Medenica, Nemanja Memarovi\u00b4 c, and Oskar Palinko. 2009. Glancing at personal navigation devices can affect driving: experimental results and design implications. In Proceedings of the 1st International Conference on Automotive User Interfaces and Interactive Vehicular Applications. ACM, 129--136. DOI: http://dx.doi.org/10.1145/1620509.1620534",
      "doi": "10.1145/1620509.1620534"
    },
    {
      "text": "David R Large, Leigh Clark, Annie Quandt, Gary Burnett, and Lee Skrypchuk. 2017. Steering the conversation: a linguistic exploration of natural language interactions with a digital assistant during simulated driving. Applied ergonomics 63 (2017), 53--61. DOI: http://dx.doi.org/10.1016/j.apergo.2017.04.003",
      "doi": ""
    },
    {
      "text": "Victor Ei-Wen Lo and Paul A Green. 2013. Development and evaluation of automotive speech interfaces: useful information from the human factors and the related literature. International Journal of Vehicular Technology 2013 (2013). DOI: http://dx.doi.org/10.1155/2013/924170",
      "doi": ""
    },
    {
      "text": "Jannette Maciej and Mark Vollrath. 2009. Comparison of manual vs. speech-based interaction with in-vehicle information systems. Accident Analysis & Prevention 41, 5 (2009), 924--930. DOI: http://dx.doi.org/10.1016/j.aap.2009.05.007",
      "doi": ""
    },
    {
      "text": "Angela Mahr, Michael Feld, Mohammad Mehdi Moniri, and Rafael Math. 2012. The ConTRe (Continuous Tracking and Reaction) task: A flexible approach for assessing driver cognitive workload with high sensitivity. In Adjunct Proceedings of the 4th International Conference on Automotive User Interfaces and Interactive Vehicular Applications. 88--91.",
      "doi": ""
    },
    {
      "text": "Rafael Math, Angela Mahr, Mohammad M Moniri, and Christian M\u00fcller. 2012. OpenDS: A new open-source driving simulator for research. In Proceedings of the International Conference on Automotive User Interfaces and Interactive Vehicular Applications, Adjunct Proceedings. 7--8.",
      "doi": ""
    },
    {
      "text": "Yoshiyuki Mihara, Etsuya Shibayama, and Shin Takahashi. 2005. The migratory cursor: accurate speech-based cursor movement by moving multiple ghost cursors using non-verbal vocalizations. In Proceedings of the 7th international ACM SIGACCESS conference on Computers and accessibility. ACM, 76--83. DOI: http://dx.doi.org/10.1145/1090785.1090801",
      "doi": "10.1145/1090785.1090801"
    },
    {
      "text": "Tomoyasu Nakano, Masataka Goto, Jun Ogata, and Yuzuru Hiraga. 2005. Voice drummer: A music notation interface of drum sounds using voice percussion input. In Proceedings of the 18th Annual ACM Symposium on User Interface Software and Technology (UIST2005). 49--50.",
      "doi": ""
    },
    {
      "text": "Michael A Nees and Bruce N Walker. 2011. Auditory displays for in-vehicle technologies. Reviews of human factors and ergonomics 7, 1 (2011), 58--99. DOI: http://dx.doi.org/10.1177/1557234X11410396",
      "doi": ""
    },
    {
      "text": "Robert Ne\u00dfelrath, Mohammad Mehdi Moniri, and Michael Feld. 2016. Combining speech, gaze, and micro-gestures for the multimodal control of in-car functions. In 2016 12th International Conference on Intelligent Environments (IE). IEEE, 190--193. DOI: http://dx.doi.org/10.1109/IE.2016.42",
      "doi": ""
    },
    {
      "text": "Bastian Pfleging, Stefan Schneegass, and Albrecht Schmidt. 2012. Multimodal interaction in the car: combining speech and gestures on the steering wheel. In Proceedings of the 4th International Conference on Automotive User Interfaces and Interactive Vehicular Applications. ACM, 155--162. DOI: http://dx.doi.org/10.1145/2390256.2390282",
      "doi": "10.1145/2390256.2390282"
    },
    {
      "text": "Ondrej Pol\u00e1 cek, Zden? ek M\u00edkovec, Adam J Sporka, and Pavel Slav\u00edk. 2010. New way of vocal interface design: Formal description of non-verbal vocal gestures. Proceedings of the CWUAAT (2010), 137--144.",
      "doi": ""
    },
    {
      "text": "Martin Porcheron, Joel E Fischer, and Sarah Sharples. 2017. Do animals have accents?: talking with agents in multi-party conversation. In Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing. ACM, 207--219.",
      "doi": "10.1145/2998181.2998298"
    },
    {
      "text": "Sven Reichel, Jasmin Sohn, Ute Ehrlich, Andr\u00e9 Berton, and Michael Weber. 2014. Out-of-domain spoken dialogs in the car: a WoZ study. In Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL). 12--21.",
      "doi": ""
    },
    {
      "text": "Ju-Hwan Seo, Jeong-Yean Yang, Jaewoo Kim, and Dong-Soo Kwon. 2013. Autonomous humanoid robot dance generation system based on real-time music input. In 2013 IEEE RO-MAN. IEEE, 204--209.",
      "doi": ""
    },
    {
      "text": "Joren Six, Olmo Cornelis, and Marc Leman. 2014. TarsosDSP, a Real-Time Audio Processing Framework in Java. In Proceedings of the 53rd AES Conference (AES 53rd).",
      "doi": ""
    },
    {
      "text": "AJ Sporka, SH Kurniawan, and P Slav\u00edk. 2006a. Non-speech operated emulation of keyboard. In Designing Accessible Technology. Springer, 145--154. DOI: http://dx.doi.org/10.1007/1--84628--365--5_15",
      "doi": ""
    },
    {
      "text": "Adam J Sporka, Torsten Felzer, Sri H Kurniawan, Ondrej Pol\u00e1? cek, Paul Haiduk, and I Scott MacKenzie. 2011. CHANTI: predictive text entry using non-verbal vocal input. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 2463--2472. DOI: http://dx.doi.org/10.1145/1978942.1979302",
      "doi": "10.1145/1978942.1979302"
    },
    {
      "text": "Adam J Sporka, Sri H Kurniawan, Murni Mahmud, and Pavel Slav\u00edk. 2006b. Non-speech input and speech recognition for real-time control of computer games. In Proceedings of the 8th international ACM SIGACCESS conference on Computers and accessibility. ACM, 213--220. DOI: http://dx.doi.org/10.1145/1168987.1169023",
      "doi": "10.1145/1168987.1169023"
    },
    {
      "text": "Adam J Sporka, Sri Hastuti Kurniawan, and Pavel Slavik. 2004. Whistling user interface (U 3 I). In ERCIM Workshop on User Interfaces for All. Springer, 472--478. DOI: http://dx.doi.org/10.1007/978--3--540--30111-0_41",
      "doi": ""
    },
    {
      "text": "Carlile R Stevens and Dale E Reamer. 1996. Method and apparatus for activating switches in response to different acoustic signals. (Feb. 20 1996). US Patent 5,493,618.",
      "doi": ""
    },
    {
      "text": "Jane C Stutts, Donald W Reinfurt, Loren Staplin, Eric Rodgman, and others. 2001. The role of driver distraction in traffic crashes. (2001).",
      "doi": ""
    },
    {
      "text": "Marcus Tonnis, Verena Broy, and Gudrun Klinker. 2006. A survey of challenges related to the design of 3d user interfaces for car drivers. In 3D User Interfaces (3DUI'06). IEEE, 127--134. DOI: http://dx.doi.org/10.1109/VR.2006.19",
      "doi": ""
    },
    {
      "text": "Omer Tsimhoni, Daniel Smith, and Paul Green. 2004. Address entry while driving: Speech recognition versus a touch-screen keyboard. Human factors 46, 4 (2004), 600--610. DOI: http://dx.doi.org/10.1518/hfes.46.4.600.56813",
      "doi": ""
    },
    {
      "text": "Sampo Vesa and Tapio Lokki. 2005. An eyes-free user interface controlled by finger snaps. In Proceedings of the 8th International Conference on Digital Audio Effects (DAFx-05). 262--265.",
      "doi": ""
    },
    {
      "text": "Roman Vilimek and Thomas Hempel. 2005. Effects of speech and non-speech sounds on short-term memory and possible implications for in-vehicle use. In In Proceedings of the International Conference on Auditory Display (ICAD).",
      "doi": ""
    },
    {
      "text": "Nigel Ward. 2006. Non-lexical conversational sounds in American English. Pragmatics & Cognition 14, 1 (2006), 129--182. DOI: http://dx.doi.org/10.1075/pc.14.1.08war",
      "doi": ""
    },
    {
      "text": "Richard Watts and Peter Robinson. 1999. Controlling computers by whistling. In Proceedings of Eurographics UK. Cambridge University Press.",
      "doi": ""
    }
  ]
}