{
  "doi": "10.1145/3313831.3376514",
  "title": "Music Creation by Example",
  "published": "2020-04-23",
  "proctitle": "CHI '20: Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems",
  "pages": "1-13",
  "year": 2020,
  "badges": [],
  "abstract": "Short online videos have become the dominating media on social platforms. However, finding suitable music to accompany videos can be a challenging task to some video creators, due to copyright constraints, limitations in search engines, and required audio-editing expertise. One possible solution to these problems is to use AI music generation. In this paper we present a user interface (UI) paradigm that allows users to input a song to an AI engine and then interactively regenerate and mix AI-generated music. To arrive at this design, we conducted user studies with a total of 104 video creators at several stages of our design and development process. User studies supported the effectiveness of our approach and provided valuable insights about human-AI interaction as well as the design and evaluation of mixed-initiative interfaces in creative practice.",
  "tags": [
    "mixed-initiative interaction",
    "artificial intelligence",
    "algorithmic composition",
    "music generation"
  ],
  "authors": [
    {
      "name": "Emma Frid",
      "institution": "KTH Royal Institute of Technology, Stockholm, Sweden",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659365116",
      "orcid": "0000-0002-4422-5223"
    },
    {
      "name": "Celso Gomes",
      "institution": "Adobe Research, Seattle, WA, USA",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659366606",
      "orcid": "missing"
    },
    {
      "name": "Zeyu Jin",
      "institution": "Adobe Research, San Francisco, CA, USA",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659527598",
      "orcid": "missing"
    }
  ],
  "references": [
    {
      "text": "Adobe. 2019. Adobe Audition. (2019). https://www.adobe.com/se/products/audition.html.",
      "doi": ""
    },
    {
      "text": "Kat Agres, Jamie Forth, and Geraint A Wiggins. 2016. Evaluation of Musical Creativity and Musical Metacreation Systems. Computers in Entertainment (CIE) 14, 3 (2016), 3.",
      "doi": ""
    },
    {
      "text": "Google Research / AI. 2019. Magenta. (2019). https://ai.google/research/teams/brain/magenta/.",
      "doi": ""
    },
    {
      "text": "JE Allen, Curry I Guinn, and Eric Horivtz. 1999a. Mixed-Initiative Interaction. IEEE Intelligent Systems and their Applications 14, 5 (1999), 14--23.",
      "doi": ""
    },
    {
      "text": "JE Allen, Curry I Guinn, and E Horvtz. 1999b. Mixed-Initiative Interaction. IEEE Intelligent Systems and their Applications 14, 5 (1999), 14--23.",
      "doi": ""
    },
    {
      "text": "Giuseppe Amato, Malte Behrmann, Fr\u00e9d\u00e9ric Bimbot, Baptiste Caramiaux, Fabrizio Falchi, Ander Garcia, Joost Geurts, Jaume Gibert, Guillaume Gravier, Hadmut Holken, and others. 2019. AI in the Media and Creative Industries. (2019).",
      "doi": ""
    },
    {
      "text": "Vanessa Theme Ament. 2014. The Foley Grail: The Art of Performing Sound for Film, Games, and Animation. Routledge, Burlingto, MA, USA.",
      "doi": ""
    },
    {
      "text": "Saleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira Nushi, Penny Collisson, Jina Suh, Shamsi Iqbal, Paul N Bennett, Kori Inkpen, and others. 2019. Guidelines for Human-AI Interaction. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. ACM, ACM, New York, NY, USA, 3.",
      "doi": "10.1145/3290605.3300233"
    },
    {
      "text": "Christopher Ariza. 2009. The Interrogator as Critic: The Turing Test and the Evaluation of Generative Music Systems. Computer Music Journal 33, 2 (2009), 48--70.",
      "doi": "10.1162/comj.2009.33.2.48"
    },
    {
      "text": "G\u00e9rard Assayag, Camilo Rueda, Mikael Laurson, Carlos Agon, and Olivier Delerue. 1999. Computer-Assisted Composition at IRCAM: From PatchWork to OpenMusic. Computer Music Journal 23, 3 (1999), 59--72.",
      "doi": "10.1162/014892699559896"
    },
    {
      "text": "Francisco Bernardo, Michael Zbyszynski, Rebecca Fiebrink, and Mick Grierson. 2017. Interactive Machine Learning for End-User Innovation. In 2017 AAAI Spring Symposium Series. The AAAI Press, Palo Alto, CA, USA, 369--375.",
      "doi": ""
    },
    {
      "text": "J Bharucha. 1988. Neural Net Modeling of Music. In Proceedings of the First Workshop on Artificial Intelligence and Music. American Association for Artifical Intelligence, Menlo Park, CA, USA, 173--182.",
      "doi": ""
    },
    {
      "text": "Margaret A. Boden. 1998a. Creativity and Artificial Intelligence. Artificial Intelligence 103, 1 (1998), 347 -- 356. http://www.sciencedirect.com/science/article/pii/S0004370298000551 Artificial Intelligence 40 Years Later.",
      "doi": "10.1016/S0004-3702%2898%2900055-1"
    },
    {
      "text": "Margaret A Boden. 1998b. Creativity and Artificial Intelligence. Artificial Intelligence 103, 1--2 (1998), 347--356.",
      "doi": "10.1016/S0004-3702%2898%2900055-1"
    },
    {
      "text": "Jean-Pierre Briot, Ga\u00ebtan Hadjeres, and Francc ois Pachet. 2017. Deep Learning Techniques for Music Generation - A Survey. (2017).",
      "doi": ""
    },
    {
      "text": "Kris Bryden. 2007. Developing Metrics for Evolving a Musically Satisfying Result from a Population of Computer Generated Music Compositions. In 2007 IEEE Symposium on Computational Intelligence in Image and Signal Processing. IEEE, Honolulu, HI, USA, 171--176.",
      "doi": ""
    },
    {
      "text": "Cognitive Catalyst. 2019. Watson Beat. (2019). https://github.com/cognitive-catalyst/watson-beat.",
      "doi": ""
    },
    {
      "text": "Elizabeth Charters. 2003. The Use of Think-Aloud Methods in Qualitative Research - An Introduction to Think-Aloud Methods. Brock Education: A Journal of Educational Research and Practice 12, 2 (2003), 68--82.",
      "doi": ""
    },
    {
      "text": "Simon Colton, Ramon L\u00f3pez de M\u00e1ntaras, and Oliviero Stock. 2009. Computational Creativity: Coming of Age. AI Magazine 30, 3 (2009), 11--11.",
      "doi": ""
    },
    {
      "text": "David Cope. 1989. Experiments in Musical Intelligence (EMI): Non-Linear Linguistic-Based Composition. Journal of New Music Research 18, 1--2 (1989), 117--139.",
      "doi": ""
    },
    {
      "text": "David Cope. 2000. The Algorithmic Composer. Vol. 16. AR Editions, Inc., Madison, Wisconsin, USA.",
      "doi": ""
    },
    {
      "text": "Boomy Corporation. 2019. Make Instant Music with Artificial Intelligence. (2019). https://boomy.com/.",
      "doi": ""
    },
    {
      "text": "Cycling'74. 2019. Cycling'74 - Tools For Sound, Graphics and Interactivity. (2019). https://cycling74.com/.",
      "doi": ""
    },
    {
      "text": "Ramon Lopez De Mantaras and Josep Lluis Arcos. 2002. AI and Music: From Composition to Expressive Performance. AI magazine 23, 3 (2002), 43--43.",
      "doi": ""
    },
    {
      "text": "Steven P Dow, Alana Glassco, Jonathan Kass, Melissa Schwarz, Daniel L Schwartz, and Scott R Klemmer. 2010. Parallel Prototyping Leads to Better Design Results, More Divergence, and Increased Self-Efficacy. ACM Transactions on Computer-Human Interaction (TOCHI) 17, 4 (2010), 18.",
      "doi": "10.1145/1879831.1879836"
    },
    {
      "text": "ElectronJS. 2019. ElectronJS - Build Cross Platform Desktop Apps with JavaScript, HTML, and CSS. (2019). https://electronjs.org//.",
      "doi": ""
    },
    {
      "text": "Jose D Fern\u00e1ndez and Francisco Vico. 2013. AI Methods in Algorithmic Composition: A Comprehensive Survey. Journal of Artificial Intelligence Research 48 (2013), 513--582.",
      "doi": "10.5555/2591248.2591260"
    },
    {
      "text": "Rebecca Fiebrink, Baptiste Caramiaux, R Dean, and A McLean. 2018. The Machine Learning Algorithm as Creative Musical Tool. Oxford University Press, New York, NY, USA, Chapter 12, 181--208.",
      "doi": ""
    },
    {
      "text": "Filmstro. 2019. Filmstro - Music That Moves. (2019). https://filmstro.com/.",
      "doi": ""
    },
    {
      "text": "Dorien Herremans and Elaine Chew. 2017. MorpheuS: Generating Structured Music with Constrained Patterns and Tension. IEEE Transactions on Affective Computing (2017), 510--523.",
      "doi": ""
    },
    {
      "text": "Hexachords. 2019. Orb Composer. (2019). https://www.orb-composer.com/.",
      "doi": ""
    },
    {
      "text": "IBM. 2019. IBM Watson Beat. (2019). https://www.ibm.com/case-studies/ibm-watson-beat.",
      "doi": ""
    },
    {
      "text": "Muzeek Inc. 2019. Muzeek Pro. (2019). https://www.getmuzeek.com/home.",
      "doi": ""
    },
    {
      "text": "Bruce L. Jacob. 1996. Algorithmic Composition as a Model of Creativity. Organised Sound 1, 3 (1996), 157--165.",
      "doi": "10.1017/S1355771896000222"
    },
    {
      "text": "Lilong Jiang, Protiva Rahman, and Arnab Nandi. 2018. Evaluating Interactive Data Systems: Workloads, Metrics, and Guidelines. In Proceedings of the 2018 International Conference on Management of Data. ACM, ACM, New York, NY, USA, 1637--1644.",
      "doi": "10.1145/3183713.3197386"
    },
    {
      "text": "Simon C Jones and Thomas G Schumacher. 1992. Muzak: On Functional Music and Power. Critical Studies in Media Communication 9, 2 (1992), 156--169.",
      "doi": ""
    },
    {
      "text": "Jukedeck. 2019. Jukedeck. (2019). https://www.jukedeck.com/.",
      "doi": ""
    },
    {
      "text": "SmartSound LCC. 2019. Royalty Free music and Incredible ways to customize it. (2019). https://www.smartsound.com/.",
      "doi": ""
    },
    {
      "text": "Yuli Levtov. 2018. Algorithmic Music for Mass Consumption and Universal Production. In The Oxford Handbook of Algorithmic Music. Oxford University Press, New York, NY, USA.",
      "doi": ""
    },
    {
      "text": "Jen-Chun Lin, Wen-Li Wei, and Hsin-Min Wang. 2015. EMV-Matchmaker: Emotional Temporal course Modeling and Matching for Automatic Music Video Generation. In Proceedings of the 23rd ACM International conference on Multimedia. ACM, ACM, New York, NY, USA, 899--902.",
      "doi": "10.1145/2733373.2806359"
    },
    {
      "text": "Jen-Chun Lin, Wen-Li Wei, and Hsin-Min Wang. 2016. DEMV-matchmaker: Emotional Temporal Course Representation and Deep Similarity Matching for Automatic Music Video Generation. In Proceedings of ICASSP. IEEE, Shanghai, China, 2772--2776.",
      "doi": "10.1109/ICASSP.2016.7472182"
    },
    {
      "text": "Jen-Chun Lin, Wen-Li Wei, James Yang, Hsin-Min Wang, and Hong-Yuan Mark Liao. 2017. Automatic Music Video Generation Based on Simultaneous Soundtrack Recommendation and Video Editing. In Proceedings of the 25th ACM international conference on Multimedia. ACM, ACM, New York, NY, USA, 519--527.",
      "doi": "10.1145/3123266.3123399"
    },
    {
      "text": "Levelo Music Group LLC. 2019a. Soundstripe - Unlimited Music for Video. (2019). https://soundstripe.com/.",
      "doi": ""
    },
    {
      "text": "Marmoset LLC. 2019b. Marmoset Music. (2019). https://www.marmosetmusic.com/.",
      "doi": ""
    },
    {
      "text": "SmartSound LLC. 2019c. Sonicfire Pro 6 - The Fastest Way to Customize Music to Your Videos. (2019). https://www.smartsound.com/sonicfire/.",
      "doi": ""
    },
    {
      "text": "Envato Pty Ltd. 2019. AudioJungle. (2019). https://audiojungle.net/.",
      "doi": ""
    },
    {
      "text": "L Manovich. 2018. AI Aesthetics. Strelka Press, Moscow, Russia.",
      "doi": ""
    },
    {
      "text": "John Maurer. 1999. A Brief History of Algorithmic composition. (1999).",
      "doi": ""
    },
    {
      "text": "Jon McCormack and Mark d'Inverno. 2014. On the Future of Computers and Creativity. In AISB 2014 Symposium on Computational Creativity. Society for the Study of Artificial Intelligence and the Simulation of Behaviour, London, UK, 9.",
      "doi": ""
    },
    {
      "text": "Alex McLean and Roger T Dean. 2018a. Musical Algorithms as Tools, Languages, and Partners: A Perspective. Oxford University Press, New Yor, NY, USA, 3--15.",
      "doi": ""
    },
    {
      "text": "Alex McLean and Roger T Dean. 2018b. The Oxford Handbook of Algorithmic Music. Oxford University Press, New York, NY, USA.",
      "doi": ""
    },
    {
      "text": "Eduardo Reck Miranda. 2013. Readings in Music and Artificial Intelligence. Routledge, New York and London.",
      "doi": ""
    },
    {
      "text": "Amper Music. 2019a. (2019). https://www.ampermusic.com/.",
      "doi": ""
    },
    {
      "text": "Amper Music. 2019b. Amper A.I. vs. Stock Results. (2019). https://www.ampermusic.com/docs/veritonic_study--amper_v_stock.pdf.",
      "doi": ""
    },
    {
      "text": "PG Music. 2019c. Band-in-a-Box. (2019). https://www.pgmusic.com/.",
      "doi": ""
    },
    {
      "text": "MusicBed. 2019. MusicBed. (2019). https://www.musicbed.com/.",
      "doi": ""
    },
    {
      "text": "Gerhard Nierhaus. 2009. Algorithmic Composition: Paradigms of Automated Music Generation. Springer Science & Business Media, New York, NY, USA.",
      "doi": "10.5555/1524239"
    },
    {
      "text": "NIME. 2019. The International Conference on New Interfaces for Musical Expression. (2019). https://www.nime.org/.",
      "doi": ""
    },
    {
      "text": "Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. 2016. Wavenet: A Generative Model for Raw Audio. (2016).",
      "doi": ""
    },
    {
      "text": "Alexandre Papadopoulos, Pierre Roy, and Francc ois Pachet. 2016. Assisted Lead Sheet Composition Using FlowComposer. In Proceedings of International Conference on Principles and Practice of Constraint Programming. Springer, Toulouse, France, 769--785.",
      "doi": ""
    },
    {
      "text": "George Papadopoulos and Geraint Wiggins. 1999. AI Methods for Algorithmic Composition: A Survey, a Critical View and Future Prospects. In AISB Symposium on Musical Creativity, Vol. 124. Edinburgh, UK, 110--117.",
      "doi": ""
    },
    {
      "text": "Richard Scheines. 1988. Automating Creativity. In Aspects of Artificial Intelligence. Springer, 339--365.",
      "doi": ""
    },
    {
      "text": "Adriana Schulz, Ariel Shamir, David IW Levin, Pitchaya Sitthi-Amorn, and Wojciech Matusik. 2014. Design and Fabrication by Example. ACM Transactions on Graphics (TOG) 33, 4 (2014), 62.",
      "doi": "10.1145/2601097.2601127"
    },
    {
      "text": "Rajiv Ratn Shah, Yi Yu, and Roger Zimmermann. 2014. Advisor: Personalized Video Soundtrack Recommendation by Late Fusion with Heuristic Rankings. In Proceedings of the 22nd ACM international conference on Multimedia. ACM, ACM, New York, NY, USA, 607--616.",
      "doi": "10.1145/2647868.2654919"
    },
    {
      "text": "Kristina Shea, Robert Aish, and Marina Gourtovaia. 2005. Towards Integrated Performance-Driven Generative Design Tools. Automation in Construction 14, 2 (2005), 253--264.",
      "doi": ""
    },
    {
      "text": "Shutterstock. 2019. Premium Beat - Score the Perfect Music. (2019). https://www.premiumbeat.com/.",
      "doi": ""
    },
    {
      "text": "Epidemic Sound. 2019. Epidemic Sound. (2019). https://www.epidemicsound.com/.",
      "doi": ""
    },
    {
      "text": "Thomas Strothotte and Stefan Schlechtweg. 2002. Non-Photorealistic Computer Graphics: Modeling, Rendering, and Animation. Morgan Kaufmann, San Francisco, CA, USA.",
      "doi": ""
    },
    {
      "text": "Aiva Technologies. 2019. The Artificial Intelligence Composing Emotional Soundtrack Music. (2019). https://www.aiva.ai/.",
      "doi": ""
    },
    {
      "text": "Benjamin Tissot. 2019. Bensound's Royalty Free Music. (2019). https://www.bensound.com/.",
      "doi": ""
    },
    {
      "text": "UserTesting. 2019. UserTesting. (2019). https://www.usertesting.com/.",
      "doi": ""
    },
    {
      "text": "Sean Vasquez and Mike Lewis. 2019. MelNet: A Generative Model for Audio in the Frequency Domain. (2019).",
      "doi": ""
    },
    {
      "text": "Ju-Chiang Wang, Yi-Hsuan Yang, I-Hong Jhuo, Yen-Yu Lin, Hsin-Min Wang, and others. 2012. The Acousticvisual Emotion Guassians Model for Automatic Generation of Music Video. In Proceedings of the 20th ACM international conference on Multimedia. ACM, ACM, New York, NY, USA, 1379--1380.",
      "doi": "10.1145/2393347.2396494"
    },
    {
      "text": "Robert Philip Weber. 1990. Basic Content Analysis. Sage, Newbury Park, California.",
      "doi": ""
    },
    {
      "text": "Ariel Weingarten, Ben Lafreniere, George Fitzmaurice, and Tovi Grossman. 2019. DreamRooms: Prototyping Rooms in Collaboration with a Generative Process. In Proceedings of Graphics Interface 2019 (GI 2019). Canadian Information Processing Society, Mississauga, Ontario, Canada, 9.",
      "doi": ""
    },
    {
      "text": "Iannis Xenakis. 1992. Formalized Music: Thought and Mathematics in Composition. Pendragon Press, Stuvesant, NY, USA.",
      "doi": ""
    },
    {
      "text": "Youtube. 2019. Youtube Audio Library. (2019). https://www.youtube.com/audiolibrary/.",
      "doi": ""
    }
  ]
}