{
  "doi": "10.1145/3544548.3581001",
  "title": "\"Help Me Help the AI\": Understanding How Explainability Can Support Human-AI Interaction",
  "published": "2023-04-19",
  "proctitle": "CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems",
  "pages": "1-17",
  "year": 2023,
  "badges": [
    "Honorable Mention"
  ],
  "abstract": "Despite the proliferation of explainable AI (XAI) methods, little is understood about end-users\u2019 explainability needs and behaviors around XAI explanations. To address this gap and contribute to understanding how explainability can support human-AI interaction, we conducted a mixed-methods study with 20 end-users of a real-world AI application, the Merlin bird identification app, and inquired about their XAI needs, uses, and perceptions. We found that participants desire practically useful information that can improve their collaboration with the AI, more so than technical system details. Relatedly, participants intended to use XAI explanations for various purposes beyond understanding the AI\u2019s outputs: calibrating trust, improving their task skills, changing their behavior to supply better inputs to the AI, and giving constructive feedback to developers. Finally, among existing XAI approaches, participants preferred part-based explanations that resemble human reasoning and explanations. We discuss the implications of our findings and provide recommendations for future XAI design.",
  "authors": [
    {
      "name": "Sunnie S. Y. Kim",
      "institution": "Princeton University, Princeton, NJ, USA",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99660639057",
      "orcid": "missing"
    },
    {
      "name": "Elizabeth Anne Watkins",
      "institution": "Intel Labs, Santa Clara, CA, USA",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659230348",
      "orcid": "missing"
    },
    {
      "name": "Olga Russakovsky",
      "institution": "Princeton University, Princeton, NJ, USA",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81550142556",
      "orcid": "missing"
    },
    {
      "name": "Ruth Fong",
      "institution": "Princeton University, Princeton, NJ, USA",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99660071585",
      "orcid": "missing"
    },
    {
      "name": "Andr\u00e9s Monroy-Hern\u00e1ndez",
      "institution": "Princeton University, Princeton, NJ, USA",
      "img": "/do/10.1145/contrib-81350567841/rel-imgonly/8598964387_bdf95cfca1_z.jpg",
      "acmid": "81350567841",
      "orcid": "0000-0003-4889-9484"
    }
  ],
  "references": [
    {
      "text": "Ashraf Abdul, Jo Vermeulen, Danding Wang, Brian\u00a0Y. Lim, and Mohan Kankanhalli. 2018. Trends and Trajectories for Explainable, Accountable and Intelligible Systems: An HCI Research Agenda. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (Montreal QC, Canada) (CHI \u201918). Association for Computing Machinery, New York, NY, USA, 1\u201318. https://doi.org/10.1145/3173574.3174156",
      "doi": "10.1145/3173574.3174156"
    },
    {
      "text": "Amina Adadi and Mohammed Berrada. 2018. Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI). IEEE Access 6(2018), 52138\u201352160. https://doi.org/10.1109/ACCESS.2018.2870052",
      "doi": ""
    },
    {
      "text": "Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim. 2018. Sanity Checks for Saliency Maps. In Neural Information Processing Systems (NeurIPS).",
      "doi": ""
    },
    {
      "text": "Kasun Amarasinghe, Kit\u00a0T. Rodolfa, S\u00e9rgio Jesus, Valerie Chen, Vladimir Balayan, Pedro Saleiro, Pedro Bizarro, Ameet Talwalkar, and Rayid Ghani. 2022. On the Importance of Application-Grounded Experimental Design for Evaluating Explainable ML Methods. https://doi.org/10.48550/ARXIV.2206.13503",
      "doi": ""
    },
    {
      "text": "Anna\u00a0Markella Antoniadi, Yuhan Du, Yasmine Guendouz, Lan Wei, Claudia Mazo, Brett\u00a0A. Becker, and Catherine Mooney. 2021. Current Challenges and Future Opportunities for XAI in Machine Learning-Based Clinical Decision Support Systems: A Systematic Review. Applied Sciences 11, 11 (2021). https://doi.org/10.3390/app11115088",
      "doi": ""
    },
    {
      "text": "Ines Arous, Jie Yang, Mourad Khayati, and Philippe Cudr\u00e9-Mauroux. 2020. OpenCrowd: A Human-AI Collaborative Approach for Finding Social Influencers via Open-Ended Answers Aggregation. In Proceedings of The Web Conference 2020 (Taipei, Taiwan) (WWW \u201920). Association for Computing Machinery, New York, NY, USA, 1851\u20131862. https://doi.org/10.1145/3366423.3380254",
      "doi": "10.1145/3366423.3380254"
    },
    {
      "text": "Alejandro\u00a0Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier\u00a0Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garc\u00eda, Sergio Gil-L\u00f3pez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI. Information Fusion (2020).",
      "doi": ""
    },
    {
      "text": "Zahra Ashktorab, Q.\u00a0Vera Liao, Casey Dugan, James Johnson, Qian Pan, Wei Zhang, Sadhana Kumaravel, and Murray Campbell. 2020. Human-AI Collaboration in a Cooperative Game Setting: Measuring Social Perception and Outcomes. Proc. ACM Hum.-Comput. Interact. 4, CSCW2, Article 96 (oct 2020), 20\u00a0pages. https://doi.org/10.1145/3415167",
      "doi": "10.1145/3415167"
    },
    {
      "text": "Shahin Atakishiyev, Mohammad Salameh, Hengshuai Yao, and Randy Goebel. 2021. Explainable Artificial Intelligence for Autonomous Driving: A Comprehensive Overview and Field Guide for Future Research Directions. CoRR abs/2112.11561(2021). arXiv:2112.11561https://arxiv.org/abs/2112.11561",
      "doi": ""
    },
    {
      "text": "Gagan Bansal, Besmira Nushi, Ece Kamar, Walter\u00a0S. Lasecki, Daniel\u00a0S. Weld, and Eric Horvitz. 2019. Beyond Accuracy: The Role of Mental Models in Human-AI Team Performance. In AAAI Conference on Human Computation and Crowdsourcing (HCOMP).",
      "doi": ""
    },
    {
      "text": "Gagan Bansal, Tongshuang Wu, Joyce Zhou, Raymond Fok, Besmira Nushi, Ece Kamar, Marco\u00a0Tulio Ribeiro, and Daniel Weld. 2021. Does the Whole Exceed Its Parts? The Effect of AI Explanations on Complementary Team Performance. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (Yokohama, Japan) (CHI \u201921). Association for Computing Machinery, New York, NY, USA, Article 81, 16\u00a0pages. https://doi.org/10.1145/3411764.3445717",
      "doi": "10.1145/3411764.3445717"
    },
    {
      "text": "David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. 2017. Network Dissection: Quantifying Interpretability of Deep Visual Representations. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).",
      "doi": ""
    },
    {
      "text": "David Bau, Jun-Yan Zhu, Jonas Wulff, William Peebles, Hendrik Strobelt, Bolei Zhou, and Antonio Torralba. 2019. Seeing What a GAN Cannot Generate. In International Conference on Computer Vision (ICCV).",
      "doi": ""
    },
    {
      "text": "Kevin Baum, Susanne Mantel, Eva Schmidt, and Timo Speith. 2022. From Responsibility to Reason-Giving Explainable Artificial Intelligence. Philosophy & Technology 35, 1 (2022), 12. https://doi.org/10.1007/s13347-022-00510-w",
      "doi": ""
    },
    {
      "text": "Umang Bhatt, Alice Xiang, Shubham Sharma, Adrian Weller, Ankur Taly, Yunhan Jia, Joydeep Ghosh, Ruchir Puri, Jos\u00e9 M.\u00a0F. Moura, and Peter Eckersley. 2020. Explainable Machine Learning in Deployment. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (Barcelona, Spain) (FAT* \u201920). Association for Computing Machinery, New York, NY, USA, 648\u2013657. https://doi.org/10.1145/3351095.3375624",
      "doi": "10.1145/3351095.3375624"
    },
    {
      "text": "Jeffrey\u00a0P Bigham, Chandrika Jayant, Andrew Miller, Brandyn White, and Tom Yeh. 2010. VizWiz:: LocateIt-enabling blind people to locate objects in their environment. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition-Workshops. IEEE, 65\u201372.",
      "doi": ""
    },
    {
      "text": "Wieland Brendel and Matthias Bethge. 2019. Approximating CNNs with Bag-of-local-Features Models Works Surprisingly Well on ImageNet. In International Conference on Learning Representations (ICLR).",
      "doi": ""
    },
    {
      "text": "David Broniatowski. 2021. Psychological Foundations of Explainability and Interpretability in Artificial Intelligence. https://doi.org/10.6028/NIST.IR.8367",
      "doi": ""
    },
    {
      "text": "Miles Brundage, Shahar Avin, Jasmine Wang, Haydn Belfield, Gretchen Krueger, Gillian Hadfield, Heidy Khlaaf, Jingying Yang, Helen Toner, Ruth Fong, Tegan Maharaj, Pang\u00a0Wei Koh, Sara Hooker, Jade Leung, Andrew Trask, Emma Bluemke, Jonathan Lebensold, Cullen O\u2019Keefe, Mark Koren, Th\u00e9o Ryffel, JB Rubinovitz, Tamay Besiroglu, Federica Carugati, Jack Clark, Peter Eckersley, Sarah de Haas, Maritza Johnson, Ben Laurie, Alex Ingerman, Igor Krawczuk, Amanda Askell, Rosario Cammarota, Andrew Lohn, David Krueger, Charlotte Stix, Peter Henderson, Logan Graham, Carina Prunkl, Bianca Martin, Elizabeth Seger, Noa Zilberman, Se\u00e1n\u00a0\u00d3 h\u00c9igeartaigh, Frens Kroeger, Girish Sastry, Rebecca Kagan, Adrian Weller, Brian Tse, Elizabeth Barnes, Allan Dafoe, Paul Scharre, Ariel Herbert-Voss, Martijn Rasser, Shagun Sodhani, Carrick Flynn, Thomas\u00a0Krendl Gilbert, Lisa Dyer, Saif Khan, Yoshua Bengio, and Markus Anderljung. 2020. Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims. https://doi.org/10.48550/ARXIV.2004.07213",
      "doi": ""
    },
    {
      "text": "Moritz B\u00f6hle, Mario Fritz, and Bernt Schiele. 2021. Convolutional Dynamic Alignment Networks for Interpretable Classifications. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).",
      "doi": ""
    },
    {
      "text": "Moritz B\u00f6hle, Mario Fritz, and Bernt Schiele. 2022. B-cos Networks: Alignment is All We Need for Interpretability. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).",
      "doi": ""
    },
    {
      "text": "Carrie\u00a0J. Cai, Emily Reif, Narayan Hegde, Jason Hipp, Been Kim, Daniel Smilkov, Martin Wattenberg, Fernanda Viegas, Greg\u00a0S. Corrado, Martin\u00a0C. Stumpe, and Michael Terry. 2019. Human-Centered Tools for Coping with Imperfect Algorithms During Medical Decision-Making. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (Glasgow, Scotland Uk) (CHI \u201919). Association for Computing Machinery, New York, NY, USA, 1\u201314. https://doi.org/10.1145/3290605.3300234",
      "doi": "10.1145/3290605.3300234"
    },
    {
      "text": "Carrie\u00a0J. Cai, Samantha Winter, David Steiner, Lauren Wilcox, and Michael Terry. 2019. \"Hello AI\": Uncovering the Onboarding Needs of Medical Practitioners for Human-AI Collaborative Decision-Making. Proc. ACM Hum.-Comput. Interact. 3, CSCW, Article 104 (nov 2019), 24\u00a0pages. https://doi.org/10.1145/3359206",
      "doi": "10.1145/3359206"
    },
    {
      "text": "Chaofan Chen, Oscar Li, Daniel Tao, Alina Barnett, Cynthia Rudin, and Jonathan\u00a0K Su. 2019. This Looks Like That: Deep Learning for Interpretable Image Recognition. In Neural Information Processing Systems (NeurIPS).",
      "doi": ""
    },
    {
      "text": "Nazli Cila. 2022. Designing Human-Agent Collaborations: Commitment, Responsiveness, and Support. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems (New Orleans, LA, USA) (CHI \u201922). Association for Computing Machinery, New York, NY, USA, Article 420, 18\u00a0pages. https://doi.org/10.1145/3491102.3517500",
      "doi": "10.1145/3491102.3517500"
    },
    {
      "text": "Roberto Confalonieri, Tarek\u00a0R. Besold, Tillman Weyde, Kathleen Creel, Tania Lombrozo, Shane\u00a0T. Mueller, and Patrick Shafto. 2019. What makes a good explanation? Cognitive dimensions of explaining intelligent machines. In CogSci. 25\u201326. https://mindmodeling.org/cogsci2019/papers/0013/index.html",
      "doi": ""
    },
    {
      "text": "Arun Das and Paul Rad. 2020. Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey. CoRR abs/2006.11371(2020). arXiv:2006.11371https://arxiv.org/abs/2006.11371",
      "doi": ""
    },
    {
      "text": "Jon Donnelly, Alina\u00a0Jade Barnett, and Chaofan Chen. 2022. Deformable ProtoPNet: An Interpretable Image Classifier Using Deformable Prototypes. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).",
      "doi": ""
    },
    {
      "text": "Finale Doshi-Velez and Been Kim. 2017. Towards A Rigorous Science of Interpretable Machine Learning. arxiv:1702.08608\u00a0[stat.ML]",
      "doi": ""
    },
    {
      "text": "Abhimanyu Dubey, Filip Radenovic, and Dhruv Mahajan. 2022. Scalable Interpretability via Polynomials. In Neural Information Processing Systems (NeurIPS).",
      "doi": ""
    },
    {
      "text": "Salvatore D\u2019Avella, Gerardo Camacho-Gonzalez, and Paolo Tripicchio. 2022. On Multi-Agent Cognitive Cooperation: Can virtual agents behave like humans?Neurocomputing 480(2022), 27\u201338. https://doi.org/10.1016/j.neucom.2022.01.025",
      "doi": "10.1016/j.neucom.2022.01.025"
    },
    {
      "text": "Upol Ehsan, Q.\u00a0Vera Liao, Michael Muller, Mark\u00a0O. Riedl, and Justin\u00a0D. Weisz. 2021. Expanding Explainability: Towards Social Transparency in AI Systems. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (Yokohama, Japan) (CHI \u201921). Association for Computing Machinery, New York, NY, USA, Article 82, 19\u00a0pages. https://doi.org/10.1145/3411764.3445188",
      "doi": "10.1145/3411764.3445188"
    },
    {
      "text": "Upol Ehsan, Samir Passi, Q.\u00a0Vera Liao, Larry Chan, I-Hsiang Lee, Michael\u00a0J. Muller, and Mark\u00a0O. Riedl. 2021. The Who in Explainable AI: How AI Background Shapes Perceptions of AI Explanations. CoRR abs/2107.13509(2021). arXiv:2107.13509https://arxiv.org/abs/2107.13509",
      "doi": ""
    },
    {
      "text": "Upol Ehsan and Mark\u00a0O. Riedl. 2020. Human-centered Explainable AI: Towards a Reflective Sociotechnical Approach. CoRR abs/2002.01092(2020). arXiv:2002.01092https://arxiv.org/abs/2002.01092",
      "doi": ""
    },
    {
      "text": "Upol Ehsan, Philipp Wintersberger, Q.\u00a0Vera Liao, Martina Mara, Marc Streit, Sandra Wachter, Andreas Riener, and Mark\u00a0O. Riedl. 2021. Operationalizing Human-Centered Perspectives in Explainable AI. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems (Yokohama, Japan) (CHI EA \u201921). Association for Computing Machinery, New York, NY, USA, Article 94, 6\u00a0pages. https://doi.org/10.1145/3411763.3441342",
      "doi": "10.1145/3411763.3441342"
    },
    {
      "text": "Upol Ehsan, Philipp Wintersberger, Q.\u00a0Vera Liao, Elizabeth\u00a0Anne Watkins, Carina Manger, Hal Daum\u00e9\u00a0III, Andreas Riener, and Mark\u00a0O Riedl. 2022. Human-Centered Explainable AI (HCXAI): Beyond Opening the Black-Box of AI. In Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems (New Orleans, LA, USA) (CHI EA \u201922). Association for Computing Machinery, New York, NY, USA, Article 109, 7\u00a0pages. https://doi.org/10.1145/3491101.3503727",
      "doi": "10.1145/3491101.3503727"
    },
    {
      "text": "Madeleine\u00a0Clare Elish and Elizabeth\u00a0Anne Watkins. 2020. Repairing innovation: A study of integrating AI in clinical care. Data & Society (2020).",
      "doi": ""
    },
    {
      "text": "Thomas Fel, Julien Colin, R\u00e9mi Cad\u00e8ne, and Thomas Serre. 2021. What I Cannot Predict, I Do Not Understand: A Human-Centered Evaluation Framework for Explainability Methods. arxiv:2112.04417\u00a0[s.CV]",
      "doi": ""
    },
    {
      "text": "Andrea Ferrario and Michele Loi. 2022. How Explainability Contributes to Trust in AI. In 2022 ACM Conference on Fairness, Accountability, and Transparency (Seoul, Republic of Korea) (FAccT \u201922). Association for Computing Machinery, New York, NY, USA, 1457\u20131466. https://doi.org/10.1145/3531146.3533202",
      "doi": "10.1145/3531146.3533202"
    },
    {
      "text": "Riccardo Fogliato, Shreya Chappidi, Matthew Lungren, Paul Fisher, Diane Wilson, Michael Fitzke, Mark Parkinson, Eric Horvitz, Kori Inkpen, and Besmira Nushi. 2022. Who Goes First? Influences of Human-AI Workflow on Decision Making in Clinical Imaging. In 2022 ACM Conference on Fairness, Accountability, and Transparency (Seoul, Republic of Korea) (FAccT \u201922). Association for Computing Machinery, New York, NY, USA, 1362\u20131374. https://doi.org/10.1145/3531146.3533193",
      "doi": "10.1145/3531146.3533193"
    },
    {
      "text": "Ruth Fong. 2020. Understanding convolutional neural networks. Ph.\u00a0D. Dissertation. University of Oxford.",
      "doi": ""
    },
    {
      "text": "Ruth Fong, Mandela Patrick, and Andrea Vedaldi. 2019. Understanding Deep Networks via Extremal Perturbations and Smooth Masks. In International Conference on Computer Vision (ICCV).",
      "doi": ""
    },
    {
      "text": "Ruth Fong and Andrea Vedaldi. 2017. Interpretable Explanations of Black Boxes by Meaningful Perturbation. In International Conference on Computer Vision (ICCV).",
      "doi": ""
    },
    {
      "text": "Ruth Fong and Andrea Vedaldi. 2018. Net2Vec: Quantifying and Explaining how Concepts are Encoded by Filters in Deep Neural Networks. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).",
      "doi": ""
    },
    {
      "text": "Julie Gerlings, Millie\u00a0S\u00f8ndergaard Jensen, and Arisa Shollo. 2021. Explainable AI, but explainable to whom?CoRR abs/2106.05568(2021). arXiv:2106.05568https://arxiv.org/abs/2106.05568",
      "doi": ""
    },
    {
      "text": "Leilani\u00a0H. Gilpin, David Bau, Ben\u00a0Z Yuan, Ayesha Bajwa, Michael Specter, and Lalana Kagal. 2018. Explaining explanations: An overview of interpretability of machine learning. In The 5th IEEE International Conference on Data Science and Advanced Analytics (DSAA).",
      "doi": ""
    },
    {
      "text": "Yash Goyal, Ziyan Wu, Jan Ernst, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019. Counterfactual Visual Explanations. In International Conference on Machine Learning (ICML).",
      "doi": ""
    },
    {
      "text": "Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti, and Dino Pedreschi. 2018. A Survey of Methods for Explaining Black Box Models. ACM Comput. Surv. 51, 5, Article 93 (aug 2018), 42\u00a0pages. https://doi.org/10.1145/3236009",
      "doi": "10.1145/3236009"
    },
    {
      "text": "David Gunning and David Aha. 2019. DARPA\u2019s Explainable Artificial Intelligence (XAI) Program. AI Magazine (2019).",
      "doi": ""
    },
    {
      "text": "Robert\u00a0R. Hoffman, Gary Klein, and Shane\u00a0T. Mueller. 2018. Explaining Explanation For \u201cExplainable AI\u201d. Proceedings of the Human Factors and Ergonomics Society Annual Meeting 62, 1(2018), 197\u2013201. https://doi.org/10.1177/1541931218621047 arXiv:https://doi.org/10.1177/1541931218621047",
      "doi": ""
    },
    {
      "text": "Adrian Hoffmann, Claudio Fanconi, Rahul Rade, and Jonas Kohler. 2021. This Looks Like That... Does it? Shortcomings of Latent Space Prototype Interpretability in Deep Networks. In International Conference on Machine Learning (ICML) Workshop on Theoretic Foundation, Criticism, and Application Trend of Explainable AI.",
      "doi": ""
    },
    {
      "text": "Sungsoo\u00a0Ray Hong, Jessica Hullman, and Enrico Bertini. 2020. Human Factors in Model Interpretability: Industry Practices, Challenges, and Needs. Proc. ACM Hum.-Comput. Interact. 4, CSCW1, Article 68 (may 2020), 26\u00a0pages. https://doi.org/10.1145/3392878",
      "doi": "10.1145/3392878"
    },
    {
      "text": "Shagun Jhaver, Iris Birman, Eric Gilbert, and Amy Bruckman. 2019. Human-Machine Collaboration for Content Regulation: The Case of Reddit Automoderator. ACM Trans. Comput.-Hum. Interact. 26, 5, Article 31 (jul 2019), 35\u00a0pages. https://doi.org/10.1145/3338243",
      "doi": "10.1145/3338243"
    },
    {
      "text": "Atoosa Kasirzadeh. 2021. Reasons, Values, Stakeholders: A Philosophical Framework for Explainable Artificial Intelligence. CoRR abs/2103.00752(2021). arXiv:2103.00752https://arxiv.org/abs/2103.00752",
      "doi": ""
    },
    {
      "text": "Been Kim, Emily Reif, Martin Wattenberg, Samy Bengio, and Michael\u00a0C. Mozer. 2021. Neural Networks Trained on Natural Scenes Exhibit Gestalt Closure. Computational Brain & Behavior(2021).",
      "doi": ""
    },
    {
      "text": "Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, 2018. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (TCAV). In ICML.",
      "doi": ""
    },
    {
      "text": "Sunnie S.\u00a0Y. Kim, Nicole Meister, Vikram\u00a0V. Ramaswamy, Ruth Fong, and Olga Russakovsky. 2022. HIVE: Evaluating the Human Interpretability of Visual Explanations. In European Conference on Computer Vision (ECCV).",
      "doi": ""
    },
    {
      "text": "Pang\u00a0Wei Koh and Percy Liang. 2017. Understanding Black-box Predictions via Influence Functions. In International Conference on Machine Learning (ICML).",
      "doi": ""
    },
    {
      "text": "Pang\u00a0Wei Koh, Thao Nguyen, Yew\u00a0Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and Percy Liang. 2020. Concept Bottleneck Models. In International Conference on Machine Learning (ICML).",
      "doi": ""
    },
    {
      "text": "Ranjay Krishna, Donsuk Lee, Li Fei-Fei, and Michael\u00a0S. Bernstein. 2022. Socially situated artificial intelligence enables learning from human interaction. Proceedings of the National Academy of Sciences 119, 39(2022), e2115730119. https://doi.org/10.1073/pnas.2115730119 arXiv:https://www.pnas.org/doi/pdf/10.1073/pnas.2115730119",
      "doi": ""
    },
    {
      "text": "Shitij Kumar, Celal Savur, and Ferat Sahin. 2021. Survey of Human\u2013Robot Collaboration in Industrial Settings: Awareness, Intelligence, and Compliance. IEEE Transactions on Systems, Man, and Cybernetics: Systems 51, 1(2021), 280\u2013297. https://doi.org/10.1109/TSMC.2020.3041231",
      "doi": ""
    },
    {
      "text": "Vivian Lai, Samuel Carton, Rajat Bhatnagar, Q.\u00a0Vera Liao, Yunfeng Zhang, and Chenhao Tan. 2022. Human-AI Collaboration via Conditional Delegation: A Case Study of Content Moderation. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems (New Orleans, LA, USA) (CHI \u201922). Association for Computing Machinery, New York, NY, USA, Article 54, 18\u00a0pages. https://doi.org/10.1145/3491102.3501999",
      "doi": "10.1145/3491102.3501999"
    },
    {
      "text": "Vivian Lai and Chenhao Tan. 2019. On Human Predictions with Explanations and Predictions of Machine Learning Models: A Case Study on Deception Detection. In Proceedings of the Conference on Fairness, Accountability, and Transparency (Atlanta, GA, USA) (FAT* \u201919). Association for Computing Machinery, New York, NY, USA, 29\u201338. https://doi.org/10.1145/3287560.3287590",
      "doi": "10.1145/3287560.3287590"
    },
    {
      "text": "Markus Langer, Daniel Oster, Timo Speith, Holger Hermanns, Lena K\u00e4stner, Eva Schmidt, Andreas Sesing, and Kevin Baum. 2021. What Do We Want From Explainable Artificial Intelligence (XAI)? - A Stakeholder Perspective on XAI and a Conceptual Model Guiding Interdisciplinary XAI Research. CoRR abs/2102.07817(2021). arXiv:2102.07817https://arxiv.org/abs/2102.07817",
      "doi": ""
    },
    {
      "text": "Mina Lee, Percy Liang, and Qian Yang. 2022. CoAuthor: Designing a Human-AI Collaborative Writing Dataset for Exploring Language Model Capabilities. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems (New Orleans, LA, USA) (CHI \u201922). Association for Computing Machinery, New York, NY, USA, Article 388, 19\u00a0pages. https://doi.org/10.1145/3491102.3502030",
      "doi": "10.1145/3491102.3502030"
    },
    {
      "text": "Q.\u00a0Vera Liao, Daniel Gruen, and Sarah Miller. 2020. Questioning the AI: Informing Design Practices for Explainable AI User Experiences. Association for Computing Machinery, New York, NY, USA, 1\u201315. https://doi.org/10.1145/3313831.3376590",
      "doi": "10.1145/3313831.3376590"
    },
    {
      "text": "Q.\u00a0Vera Liao and Kush\u00a0R. Varshney. 2021. Human-Centered Explainable AI (XAI): From Algorithms to User Experiences. CoRR abs/2110.10790(2021). arXiv:2110.10790https://arxiv.org/abs/2110.10790",
      "doi": ""
    },
    {
      "text": "Q.\u00a0Vera Liao, Yunfeng Zhang, Ronny Luss, Finale Doshi-Velez, and Amit Dhurandhar. 2022. Connecting Algorithmic Research and Usage Contexts: A Perspective of Contextualized Evaluation for Explainable AI. https://doi.org/10.48550/ARXIV.2206.10847",
      "doi": ""
    },
    {
      "text": "Zachary\u00a0C. Lipton. 2018. The Mythos of Model Interpretability: In Machine Learning, the Concept of Interpretability is Both Important and Slippery.Queue (2018).",
      "doi": ""
    },
    {
      "text": "Ryan Louie, Andy Coenen, Cheng\u00a0Zhi Huang, Michael Terry, and Carrie\u00a0J. Cai. 2020. Novice-AI Music Co-Creation via AI-Steering Tools for Deep Generative Models. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI \u201920). Association for Computing Machinery, New York, NY, USA, 1\u201313. https://doi.org/10.1145/3313831.3376739",
      "doi": "10.1145/3313831.3376739"
    },
    {
      "text": "J\u00f6rn L\u00f6tsch, Dario Kringel, and Alfred Ultsch. 2022. Explainable Artificial Intelligence (XAI) in Biomedicine: Making AI Decisions Trustworthy for Physicians and Patients. BioMedInformatics 2, 1 (2022), 1\u201317. https://doi.org/10.3390/biomedinformatics2010001",
      "doi": ""
    },
    {
      "text": "R. Machlev, L. Heistrene, M. Perl, K.Y. Levy, J. Belikov, S. Mannor, and Y. Levron. 2022. Explainable Artificial Intelligence (XAI) techniques for energy and power systems: Review, challenges and opportunities. Energy and AI 9(2022), 100169. https://doi.org/10.1016/j.egyai.2022.100169",
      "doi": ""
    },
    {
      "text": "Antonios Mamalakis, Imme Ebert-Uphoff, and Elizabeth\u00a0A. Barnes. 2022. Explainable Artificial Intelligence in Meteorology and Climate Science: Model Fine-Tuning, Calibrating Trust and Learning New Science. Springer International Publishing, Cham, 315\u2013339. https://doi.org/10.1007/978-3-031-04083-2_16",
      "doi": "10.1007/978-3-031-04083-2_16"
    },
    {
      "text": "Andrei Margeloiu, Matthew Ashman, Umang Bhatt, Yanzhi Chen, Mateja Jamnik, and Adrian Weller. 2021. Do Concept Bottleneck Models Learn as Intended?. In International Conference on Learning Representations (ICLR) Workshop on Responsible AI.",
      "doi": ""
    },
    {
      "text": "Aniek\u00a0F Markus, Jan\u00a0A Kors, and Peter\u00a0R Rijnbeek. 2021. The role of explainability in creating trustworthy artificial intelligence for health care: A comprehensive survey of the terminology, design choices, and evaluation strategies. Journal of biomedical informatics 113 (2021). https://doi.org/doi:10.1016/j.jbi.2020.103655",
      "doi": "10.1016/j.jbi.2020.103655"
    },
    {
      "text": "Nora McDonald, Sarita Schoenebeck, and Andrea Forte. 2019. Reliability and inter-rater reliability in qualitative research: Norms and guidelines for CSCW and HCI practice. Proceedings of the ACM on human-computer interaction 3, CSCW(2019), 1\u201323.",
      "doi": "10.1145/3359174"
    },
    {
      "text": "Quentin Meteier, Marine Capallera, Leonardo Angelini, Elena Mugellini, Omar\u00a0Abou Khaled, Stefano Carrino, Emmanuel De\u00a0Salis, St\u00e9phane Galland, and Susanne Boll. 2019. Workshop on Explainable AI in Automated Driving: A User-Centered Interaction Approach. In Proceedings of the 11th International Conference on Automotive User Interfaces and Interactive Vehicular Applications: Adjunct Proceedings (Utrecht, Netherlands) (AutomotiveUI \u201919). Association for Computing Machinery, New York, NY, USA, 32\u201337. https://doi.org/10.1145/3349263.3350762",
      "doi": "10.1145/3349263.3350762"
    },
    {
      "text": "Tim Miller. 2017. Explanation in Artificial Intelligence: Insights from the Social Sciences. CoRR abs/1706.07269(2017). arXiv:1706.07269http://arxiv.org/abs/1706.07269",
      "doi": ""
    },
    {
      "text": "Tim Miller. 2022. Are we measuring trust correctly in explainability, interpretability, and transparency research?https://doi.org/10.48550/ARXIV.2209.00651",
      "doi": ""
    },
    {
      "text": "Tim Miller, Piers Howe, and Liz Sonenberg. 2017. Explainable AI: Beware of Inmates Running the Asylum Or: How I Learnt to Stop Worrying and Love the Social and Behavioural Sciences. CoRR abs/1712.00547(2017). arXiv:1712.00547http://arxiv.org/abs/1712.00547",
      "doi": ""
    },
    {
      "text": "Mitch Waite Group. [n. d.]. iBird Pro Guide. https://apps.apple.com/us/app/ibird-pro-guide-to-birds/id308018823",
      "doi": ""
    },
    {
      "text": "Brent Mittelstadt, Chris Russell, and Sandra Wachter. 2019. Explaining Explanations in AI. In Proceedings of the Conference on Fairness, Accountability, and Transparency (Atlanta, GA, USA) (FAT* \u201919). Association for Computing Machinery, New York, NY, USA, 279\u2013288. https://doi.org/10.1145/3287560.3287574",
      "doi": "10.1145/3287560.3287574"
    },
    {
      "text": "Sina Mohseni, Niloofar Zarei, and Eric\u00a0D. Ragan. 2018. A Survey of Evaluation Methods and Measures for Interpretable Machine Learning. CoRR abs/1811.11839(2018). arXiv:1811.11839http://arxiv.org/abs/1811.11839",
      "doi": ""
    },
    {
      "text": "Christoph Molnar. 2022. Interpretable Machine Learning(2 ed.). https://christophm.github.io/interpretable-ml-book",
      "doi": ""
    },
    {
      "text": "Raha Moraffah, Mansooreh Karami, Ruocheng Guo, Adrienne Raglin, and Huan Liu. 2020. Causal Interpretability for Machine Learning - Problems, Methods and Evaluation. SIGKDD Explor. Newsl. 22, 1 (may 2020), 18\u201333. https://doi.org/10.1145/3400051.3400058",
      "doi": "10.1145/3400051.3400058"
    },
    {
      "text": "Michael\u00a0J. Muller. 2002. Participatory Design: The Third Space in HCI. L. Erlbaum Associates Inc., USA, 1051\u20131068.",
      "doi": ""
    },
    {
      "text": "National Audubon Society. [n. d.]. Audubon Bird Guide. https://apps.apple.com/us/app/audubon-bird-guide/id333227386",
      "doi": ""
    },
    {
      "text": "Meike Nauta, Ron van Bree, and Christin Seifert. 2021. Neural Prototype Trees for Interpretable Fine-Grained Image Recognition. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).",
      "doi": ""
    },
    {
      "text": "An\u00a0T. Nguyen, Aditya Kharosekar, Saumyaa Krishnan, Siddhesh Krishnan, Elizabeth Tate, Byron\u00a0C. Wallace, and Matthew Lease. 2018. Believe It or Not: Designing a Human-AI Partnership for Mixed-Initiative Fact-Checking. In Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology(Berlin, Germany) (UIST \u201918). Association for Computing Machinery, New York, NY, USA, 189\u2013199. https://doi.org/10.1145/3242587.3242666",
      "doi": "10.1145/3242587.3242666"
    },
    {
      "text": "Giang Nguyen, Daeyoung Kim, and Anh Nguyen. 2021. The effectiveness of feature attribution methods and its correlation with automatic evaluation scores. In Neural Information Processing Systems (NeurIPS).",
      "doi": ""
    },
    {
      "text": "Giang Nguyen, Mohammad\u00a0Reza Taesiri, and Anh Nguyen. 2022. Visual correspondence-based explanations improve AI robustness and human-AI team accuracy. In Neural Information Processing Systems (NeurIPS).",
      "doi": ""
    },
    {
      "text": "Takashi Numata, Hiroki Sato, Yasuhiro Asa, Takahiko Koike, Kohei Miyata, Eri Nakagawa, Motofumi Sumiya, and Norihiro Sadato. 2020. Achieving affective human\u2013virtual agent communication by enabling virtual agents to imitate positive expressions. Scientific Reports 10, 1 (2020), 5977. https://doi.org/10.1038/s41598-020-62870-7",
      "doi": ""
    },
    {
      "text": "Uchenna\u00a0Emeoha Ogenyi, Jinguo Liu, Chenguang Yang, Zhaojie Ju, and Honghai Liu. 2021. Physical Human\u2013Robot Collaboration: Robotic Systems, Learning Methods, Collaborative Strategies, Sensors, and Actuators. IEEE Transactions on Cybernetics 51, 4 (2021), 1888\u20131901. https://doi.org/10.1109/TCYB.2019.2947532",
      "doi": ""
    },
    {
      "text": "Daniel Omeiza, Helena Webb, Marina Jirotka, and Lars Kunze. 2022. Explanations in Autonomous Driving: A Survey. IEEE Transactions on Intelligent Transportation Systems 23, 8(2022), 10142\u201310162. https://doi.org/10.1109/TITS.2021.3122865",
      "doi": ""
    },
    {
      "text": "Thomas O\u2019Neill, Nathan McNeese, Amy Barron, and Beau Schelble. 2022. Human\u2013Autonomy Teaming: A Review and Analysis of the Empirical Literature. Human Factors 64, 5 (2022), 904\u2013938. https://doi.org/10.1177/0018720820960865 arXiv:https://doi.org/10.1177/0018720820960865PMID: 33092417.",
      "doi": ""
    },
    {
      "text": "Michael Pazzani, Severine Soltani, Robert Kaufman, Samson Qian, and Albert Hsiao. 2022. Expert-Informed, User-Centric Explanations for Machine Learning. Proceedings of the AAAI Conference on Artificial Intelligence 36, 11 (Jun. 2022), 12280\u201312286. https://doi.org/10.1609/aaai.v36i11.21491",
      "doi": ""
    },
    {
      "text": "Dino Pedreschi, Fosca Giannotti, Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, and Franco Turini. 2019. Meaningful Explanations of Black Box AI Decision Systems. Proceedings of the AAAI Conference on Artificial Intelligence 33, 01 (Jul. 2019), 9780\u20139784. https://doi.org/10.1609/aaai.v33i01.33019780",
      "doi": "10.1609/aaai.v33i01.33019780"
    },
    {
      "text": "Vitali Petsiuk, Abir Das, and Kate Saenko. 2018. RISE: Randomized Input Sampling for Explanation of Black-box Models. In British Machine Vision Conference (BMVC).",
      "doi": ""
    },
    {
      "text": "Barbara Pintar. 2017. Birdadvisor 360\u00b0: A digital support for birdwatching tourism in Algarve. Ph.\u00a0D. Dissertation. University of Algarve.",
      "doi": ""
    },
    {
      "text": "Milda Pocevi\u010di\u016bt\u0117, Gabriel Eilertsen, and Claes Lundstr\u00f6m. 2020. Survey of XAI in Digital Pathology. Springer International Publishing, Cham, 56\u201388. https://doi.org/10.1007/978-3-030-50402-1_4",
      "doi": ""
    },
    {
      "text": "Forough Poursabzi-Sangdeh, Daniel\u00a0G Goldstein, Jake\u00a0M Hofman, Jennifer\u00a0Wortman Wortman\u00a0Vaughan, and Hanna Wallach. 2021. Manipulating and Measuring Model Interpretability. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (Yokohama, Japan) (CHI \u201921). Association for Computing Machinery, New York, NY, USA, Article 237, 52\u00a0pages. https://doi.org/10.1145/3411764.3445315",
      "doi": "10.1145/3411764.3445315"
    },
    {
      "text": "Alun\u00a0D. Preece, Dan Harborne, Dave Braines, Richard Tomsett, and Supriyo Chakraborty. 2018. Stakeholders in Explainable AI. CoRR abs/1810.00184(2018). arXiv:1810.00184http://arxiv.org/abs/1810.00184",
      "doi": ""
    },
    {
      "text": "Filip Radenovic, Abhimanyu Dubey, and Dhruv Mahajan. 2022. Neural Basis Models for Interpretability. In Neural Information Processing Systems (NeurIPS).",
      "doi": ""
    },
    {
      "text": "Vikram\u00a0V. Ramaswamy, Sunnie S.\u00a0Y. Kim, Ruth Fong, and Olga Russakovsky. 2022. Overlooked factors in concept-based explanations: Dataset choice, concept salience, and human capability. https://doi.org/10.48550/ARXIV.2207.09615",
      "doi": ""
    },
    {
      "text": "Vikram\u00a0V. Ramaswamy, Sunnie S.\u00a0Y. Kim, Nicole Meister, Ruth Fong, and Olga Russakovsky. 2022. ELUDE: Generating interpretable explanations via a decomposition into labelled and unlabelled features. https://doi.org/10.48550/ARXIV.2206.07690",
      "doi": ""
    },
    {
      "text": "Marco\u00a0Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. \u201cWhy Should I Trust You?\": Explaining the Predictions of Any Classifier. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD).",
      "doi": "10.1145/2939672.2939778"
    },
    {
      "text": "Cynthia Rudin, Chaofan Chen, Zhi Chen, Haiyang Huang, Lesia Semenova, and Chudi Zhong. 2022. Interpretable machine learning: Fundamental principles and 10 grand challenges. Statistics Surveys 16, none (2022), 1 \u2013 85. https://doi.org/10.1214/21-SS133",
      "doi": ""
    },
    {
      "text": "Johnny Salda\u00f1a. 2021. The coding manual for qualitative researchers. The coding manual for qualitative researchers (2021), 1\u2013440.",
      "doi": ""
    },
    {
      "text": "Wojciech Samek, Gr\u00e9goire Montavon, Andrea Vedaldi, Lars\u00a0Kai Hansen, and Klaus-Robert M\u00fcller (Eds.). 2019. Explainable AI: Interpreting, Explaining and Visualizing Deep Learning. Lecture Notes in Computer Science, Vol.\u00a011700. Springer.",
      "doi": "10.1007/978-3-030-28954-6"
    },
    {
      "text": "Lindsay Sanneman and Julie\u00a0A. Shah. 2020. A Situation Awareness-Based Framework for Design and Evaluation of Explainable AI. In Explainable, Transparent Autonomous Agents and Multi-Agent Systems, Davide Calvaresi, Amro Najjar, Michael Winikoff, and Kary Fr\u00e4mling (Eds.). Springer International Publishing, Cham, 94\u2013110.",
      "doi": ""
    },
    {
      "text": "Nicolas Scharowski, Sebastian A.\u00a0C. Perrig, Nick von Felten, and Florian Br\u00fchlmann. 2022. Trust and Reliance in XAI \u2013 Distinguishing Between Attitudinal and Behavioral Measures. https://doi.org/10.48550/ARXIV.2203.12318",
      "doi": ""
    },
    {
      "text": "Ramprasaath\u00a0R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. 2017. Grad-CAM: Visual Explanations From Deep Networks via Gradient-Based Localization. In International Conference on Computer Vision (ICCV).",
      "doi": ""
    },
    {
      "text": "Mark Sendak, Madeleine\u00a0Clare Elish, Michael Gao, Joseph Futoma, William Ratliff, Marshall Nichols, Armando Bedoya, Suresh Balu, and Cara O\u2019Brien. 2020. \"The Human Body is a Black Box\": Supporting Clinical Decision-Making with Deep Learning. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (Barcelona, Spain) (FAT* \u201920). Association for Computing Machinery, New York, NY, USA, 99\u2013109. https://doi.org/10.1145/3351095.3372827",
      "doi": "10.1145/3351095.3372827"
    },
    {
      "text": "Hua Shen and Ting-Hao\u00a0Kenneth Huang. 2020. How Useful Are the Machine-Generated Interpretations to General Users? A Human Evaluation on Guessing the Incorrectly Predicted Labels. In AAAI Conference on Human Computation and Crowdsourcing (HCOMP).",
      "doi": ""
    },
    {
      "text": "Vivswan Shitole, Fuxin Li, Minsuk Kahng, Prasad Tadepalli, and Alan Fern. 2021. One Explanation is Not Enough: Structured Attention Graphs for Image Classification. In Neural Information Processing Systems (NeurIPS).",
      "doi": ""
    },
    {
      "text": "Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2014. Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps. In International Conference on Learning Representations (ICLR) Workshops.",
      "doi": ""
    },
    {
      "text": "Amitojdeep Singh, Sourya Sengupta, and Vasudevan Lakshminarayanan. 2020. Explainable Deep Learning Models in Medical Image Analysis. Journal of Imaging 6(2020). https://doi.org/10.3390/jimaging6060052",
      "doi": ""
    },
    {
      "text": "Dylan Slack, Satyapriya Krishna, Himabindu Lakkaraju, and Sameer Singh. 2022. TalkToModel: Explaining Machine Learning Models with Interactive Natural Language Conversations. arXiv (2022).",
      "doi": ""
    },
    {
      "text": "Helen Smith. 2021. Clinical AI: opacity, accountability, responsibility and liability. AI & SOCIETY 36, 2 (2021), 535\u2013545. https://doi.org/10.1007/s00146-020-01019-6",
      "doi": "10.1007/s00146-020-01019-6"
    },
    {
      "text": "Alison Smith-Renner, Ron Fan, Melissa Birchfield, Tongshuang Wu, Jordan Boyd-Graber, Daniel\u00a0S. Weld, and Leah Findlater. 2020. No Explainability without Accountability: An Empirical Study of Explanations and Feedback in Interactive ML. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI \u201920). Association for Computing Machinery, New York, NY, USA, 1\u201313. https://doi.org/10.1145/3313831.3376624",
      "doi": "10.1145/3313831.3376624"
    },
    {
      "text": "Nathalie\u00a0A. Smuha. 2019. The EU Approach to Ethics Guidelines for Trustworthy Artificial Intelligence. Computer Law Review International 20, 4 (2019), 97\u2013106. https://doi.org/doi:10.9785/cri-2019-200402",
      "doi": ""
    },
    {
      "text": "Ramya Srinivasan and Ajay Chander. 2020. Explanation Perspectives from the Cognitive Sciences\u2014A Survey. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, Christian Bessiere (Ed.). International Joint Conferences on Artificial Intelligence Organization, 4812\u20134818. https://doi.org/10.24963/ijcai.2020/670 Survey track.",
      "doi": ""
    },
    {
      "text": "Harini Suresh, Steven\u00a0R. Gomez, Kevin\u00a0K. Nam, and Arvind Satyanarayan. 2021. Beyond Expertise and Roles: A Framework to Characterize the Stakeholders of Interpretable Machine Learning and Their Needs. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (Yokohama, Japan) (CHI \u201921). Association for Computing Machinery, New York, NY, USA, Article 74, 16\u00a0pages. https://doi.org/10.1145/3411764.3445088",
      "doi": "10.1145/3411764.3445088"
    },
    {
      "text": "J.\u00a0Eric\u00a0T. Taylor and Graham\u00a0W. Taylor. 2018. Artificial cognition: How experimental psychology can help generate explainable artificial intelligence. Psychonomic Bulletin & Review 28, 2 (2018), 454\u2013475. https://doi.org/10.3758/s13423-020-01825-5",
      "doi": ""
    },
    {
      "text": "The Cornell Lab of Ornithology. [n. d.]. Merlin Bird ID. https://merlin.allaboutbirds.org/",
      "doi": ""
    },
    {
      "text": "Richard Tomsett, Dave Braines, Dan Harborne, Alun\u00a0D. Preece, and Supriyo Chakraborty. 2018. Interpretable to Whom? A Role-based Model for Analyzing Interpretable Machine Learning Systems. CoRR abs/1806.07552(2018). arXiv:1806.07552http://arxiv.org/abs/1806.07552",
      "doi": ""
    },
    {
      "text": "Sana Tonekaboni, Shalmali Joshi, Melissa\u00a0D. McCradden, and Anna Goldenberg. 2019. What Clinicians Want: Contextualizing Explainable Machine Learning for Clinical End Use. In Proceedings of the 4th Machine Learning for Healthcare Conference(Proceedings of Machine Learning Research, Vol.\u00a0106), Finale Doshi-Velez, Jim Fackler, Ken Jung, David Kale, Rajesh Ranganath, Byron Wallace, and Jenna Wiens (Eds.). PMLR, 359\u2013380. https://proceedings.mlr.press/v106/tonekaboni19a.html",
      "doi": ""
    },
    {
      "text": "Philipp Tschandl, Christoph Rinner, Zoe Apalla, Giuseppe Argenziano, Noel Codella, Allan Halpern, Monika Janda, Aimilios Lallas, Caterina Longo, Josep Malvehy, John Paoli, Susana Puig, Cliff Rosendahl, H.\u00a0Peter Soyer, Iris Zalaudek, and Harald Kittler. 2020. Human\u2013computer collaboration for skin cancer recognition. Nature Medicine 26, 8 (2020), 1229\u20131234. https://doi.org/10.1038/s41591-020-0942-0",
      "doi": ""
    },
    {
      "text": "Simon Vandenhende, Dhruv Mahajan, Filip Radenovic, and Deepti Ghadiyaram. 2022. Making Heads or Tails: Towards Semantically Consistent Visual Counterfactuals. In European Conference on Computer Vision (ECCV).",
      "doi": "10.1007/978-3-031-19775-8_16"
    },
    {
      "text": "Valeria Villani, Fabio Pini, Francesco Leali, and Cristian Secchi. 2018. Survey on human\u2013robot collaboration in industrial settings: Safety, intuitive interfaces and applications. Mechatronics 55(2018), 248\u2013266. https://doi.org/10.1016/j.mechatronics.2018.02.009",
      "doi": ""
    },
    {
      "text": "Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. 2011. The Caltech-UCSD Birds-200-2011 dataset. Technical Report CNS-TR-2011-001. California Institute of Technology.",
      "doi": ""
    },
    {
      "text": "Dakuo Wang, Justin\u00a0D. Weisz, Michael Muller, Parikshit Ram, Werner Geyer, Casey Dugan, Yla Tausczik, Horst Samulowitz, and Alexander Gray. 2019. Human-AI Collaboration in Data Science: Exploring Data Scientists\u2019 Perceptions of Automated AI. Proc. ACM Hum.-Comput. Interact. 3, CSCW, Article 211 (nov 2019), 24\u00a0pages. https://doi.org/10.1145/3359313",
      "doi": "10.1145/3359313"
    },
    {
      "text": "Danding Wang, Qian Yang, Ashraf Abdul, and Brian\u00a0Y. Lim. 2019. Designing Theory-Driven User-Centric Explainable AI. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (Glasgow, Scotland Uk) (CHI \u201919). Association for Computing Machinery, New York, NY, USA, 1\u201315. https://doi.org/10.1145/3290605.3300831",
      "doi": "10.1145/3290605.3300831"
    },
    {
      "text": "Pei Wang and Nuno Vasconcelos. 2020. SCOUT: Self-Aware Discriminant Counterfactual Explanations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).",
      "doi": ""
    },
    {
      "text": "Guang Yang, Qinghao Ye, and Jun Xia. 2022. Unbox the black-box for the medical explainable AI via multi-modal and multi-centre data fusion: A mini-review, two showcases and beyond. Information Fusion 77(2022), 29\u201352. https://doi.org/10.1016/j.inffus.2021.07.016",
      "doi": "10.1016/j.inffus.2021.07.016"
    },
    {
      "text": "Chih-Kuan Yeh, Joon Kim, Ian En-Hsu Yen, and Pradeep\u00a0K Ravikumar. 2018. Representer Point Selection for Explaining Deep Neural Networks. In Neural Information Processing Systems (NeurIPS).",
      "doi": ""
    },
    {
      "text": "Ming Yin, Jennifer Wortman\u00a0Vaughan, and Hanna Wallach. 2019. Understanding the Effect of Accuracy on Trust in Machine Learning Models. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (Glasgow, Scotland Uk) (CHI \u201919). Association for Computing Machinery, New York, NY, USA, 1\u201312. https://doi.org/10.1145/3290605.3300509",
      "doi": "10.1145/3290605.3300509"
    },
    {
      "text": "Matthew\u00a0D Zeiler and Rob Fergus. 2014. Visualizing and understanding convolutional networks. In European Conference on Computer Vision (ECCV).",
      "doi": ""
    },
    {
      "text": "Wencan Zhang and Brian\u00a0Y Lim. 2022. Towards Relatable Explainable AI with the Perceptual Process. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems (New Orleans, LA, USA) (CHI \u201922). Association for Computing Machinery, New York, NY, USA, Article 181, 24\u00a0pages. https://doi.org/10.1145/3491102.3501826",
      "doi": "10.1145/3491102.3501826"
    },
    {
      "text": "Yunfeng Zhang, Q.\u00a0Vera Liao, and Rachel K.\u00a0E. Bellamy. 2020. Effect of Confidence and Explanation on Accuracy and Trust Calibration in AI-Assisted Decision Making. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (Barcelona, Spain) (FAT* \u201920). Association for Computing Machinery, New York, NY, USA, 295\u2013305. https://doi.org/10.1145/3351095.3372852",
      "doi": "10.1145/3351095.3372852"
    },
    {
      "text": "Yiming Zhang, Ying Weng, and Jonathan Lund. 2022. Applications of Explainable Artificial Intelligence in Diagnosis and Surgery. Diagnostics 12, 2 (2022). https://doi.org/10.3390/diagnostics12020237",
      "doi": ""
    },
    {
      "text": "Zheng Zhang, Ying Xu, Yanhao Wang, Bingsheng Yao, Daniel Ritchie, Tongshuang Wu, Mo Yu, Dakuo Wang, and Toby Jia-Jun Li. 2022. StoryBuddy: A Human-AI Collaborative Chatbot for Parent-Child Interactive Storytelling with Flexible Parental Involvement. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems (New Orleans, LA, USA) (CHI \u201922). Association for Computing Machinery, New York, NY, USA, Article 218, 21\u00a0pages. https://doi.org/10.1145/3491102.3517479",
      "doi": "10.1145/3491102.3517479"
    },
    {
      "text": "Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. 2016. Learning Deep Features for Discriminative Localization. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).",
      "doi": ""
    },
    {
      "text": "Bolei Zhou, Yiyou Sun, David Bau, and Antonio Torralba. 2018. Interpretable basis decomposition for visual explanation. In European Conference on Computer Vision (ECCV).",
      "doi": "10.1007/978-3-030-01237-3_8"
    }
  ]
}