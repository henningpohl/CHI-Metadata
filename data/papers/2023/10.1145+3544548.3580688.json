{
  "doi": "10.1145/3544548.3580688",
  "title": "Evaluating Large Language Models in Generating Synthetic HCI Research Data: a Case Study",
  "published": "2023-04-19",
  "proctitle": "CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems",
  "pages": "1-19",
  "year": 2023,
  "badges": [],
  "abstract": "Collecting data is one of the bottlenecks of Human-Computer Interaction (HCI) research. Motivated by this, we explore the potential of large language models (LLMs) in generating synthetic user research data. We use OpenAI\u2019s GPT-3 model to generate open-ended questionnaire responses about experiencing video games as art, a topic not tractable with traditional computational user models. We test whether synthetic responses can be distinguished from real responses, analyze errors of synthetic data, and investigate content similarities between synthetic and real data. We conclude that GPT-3 can, in this context, yield believable accounts of HCI experiences. Given the low cost and high speed of LLM data generation, synthetic data should be useful in ideating and piloting new experiments, although any findings must obviously always be validated with real data. The results also raise concerns: if employed by malicious users of crowdsourcing services, LLMs may make crowdsourcing of self-report data fundamentally unreliable.",
  "authors": [
    {
      "name": "Perttu H\u00e4m\u00e4l\u00e4inen",
      "institution": "Aalto University, Finland",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81100307069",
      "orcid": "0000-0001-7764-3459"
    },
    {
      "name": "Mikke Tavast",
      "institution": "Aalto University, Finland",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659474687",
      "orcid": "0000-0003-0671-5755"
    },
    {
      "name": "Anton Kunnari",
      "institution": "University of Helsinki, Finland",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99660049878",
      "orcid": "0000-0002-2951-6399"
    }
  ],
  "references": [
    {
      "text": "Alexander\u00a0L Anwyl-Irvine, Jessica Massonni\u00e9, Adam Flitton, Natasha Kirkham, and Jo\u00a0K Evershed. 2020. Gorilla in our midst: An online behavioral experiment builder. Behavior research methods 52, 1 (2020), 388\u2013407. https://doi.org/10.3758/s13428-019-01237-x",
      "doi": ""
    },
    {
      "text": "Lisa\u00a0P Argyle, Ethan\u00a0C Busby, Nancy Fulda, Joshua Gubler, Christopher Rytting, and David Wingate. 2022. Out of One, Many: Using Language Models to Simulate Human Samples. arXiv preprint arXiv:2209.06899(2022). https://doi.org/10.48550/arXiv.2209.06899",
      "doi": ""
    },
    {
      "text": "Javier\u00a0A. Bargas-Avila and Kasper Hornb\u00e6k. 2011. Old Wine in New Bottles or Novel Challenges: A Critical Analysis of Empirical Studies of User Experience. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. Association for Computing Machinery, New York, NY, USA, 2689\u20132698. https://doi.org/10.1145/1978942.1979336",
      "doi": "10.1145/1978942.1979336"
    },
    {
      "text": "Etienne Becht, Leland McInnes, John Healy, Charles-Antoine Dutertre, Immanuel\u00a0WH Kwok, Lai\u00a0Guan Ng, Florent Ginhoux, and Evan\u00a0W Newell. 2019. Dimensionality reduction for visualizing single-cell data using UMAP. Nature biotechnology 37, 1 (2019), 38\u201344. https://doi.org/10.1038/nbt.4314",
      "doi": ""
    },
    {
      "text": "Yoshua Bengio, Aaron Courville, and Pascal Vincent. 2013. Representation Learning: A Review and New Perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence 35, 8(2013), 1798\u20131828. https://doi.org/10.1109/TPAMI.2013.50",
      "doi": "10.1109/TPAMI.2013.50"
    },
    {
      "text": "Julia Bopp, Jan\u00a0Benjamin Vornhagen, Roosa Piitulainen, Barbara Keller, and Elisa\u00a0D. Mekler. 2020. GamesAsArt. (July 2020). https://doi.org/10.17605/OSF.IO/RYVT6 Publisher: OSF.",
      "doi": ""
    },
    {
      "text": "Julia\u00a0A. Bopp, Jan\u00a0B. Vornhagen, and Elisa\u00a0D. Mekler. 2021. \"My Soul Got a Little Bit Cleaner\": Art Experience in Videogames. Proc. ACM Hum.-Comput. Interact. 5, CHI PLAY, Article 237 (oct 2021), 19\u00a0pages. https://doi.org/10.1145/3474664",
      "doi": "10.1145/3474664"
    },
    {
      "text": "Gwern Branwen. 2020. GPT-3 creative fiction. (2020). https://www.gwern.net/GPT-3",
      "doi": ""
    },
    {
      "text": "Virginia Braun and Victoria Clarke. 2006. Using thematic analysis in psychology. Qualitative research in psychology 3, 2 (2006), 77\u2013101. https://doi.org/10.1191/1478088706qp063oa",
      "doi": ""
    },
    {
      "text": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared\u00a0D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Advances in Neural Information Processing Systems, H.\u00a0Larochelle, M.\u00a0Ranzato, R.\u00a0Hadsell, M.\u00a0F. Balcan, and H.\u00a0Lin (Eds.). Vol.\u00a033. Curran Associates, Inc., 1877\u20131901. https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf",
      "doi": ""
    },
    {
      "text": "Marc Brysbaert. 2019. How many words do we read per minute? A review and meta-analysis of reading rate. Journal of Memory and Language 109 (2019), 104047. https://doi.org/10.1016/j.jml.2019.104047",
      "doi": ""
    },
    {
      "text": "Erik Cambria and Bebo White. 2014. Jumping NLP Curves: A Review of Natural Language Processing Research [Review Article]. IEEE Computational Intelligence Magazine 9, 2 (2014), 48\u201357. https://doi.org/10.1109/MCI.2014.2307227",
      "doi": "10.1109/MCI.2014.2307227"
    },
    {
      "text": "Noshaba Cheema, Laura\u00a0A. Frey-Law, Kourosh Naderi, Jaakko Lehtinen, Philipp Slusallek, and Perttu H\u00e4m\u00e4l\u00e4inen. 2020. Predicting Mid-Air Interaction Movements and Fatigue Using Deep Reinforcement Learning. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI \u201920). Association for Computing Machinery, New York, NY, USA, 1\u201313. https://doi.org/10.1145/3313831.3376701",
      "doi": "10.1145/3313831.3376701"
    },
    {
      "text": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung\u00a0Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew\u00a0M. Dai, Thanumalayan\u00a0Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. PaLM: Scaling Language Modeling with Pathways. arxiv:2204.02311\u00a0[cs.CL]",
      "doi": ""
    },
    {
      "text": "Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2021. Transformers as Soft Reasoners over Language. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence (Yokohama, Yokohama, Japan) (IJCAI\u201920). Article 537, 9\u00a0pages. https://doi.org/10.24963/ijcai.2020/537",
      "doi": ""
    },
    {
      "text": "Matt Cox. 2019. This AI text adventure generator lets you do anything you\u00a0want. https://www.rockpapershotgun.com/this-ai-text-adventure-generator-lets-you-do-anything-you-want",
      "doi": ""
    },
    {
      "text": "Jonas Degrave. 2022. Building A Virtual Machine inside ChatGPT. https://www.engraved.blog/building-a-virtual-machine-inside/",
      "doi": ""
    },
    {
      "text": "Pieter Delobelle, Ewoenam Tokpo, Toon Calders, and Bettina Berendt. 2022. Measuring fairness with biased rulers: A comparative study on bias metrics for pre-trained language models. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 1693\u20131706. https://doi.org/10.18653/v1/2022.naacl-main.122",
      "doi": ""
    },
    {
      "text": "Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta. 2021. BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency(Virtual Event, Canada) (FAccT \u201921). Association for Computing Machinery, New York, NY, USA, 862\u2013872. https://doi.org/10.1145/3442188.3445924",
      "doi": "10.1145/3442188.3445924"
    },
    {
      "text": "Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong\u00a0Wook Kim, Alec Radford, and Ilya Sutskever. 2020. Jukebox: A generative model for music. arXiv preprint arXiv:2005.00341(2020). https://doi.org/10.48550/ARXIV.2005.00341",
      "doi": ""
    },
    {
      "text": "Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. 2021. A Mathematical Framework for Transformer Circuits. Transformer Circuits Thread(2021). https://transformer-circuits.pub/2021/framework/index.html.",
      "doi": ""
    },
    {
      "text": "Martin Ester, Hans-Peter Kriegel, J\u00f6rg Sander, and Xiaowei Xu. 1996. A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise. In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining (Portland, Oregon) (KDD\u201996). AAAI Press, 226\u2013231.",
      "doi": "10.5555/3001460.3001507"
    },
    {
      "text": "Paul\u00a0M Fitts. 1954. The information capacity of the human motor system in controlling the amplitude of movement.Journal of experimental psychology 47, 6 (1954), 381\u2013391.",
      "doi": ""
    },
    {
      "text": "Leo Gao. 2021. On the Sizes of OpenAI API Models.Retrieved 2022-09-12 from https://blog.eleuther.ai/gpt3-model-sizes/",
      "doi": ""
    },
    {
      "text": "Marybec Griffin, Richard\u00a0J Martino, Caleb LoSchiavo, Camilla Comer-Carruthers, Kristen\u00a0D Krause, Christopher\u00a0B Stults, and Perry\u00a0N Halkitis. 2022. Ensuring survey research data integrity in the era of internet bots. Quality & quantity 56, 4 (2022), 2841\u20132852. https://doi.org/10.1007/s11135-021-01252-1",
      "doi": ""
    },
    {
      "text": "Maarten Grootendorst. 2022. BERTopic: Neural topic modeling with a class-based TF-IDF procedure. arXiv preprint arXiv:2203.05794(2022). https://doi.org/10.48550/ARXIV.2203.05794",
      "doi": ""
    },
    {
      "text": "Perttu H\u00e4m\u00e4l\u00e4inen, Mikke Tavast, and Anton Kunnari. 2022. Neural Language Models as What If? -Engines for HCI Research. In 27th International Conference on Intelligent User Interfaces (Helsinki, Finland) (IUI \u201922 Companion). Association for Computing Machinery, New York, NY, USA, 77\u201380. https://doi.org/10.1145/3490100.3516458",
      "doi": "10.1145/3490100.3516458"
    },
    {
      "text": "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. 2017. GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium. In Advances in Neural Information Processing Systems, I.\u00a0Guyon, U.\u00a0Von Luxburg, S.\u00a0Bengio, H.\u00a0Wallach, R.\u00a0Fergus, S.\u00a0Vishwanathan, and R.\u00a0Garnett (Eds.). Vol.\u00a030. Curran Associates, Inc.https://proceedings.neurips.cc/paper/2017/file/8a1d694707eb0fefe65871369074926d-Paper.pdf",
      "doi": ""
    },
    {
      "text": "Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text degeneration. In International Conference on Learning Representations.",
      "doi": ""
    },
    {
      "text": "Jussi Jokinen, Aditya Acharya, Mohammad Uzair, Xinhui Jiang, and Antti Oulasvirta. 2021. Touchscreen Typing As Optimal Supervisory Control. In CHI \u201921: CHI Conference on Human Factors in Computing Systems, Virtual Event / Yokohama, Japan, May 8-13, 2021, Yoshifumi Kitamura, Aaron Quigley, Katherine Isbister, Takeo Igarashi, Pernille Bj\u00f8rn, and Steven\u00a0Mark Drucker (Eds.). ACM, 720:1\u2013720:14. https://doi.org/10.1145/3411764.3445483",
      "doi": "10.1145/3411764.3445483"
    },
    {
      "text": "Daniel Kahneman and Amos Tversky. 2013. Prospect theory: An analysis of decision under risk. In Handbook of the fundamentals of financial decision making: Part I. World Scientific, 99\u2013127.",
      "doi": ""
    },
    {
      "text": "Jared Kaplan, Sam McCandlish, Tom Henighan, Tom\u00a0B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361(2020). https://doi.org/10.48550/ARXIV.2001.08361",
      "doi": ""
    },
    {
      "text": "Takeshi Kojima, Shixiang\u00a0Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large Language Models are Zero-Shot Reasoners. In Advances in Neural Information Processing Systems. https://arxiv.org/abs/2205.11916",
      "doi": ""
    },
    {
      "text": "Per\u00a0Ola Kristensson. 2018. Statistical Language Processing for Text Entry. In Computational Interaction. Oxford University Press, 43\u201364.",
      "doi": ""
    },
    {
      "text": "Victor Kuperman, Aki-Juhani Kyr\u00f6l\u00e4inen, Vincent Porretta, Marc Brysbaert, and Sophia Yang. 2021. A lingering question addressed: Reading rate and most efficient listening rate are highly similar.Journal of Experimental Psychology: Human Perception and Performance 47, 8(2021), 1103\u20131112. https://doi.org/10.1037/xhp0000932",
      "doi": ""
    },
    {
      "text": "Mijin Kwon, Tor Wager, and Jonathan Phillips. 2022. Representations of emotion concepts: Comparison across pairwise, appraisal feature-based, and word embedding-based similarity spaces. In Proceedings of the Annual Meeting of the Cognitive Science Society, Vol.\u00a044.",
      "doi": ""
    },
    {
      "text": "Tuomas Kynk\u00e4\u00e4nniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. 2019. Improved Precision and Recall Metric for Assessing Generative Models. In Advances in Neural Information Processing Systems, H.\u00a0Wallach, H.\u00a0Larochelle, A.\u00a0Beygelzimer, F.\u00a0d'Alch\u00e9-Buc, E.\u00a0Fox, and R.\u00a0Garnett (Eds.). Vol.\u00a032. Curran Associates, Inc.https://proceedings.neurips.cc/paper/2019/file/0234c510bc6d908b28c70ff313743079-Paper.pdf",
      "doi": ""
    },
    {
      "text": "Nils\u00a0C. K\u00f6bis, Barbora Dole\u017ealov\u00e1, and Ivan Soraperra. 2021. Fooled twice: People cannot detect deepfakes but think they can. iScience 24, 11 (2021), 103364. https://doi.org/10.1016/j.isci.2021.103364",
      "doi": ""
    },
    {
      "text": "Guillaume Lample and Fran\u00e7ois Charton. 2019. Deep Learning For Symbolic Mathematics. In International Conference on Learning Representations.",
      "doi": ""
    },
    {
      "text": "Phuc\u00a0H. Le-Khac, Graham Healy, and Alan\u00a0F. Smeaton. 2020. Contrastive Representation Learning: A Framework and Review. IEEE Access 8(2020), 193907\u2013193934. https://doi.org/10.1109/ACCESS.2020.3031549",
      "doi": ""
    },
    {
      "text": "Paul\u00a0Pu Liang, Chiyu Wu, Louis-Philippe Morency, and Ruslan Salakhutdinov. 2021. Towards understanding and mitigating social biases in language models. In International Conference on Machine Learning. PMLR, 6565\u20136576.",
      "doi": ""
    },
    {
      "text": "Shusen Liu, Peer-Timo Bremer, Jayaraman\u00a0J Thiagarajan, Vivek Srikumar, Bei Wang, Yarden Livnat, and Valerio Pascucci. 2017. Visual exploration of semantic relationships in neural word embeddings. IEEE transactions on visualization and computer graphics 24, 1(2017), 553\u2013562.",
      "doi": ""
    },
    {
      "text": "Ziming Liu, Ouail Kitouni, Niklas Nolte, Eric\u00a0J Michaud, Max Tegmark, and Mike Williams. 2022. Towards Understanding Grokking: An Effective Theory of Representation Learning. In Advances in Neural Information Processing Systems.",
      "doi": ""
    },
    {
      "text": "R\u00f3is\u00edn Loughran and Michael O\u2019Neill. 2017. Application Domains Considered in Computational Creativity.. In ICCC. 197\u2013204.",
      "doi": ""
    },
    {
      "text": "Kevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch. 2022. Frozen Pretrained Transformers as Universal Computation Engines. In Proc. AAAI 2022. 7628\u20137636. https://doi.org/10.1609/aaai.v36i7.20729",
      "doi": ""
    },
    {
      "text": "I\u00a0Scott MacKenzie and William Buxton. 1992. Extending Fitts\u2019 law to two-dimensional tasks. In Proceedings of the SIGCHI conference on Human factors in computing systems. 219\u2013226.",
      "doi": "10.1145/142750.142794"
    },
    {
      "text": "Neil\u00a0A. Macmillan. 2005. Detection theory : a user\u2019s guide(2nd ed.). Lawrence Erlbaum Associates, Mahwah, N.J.",
      "doi": ""
    },
    {
      "text": "Ali Madani, Bryan McCann, Nikhil Naik, Nitish\u00a0Shirish Keskar, Namrata Anand, Raphael\u00a0R Eguchi, Po-Ssu Huang, and Richard Socher. 2020. Progen: Language modeling for protein generation. arXiv preprint arXiv:2004.03497(2020).",
      "doi": ""
    },
    {
      "text": "Leland McInnes, John Healy, and Steve Astels. 2017. hdbscan: Hierarchical density based clustering.J. Open Source Softw. 2, 11 (2017), 205.",
      "doi": ""
    },
    {
      "text": "Leland McInnes, John Healy, and James Melville. 2018. Umap: Uniform manifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426(2018).",
      "doi": ""
    },
    {
      "text": "Tom\u00e1\u0161 Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic regularities in continuous space word representations. In Proceedings of the 2013 conference of the north american chapter of the association for computational linguistics: Human language technologies. 746\u2013751.",
      "doi": ""
    },
    {
      "text": "Guido\u00a0F Montufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. 2014. On the Number of Linear Regions of Deep Neural Networks. Advances in Neural Information Processing Systems 27 (2014), 2924\u20132932.",
      "doi": "10.5555/2969033.2969153"
    },
    {
      "text": "Sophie\u00a0J. Nightingale and Hany Farid. 2022. AI-synthesized faces are indistinguishable from real faces and more trustworthy. Proceedings of the National Academy of Sciences 119, 8 (2022), e2120481119. https://doi.org/10.1073/pnas.2120481119 arXiv:https://www.pnas.org/doi/pdf/10.1073/pnas.2120481119",
      "doi": ""
    },
    {
      "text": "Maxwell Nye, Michael Tessler, Josh Tenenbaum, and Brenden\u00a0M Lake. 2021. Improving Coherence and Consistency in Neural Sequence Models with Dual-System, Neuro-Symbolic Reasoning. In Advances in Neural Information Processing Systems, M.\u00a0Ranzato, A.\u00a0Beygelzimer, Y.\u00a0Dauphin, P.S. Liang, and J.\u00a0Wortman Vaughan (Eds.). Vol.\u00a034. Curran Associates, Inc., 25192\u201325204. https://proceedings.neurips.cc/paper/2021/file/d3e2e8f631bd9336ed25b8162aef8782-Paper.pdf",
      "doi": ""
    },
    {
      "text": "Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. 2022. In-context Learning and Induction Heads. Transformer Circuits Thread(2022). https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.",
      "doi": ""
    },
    {
      "text": "OpenAI. 2022. ChatGPT: Optimizing Language Models for Dialogue. https://openai.com/blog/chatgpt/",
      "doi": ""
    },
    {
      "text": "Antti Oulasvirta. 2019. It\u2019s time to rediscover HCI models. Interactions 26, 4 (2019), 52\u201356. https://doi.org/10.1145/3330340",
      "doi": "10.1145/3330340"
    },
    {
      "text": "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll\u00a0L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. https://doi.org/10.48550/ARXIV.2203.02155",
      "doi": ""
    },
    {
      "text": "Joon\u00a0Sung Park, Lindsay Popowski, Carrie Cai, Meredith\u00a0Ringel Morris, Percy Liang, and Michael\u00a0S Bernstein. 2022. Social Simulacra: Creating Populated Prototypes for Social Computing Systems. In Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology. 1\u201318.",
      "doi": "10.1145/3526113.3545616"
    },
    {
      "text": "Jorge P\u00e9rez, Pablo Barcel\u00f3, and Javier Marinkovic. 2021. Attention is Turing-Complete.J. Mach. Learn. Res. 22, 75 (2021), 1\u201335.",
      "doi": ""
    },
    {
      "text": "Ingrid Pettersson, Florian Lachner, Anna-Katharina Frison, Andreas Riener, and Andreas Butz. 2018. A Bermuda Triangle? A Review of Method Application and Triangulation in User Experience Evaluation. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems. Association for Computing Machinery, New York, NY, USA, 1\u201316. https://doi.org/10.1145/3173574.3174035",
      "doi": "10.1145/3173574.3174035"
    },
    {
      "text": "Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. 2016. Exponential expressivity in deep neural networks through transient chaos. Advances in neural information processing systems 29 (2016), 3360\u20133368.",
      "doi": ""
    },
    {
      "text": "Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. 2022. Grokking: Generalization beyond overfitting on small algorithmic datasets. arXiv preprint arXiv:2201.02177(2022).",
      "doi": ""
    },
    {
      "text": "Alec Radford, Rafal Jozefowicz, and Ilya Sutskever. 2017. Learning to generate reviews and discovering sentiment. arXiv preprint arXiv:1704.01444(2017).",
      "doi": ""
    },
    {
      "text": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, 2019. Language models are unsupervised multitask learners. OpenAI blog 1, 8 (2019), 9.",
      "doi": ""
    },
    {
      "text": "Jack\u00a0W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van\u00a0den Driessche, Lisa\u00a0Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang\u00a0Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson\u00a0d\u2019Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las\u00a0Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. 2021. Scaling Language Models: Methods, Analysis & Insights from Training Gopher. arxiv:2112.11446\u00a0[cs.CL]",
      "doi": ""
    },
    {
      "text": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter\u00a0J. Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. J. Mach. Learn. Res. 21(2020), 140:1\u2013140:67. http://jmlr.org/papers/v21/20-074.html",
      "doi": ""
    },
    {
      "text": "Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. Zero-Shot Text-to-Image Generation. In Proceedings of the 38th International Conference on Machine Learning(Proceedings of Machine Learning Research, Vol.\u00a0139), Marina Meila and Tong Zhang (Eds.). PMLR, 8821\u20138831. https://proceedings.mlr.press/v139/ramesh21a.html",
      "doi": ""
    },
    {
      "text": "Shaghayegh Roohi, Asko Relas, Jari Takatalo, Henri Heiskanen, and Perttu H\u00e4m\u00e4l\u00e4inen. 2020. Predicting Game Difficulty and Churn Without Players. In CHI PLAY \u201920: The Annual Symposium on Computer-Human Interaction in Play, Virtual Event, Canada, November 2-4, 2020, Pejman Mirza-Babaei, Victoria McArthur, Vero\u00a0Vanden Abeele, and Max Birk (Eds.). ACM, 585\u2013593. https://doi.org/10.1145/3410404.3414235",
      "doi": "10.1145/3410404.3414235"
    },
    {
      "text": "Ronald Rosenfeld. 2000. Two decades of statistical language modeling: Where do we go from here?Proc. IEEE 88, 8 (2000), 1270\u20131278.",
      "doi": ""
    },
    {
      "text": "Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Scao, Arun Raja, 2022. Multitask Prompted Training Enables Zero-Shot Task Generalization. (2022).",
      "doi": ""
    },
    {
      "text": "Jeff Sauro and James\u00a0R Lewis. 2016. Quantifying the user experience: Practical statistics for user research. Morgan Kaufmann.",
      "doi": ""
    },
    {
      "text": "Stanislau Semeniuta, Aliaksei Severyn, and Sylvain Gelly. 2018. On Accurate Evaluation of GANs for Language Generation. https://doi.org/10.48550/ARXIV.1806.04936",
      "doi": ""
    },
    {
      "text": "Tom Simonite. 2021. It Began as an AI-Fueled Dungeon Game. It Got Much Darker.https://www.wired.com/story/ai-fueled-dungeon-game-got-much-darker/",
      "doi": ""
    },
    {
      "text": "Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza\u00a0Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, and Bryan Catanzaro. 2022. Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model. arxiv:2201.11990\u00a0[cs.CL]",
      "doi": ""
    },
    {
      "text": "Mikke Tavast, Anton Kunnari, and Perttu H\u00e4m\u00e4l\u00e4inen. 2022. Language Models Can Generate Human-Like Self-Reports of Emotion. In 27th International Conference on Intelligent User Interfaces (Helsinki, Finland) (IUI \u201922 Companion). Association for Computing Machinery, New York, NY, USA, 69\u201372. https://doi.org/10.1145/3490100.3516464",
      "doi": "10.1145/3490100.3516464"
    },
    {
      "text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan\u00a0N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems. 5998\u20136008.",
      "doi": ""
    },
    {
      "text": "Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, 2022. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682(2022).",
      "doi": ""
    },
    {
      "text": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of Thought Prompting Elicits Reasoning in Large Language Models. https://doi.org/10.48550/ARXIV.2201.11903",
      "doi": ""
    },
    {
      "text": "Gui-Rong Xue, Jie Han, Yong Yu, and Qiang Yang. 2009. User language model for collaborative personalized search. ACM Transactions on Information Systems (TOIS) 27, 2 (2009), 1\u201328.",
      "doi": "10.1145/1462198.1462203"
    },
    {
      "text": "Qinyuan Ye, Bill\u00a0Yuchen Lin, and Xiang Ren. 2021. Crossfit: A few-shot learning challenge for cross-task generalization in nlp. arXiv preprint arXiv:2104.08835(2021).",
      "doi": ""
    },
    {
      "text": "Camilla Zallot, Gabriele Paolacci, Jesse Chandler, and Itay Sisso. 2021. Crowdsourcing in observational and experimental research. Handbook of Computational Social Science, Volume 2: Data Science, Statistical Modelling, and Machine Learning Methods(2021), 140\u2013157.",
      "doi": ""
    },
    {
      "text": "Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate Before Use: Improving Few-shot Performance of Language Models. In Proceedings of the 38th International Conference on Machine Learning(Proceedings of Machine Learning Research, Vol.\u00a0139), Marina Meila and Tong Zhang (Eds.). PMLR, 12697\u201312706. https://proceedings.mlr.press/v139/zhao21c.html",
      "doi": ""
    }
  ]
}