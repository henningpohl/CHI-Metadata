{
  "doi": "10.1145/3544548.3580801",
  "title": "EchoSpeech: Continuous Silent Speech Recognition on Minimally-obtrusive Eyewear Powered by Acoustic Sensing",
  "published": "2023-04-19",
  "proctitle": "CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems",
  "pages": "1-18",
  "year": 2023,
  "badges": [],
  "abstract": "We present EchoSpeech, a minimally-obtrusive silent speech interface (SSI) powered by low-power active acoustic sensing. EchoSpeech uses speakers and microphones mounted on a glass-frame and emits inaudible sound waves towards the skin. By analyzing echos from multiple paths, EchoSpeech captures subtle skin deformations caused by silent utterances and uses them to infer silent speech. With a user study of 12 participants, we demonstrate that EchoSpeech can recognize 31 isolated commands and 3-6 figure connected digits with 4.5% (std 3.5%) and 6.1% (std 4.2%) Word Error Rate (WER), respectively. We further evaluated EchoSpeech under scenarios including walking and noise injection to test its robustness. We then demonstrated using EchoSpeech in demo applications in real-time operating at 73.3mW, where the real-time pipeline was implemented on a smartphone with only 1-6 minutes of training data. We believe that EchoSpeech takes a solid step towards minimally-obtrusive wearable SSI for real-life deployment.",
  "authors": [
    {
      "name": "Ruidong Zhang",
      "institution": "Information Science, Cornell University, United States",
      "img": "/do/10.1145/contrib-99659765664/rel-imgonly/profile_square_zoomin.jpg",
      "acmid": "99659765664",
      "orcid": "0000-0001-8329-0522"
    },
    {
      "name": "Ke Li",
      "institution": "Information Science, Cornell University, United States",
      "img": "/do/10.1145/contrib-99659453312/rel-imgonly/profile_photo.jpg",
      "acmid": "99659453312",
      "orcid": "0000-0002-4208-7904"
    },
    {
      "name": "Yihong Hao",
      "institution": "Computer Science, Cornell University, United States",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99660782100",
      "orcid": "0000-0002-7047-3917"
    },
    {
      "name": "Yufan Wang",
      "institution": "Information Science, Cornell University, United States",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99660784039",
      "orcid": "0000-0001-8609-7066"
    },
    {
      "name": "Zhengnan Lai",
      "institution": "Information Science, Cornell University, United States",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99660782874",
      "orcid": "0000-0002-8933-7296"
    },
    {
      "name": "Fran\u00e7ois Guimbreti\u00e8re",
      "institution": "Information Science, Cornell University, United States",
      "img": "/do/10.1145/contrib-81100301845/rel-imgonly/81100301845.jpg",
      "acmid": "81100301845",
      "orcid": "0000-0002-5510-6799"
    },
    {
      "name": "Cheng Zhang",
      "institution": "Information Science, Cornell University, United States",
      "img": "/do/10.1145/contrib-81488647680/rel-imgonly/cheng_pic.jpeg",
      "acmid": "81488647680",
      "orcid": "0000-0002-5079-5927"
    }
  ],
  "references": [
    {
      "text": "Hassan Akbari, Himani Arora, Liangliang Cao, and Nima Mesgarani. 2018. Lip2Audspec: Speech Reconstruction from Silent Lip Movements Video. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 2516\u20132520. https://doi.org/10.1109/ICASSP.2018.8461856",
      "doi": "10.1109/ICASSP.2018.8461856"
    },
    {
      "text": "Najwa Alghamdi, Steve Maddock, Ricard Marxer, Jon Barker, and Guy\u00a0J. Brown. 2018. A corpus of audio-visual Lombard speech with frontal and profile views. The Journal of the Acoustical Society of America 143, 6 (2018), EL523\u2013EL529. https://doi.org/10.1121/1.5042758 arXiv:https://doi.org/10.1121/1.5042758",
      "doi": ""
    },
    {
      "text": "Yannis\u00a0M. Assael, Brendan Shillingford, Shimon Whiteson, and Nando de Freitas. 2016. LipNet: Sentence-level Lipreading. CoRR abs/1611.01599(2016). arXiv:1611.01599http://arxiv.org/abs/1611.01599",
      "doi": ""
    },
    {
      "text": "Abdelkareem Bedri, Himanshu Sahni, Pavleen Thukral, Thad Starner, David Byrd, Peter Presti, Gabriel Reyes, Maysam Ghovanloo, and Zehua Guo. 2015. Toward Silent-Speech Control of Consumer Wearables. Computer 48, 10 (2015), 54\u201362. https://doi.org/10.1109/MC.2015.310",
      "doi": "10.1109/MC.2015.310"
    },
    {
      "text": "Linnar Billman and Johan Hullberg. 2018. Speech Reading with Deep Neural Networks.",
      "doi": ""
    },
    {
      "text": "Lam\u00a0A. Cheah., James\u00a0M. Gilbert., Jose\u00a0A. Gonzalez., Phil\u00a0D. Green., Stephen R. Ell., Roger K. Moore., and Ed Holdsworth.2018. A Wearable Silent Speech Interface based on Magnetic Sensors with Motion-Artefact Removal. In Proceedings of the 11th International Joint Conference on Biomedical Engineering Systems and Technologies - BIODEVICES,. INSTICC, SciTePress, 56\u201362. https://doi.org/10.5220/0006573200560062",
      "doi": ""
    },
    {
      "text": "Tuochao Chen, Benjamin Steeper, Kinan Alsheikh, Songyun Tao, Fran\u00e7ois Guimbreti\u00e8re, and Cheng Zhang. 2020. C-Face: Continuously Reconstructing Facial Expressions by Deep Learning Contours of the Face with Ear-Mounted Miniature Cameras. In Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology (Virtual Event, USA) (UIST \u201920). Association for Computing Machinery, New York, NY, USA, 112\u2013125. https://doi.org/10.1145/3379337.3415879",
      "doi": "10.1145/3379337.3415879"
    },
    {
      "text": "J Chung and A Zisserman. 2017. Lip reading in profile. British Machine Vision Conference, 2017(2017).",
      "doi": ""
    },
    {
      "text": "Joon\u00a0Son Chung and Andrew Zisserman. 2017. Lip Reading in the Wild. In Computer Vision \u2013 ACCV 2016, Shang-Hong Lai, Vincent Lepetit, Ko\u00a0Nishino, and Yoichi Sato (Eds.). Springer International Publishing, Cham, 87\u2013103.",
      "doi": ""
    },
    {
      "text": "Martin Cooke, Jon Barker, Stuart Cunningham, and Xu Shao. 2006. An audio-visual corpus for speech perception and automatic speech recognition. The Journal of the Acoustical Society of America 120, 5 (2006), 2421\u20132424. https://doi.org/10.1121/1.2229005 arXiv:https://doi.org/10.1121/1.2229005",
      "doi": ""
    },
    {
      "text": "Thomas\u00a0Le Cornu and Ben Milner. 2015. Reconstructing intelligible audio speech from visual speech features. In sixteenth annual conference of the international speech communication association.",
      "doi": ""
    },
    {
      "text": "Tam\u00e1s\u00a0G\u00e1bor Csap\u00f3, Tam\u00e1s Gr\u00f3sz, G\u00e1bor Gosztolya, L\u00e1szl\u00f3 T\u00f3th, and Alexandra Mark\u00f3. 2017. DNN-Based Ultrasound-to-Speech Conversion for a Silent Speech Interface. In Proc. Interspeech 2017. 3672\u20133676. https://doi.org/10.21437/Interspeech.2017-939",
      "doi": ""
    },
    {
      "text": "B. Denby, Y. Oussar, G. Dreyfus, and M. Stone. 2006. Prospects for a Silent Speech Interface using Ultrasound Imaging. In 2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings, Vol.\u00a01. I\u2013I. https://doi.org/10.1109/ICASSP.2006.1660033",
      "doi": ""
    },
    {
      "text": "Ariel Ephrat and Shmuel Peleg. 2017. Vid2speech: Speech reconstruction from silent video. In 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 5095\u20135099. https://doi.org/10.1109/ICASSP.2017.7953127",
      "doi": "10.1109/ICASSP.2017.7953127"
    },
    {
      "text": "Ivan Fung and Brian Mak. 2018. End-To-End Low-Resource Lip-Reading with Maxout Cnn and Lstm. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 2511\u20132515. https://doi.org/10.1109/ICASSP.2018.8462280",
      "doi": "10.1109/ICASSP.2018.8462280"
    },
    {
      "text": "Yang Gao, Yincheng Jin, Jiyang Li, Seokmin Choi, and Zhanpeng Jin. 2020. EchoWhisper: Exploring an Acoustic-Based Silent Speech Interface for Smartphone Users. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 4, 3, Article 80 (sep 2020), 27\u00a0pages. https://doi.org/10.1145/3411830",
      "doi": "10.1145/3411830"
    },
    {
      "text": "Amit Garg, Jonathan Noyola, and Sameep Bagadia. 2016. Lip reading using CNN and LSTM. Technical report, Stanford University, CS231 n project report (2016).",
      "doi": ""
    },
    {
      "text": "Jose\u00a0A. Gonzalez, Lam\u00a0A. Cheah, Angel\u00a0M. Gomez, Phil\u00a0D. Green, James\u00a0M. Gilbert, Stephen\u00a0R. Ell, Roger\u00a0K. Moore, and Ed Holdsworth. 2017. Direct Speech Reconstruction From Articulatory Sensor Data by Machine Learning. IEEE/ACM Transactions on Audio, Speech, and Language Processing 25, 12(2017), 2362\u20132374. https://doi.org/10.1109/TASLP.2017.2757263",
      "doi": "10.1109/TASLP.2017.2757263"
    },
    {
      "text": "Naomi Harte and Eoin Gillen. 2015. TCD-TIMIT: An Audio-Visual Corpus of Continuous Speech. IEEE Transactions on Multimedia 17, 5 (2015), 603\u2013615. https://doi.org/10.1109/TMM.2015.2407694",
      "doi": "10.1109/TMM.2015.2407694"
    },
    {
      "text": "Hirotaka Hiraki and Jun Rekimoto. 2021. SilentMask: Mask-Type Silent Speech Interface with Measurement of Mouth Movement. In Augmented Humans Conference 2021 (Rovaniemi, Finland) (AHs\u201921). Association for Computing Machinery, New York, NY, USA, 86\u201390. https://doi.org/10.1145/3458709.3458985",
      "doi": "10.1145/3458709.3458985"
    },
    {
      "text": "Robin Hofe, Stephen\u00a0R. Ell, Michael\u00a0J. Fagan, James\u00a0M. Gilbert, Phil\u00a0D. Green, Roger\u00a0K. Moore, and Sergey\u00a0I. Rybchenko. 2013. Small-vocabulary speech recognition using a silent speech interface based on magnetic sensing. Speech Communication 55, 1 (2013), 22\u201332. https://doi.org/10.1016/j.specom.2012.02.001",
      "doi": "10.1016/j.specom.2012.02.001"
    },
    {
      "text": "Carl\u00a0Q Howard, Colin\u00a0H Hansen, and Anthony\u00a0C Zander. 2005. A review of current ultrasound exposure limits. The Journal of Occupational Health and Safety of Australia and New Zealand 21, 3(2005), 253\u2013257.",
      "doi": ""
    },
    {
      "text": "Yuya Igarashi, Kyosuke Futami, and Kazuya Murao. 2022. Silent Speech Eyewear Interface: Silent Speech Recognition Method Using Eyewear with Infrared Distance Sensors. (2022), 33\u201338. https://doi.org/10.1145/3544794.3558458",
      "doi": "10.1145/3544794.3558458"
    },
    {
      "text": "Yan Ji, Licheng Liu, Hongcui Wang, Zhilei Liu, Zhibin Niu, and Bruce Denby. 2018. Updating the Silent Speech Challenge benchmark with deep learning. Speech Communication 98(2018), 42\u201350. https://doi.org/10.1016/j.specom.2018.02.002",
      "doi": "10.1016/j.specom.2018.02.002"
    },
    {
      "text": "Yincheng Jin, Yang Gao, Xuhai Xu, Seokmin Choi, Jiyang Li, Feng Liu, Zhengxiong Li, and Zhanpeng Jin. 2022. EarCommand: \"Hearing\" Your Silent Speech Commands In Ear. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 6, 2, Article 57 (jul 2022), 28\u00a0pages. https://doi.org/10.1145/3534613",
      "doi": "10.1145/3534613"
    },
    {
      "text": "Yincheng Jin, Yang Gao, Yanjun Zhu, Wei Wang, Jiyang Li, Seokmin Choi, Zhangyu Li, Jagmohan Chauhan, Anind\u00a0K. Dey, and Zhanpeng Jin. 2021. SonicASL: An Acoustic-Based Sign Language Gesture Recognizer Using Earphones. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 5, 2, Article 67 (jun 2021), 30\u00a0pages. https://doi.org/10.1145/3463519",
      "doi": "10.1145/3463519"
    },
    {
      "text": "Eloi\u00a0Moliner Juanpere and Tam\u00e1s\u00a0G\u00e1bor Csap\u00f3. 2019. Ultrasound-Based Silent Speech Interface Using Convolutional and Recurrent Neural Networks. Acta Acustica united with Acustica 105, 4 (2019), 587\u2013590.",
      "doi": ""
    },
    {
      "text": "Arnav Kapur, Shreyas Kapur, and Pattie Maes. 2018. AlterEgo: A Personalized Wearable Silent Speech Interface. In 23rd International Conference on Intelligent User Interfaces (Tokyo, Japan) (IUI \u201918). Association for Computing Machinery, New York, NY, USA, 43\u201353. https://doi.org/10.1145/3172944.3172977",
      "doi": "10.1145/3172944.3172977"
    },
    {
      "text": "Arnav Kapur, Utkarsh Sarawgi, Eric Wadkins, Matthew Wu, Nora Hollenstein, and Pattie Maes. 2020. Non-Invasive Silent Speech Recognition in Multiple Sclerosis with Dysphonia. In Proceedings of the Machine Learning for Health NeurIPS Workshop(Proceedings of Machine Learning Research, Vol.\u00a0116), Adrian\u00a0V. Dalca, Matthew\u00a0B.A. McDermott, Emily Alsentzer, Samuel\u00a0G. Finlayson, Michael Oberst, Fabian Falck, and Brett Beaulieu-Jones (Eds.). PMLR, 25\u201338. https://proceedings.mlr.press/v116/kapur20a.html",
      "doi": ""
    },
    {
      "text": "Myungjong Kim, Beiming Cao, Ted Mau, and Jun Wang. 2017. Speaker-Independent Silent Speech Recognition From Flesh-Point Articulatory Movements Using an LSTM Neural Network. IEEE/ACM Transactions on Audio, Speech, and Language Processing 25, 12(2017), 2323\u20132336. https://doi.org/10.1109/TASLP.2017.2758999",
      "doi": "10.1109/TASLP.2017.2758999"
    },
    {
      "text": "Myungjong Kim, Nordine Sebkhi, Beiming Cao, Maysam Ghovanloo, and Jun Wang. 2018. Preliminary Test of a Wireless Magnetic Tongue Tracking System for Silent Speech Interface. In 2018 IEEE Biomedical Circuits and Systems Conference (BioCAS). 1\u20134. https://doi.org/10.1109/BIOCAS.2018.8584786",
      "doi": ""
    },
    {
      "text": "Naoki Kimura, Tan Gemicioglu, Jonathan Womack, Richard Li, Yuhui Zhao, Abdelkareem Bedri, Alex Olwal, Jun Rekimoto, and Thad Starner. 2021. Mobile, Hands-Free, Silent Speech Texting Using SilentSpeller. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems (Yokohama, Japan) (CHI EA \u201921). Association for Computing Machinery, New York, NY, USA, Article 178, 5\u00a0pages. https://doi.org/10.1145/3411763.3451552",
      "doi": "10.1145/3411763.3451552"
    },
    {
      "text": "Naoki Kimura, Tan Gemicioglu, Jonathan Womack, Richard Li, Yuhui Zhao, Abdelkareem Bedri, Zixiong Su, Alex Olwal, Jun Rekimoto, and Thad Starner. 2022. SilentSpeller: Towards Mobile, Hands-Free, Silent Speech Text Entry Using Electropalatography. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems (New Orleans, LA, USA) (CHI \u201922). Association for Computing Machinery, New York, NY, USA, Article 288, 19\u00a0pages. https://doi.org/10.1145/3491102.3502015",
      "doi": "10.1145/3491102.3502015"
    },
    {
      "text": "Naoki Kimura, Kentaro Hayashi, and Jun Rekimoto. 2020. TieLent: A Casual Neck-Mounted Mouth Capturing Device for Silent Speech Interaction. In Proceedings of the International Conference on Advanced Visual Interfaces (Salerno, Italy) (AVI \u201920). Association for Computing Machinery, New York, NY, USA, Article 33, 8\u00a0pages. https://doi.org/10.1145/3399715.3399852",
      "doi": "10.1145/3399715.3399852"
    },
    {
      "text": "Naoki Kimura, Michinari Kono, and Jun Rekimoto. 2019. SottoVoce: An Ultrasound Imaging-Based Silent Speech Interaction Using Deep Neural Networks. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (Glasgow, Scotland Uk) (CHI \u201919). Association for Computing Machinery, New York, NY, USA, 1\u201311. https://doi.org/10.1145/3290605.3300376",
      "doi": "10.1145/3290605.3300376"
    },
    {
      "text": "Alexandros Koumparoulis, Gerasimos Potamianos, Youssef Mroueh, and Steven\u00a0J. Rennie. 2017. Exploring ROI size in deep learning based lipreading. In Proc. The 14th International Conference on Auditory-Visual Speech Processing. 64\u201369. https://doi.org/10.21437/AVSP.2017-13",
      "doi": ""
    },
    {
      "text": "Yusuke Kunimi, Masa Ogata, Hirotaka Hiraki, Motoshi Itagaki, Shusuke Kanazawa, and Masaaki Mochimaru. 2022. E-MASK: A Mask-Shaped Interface for Silent Speech Interaction with Flexible Strain Sensors. In Augmented Humans 2022. Association for Computing Machinery, New York, NY, USA, 26\u201334. https://doi.org/10.1145/3519391.3519399",
      "doi": "10.1145/3519391.3519399"
    },
    {
      "text": "Ke Li, Ruidong Zhang, Bo Liang, Fran\u00e7ois Guimbreti\u00e8re, and Cheng Zhang. 2022. EarIO: A Low-Power Acoustic Sensing Earable for Continuously Tracking Detailed Facial Movements. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 6, 2, Article 62 (jul 2022), 24\u00a0pages. https://doi.org/10.1145/3534621",
      "doi": "10.1145/3534621"
    },
    {
      "text": "Richard Li, Jason Wu, and Thad Starner. 2019. TongueBoard: An Oral Interface for Subtle Input. In Proceedings of the 10th Augmented Human International Conference 2019 (Reims, France) (AH2019). Association for Computing Machinery, New York, NY, USA, Article 1, 9\u00a0pages. https://doi.org/10.1145/3311823.3311831",
      "doi": "10.1145/3311823.3311831"
    },
    {
      "text": "Jian Luo, Jianzong Wang, Ning Cheng, Guilin Jiang, and Jing Xiao. 2021. End-To-End Silent Speech Recognition with Acoustic Sensing. In 2021 IEEE Spoken Language Technology Workshop (SLT). 606\u2013612. https://doi.org/10.1109/SLT48900.2021.9383622",
      "doi": ""
    },
    {
      "text": "Hiroyuki Manabe, Akira Hiraiwa, and Toshiaki Sugimura. 2003. \"Unvoiced Speech Recognition Using EMG - Mime Speech Recognition\". In CHI \u201903 Extended Abstracts on Human Factors in Computing Systems (Ft. Lauderdale, Florida, USA) (CHI EA \u201903). Association for Computing Machinery, New York, NY, USA, 794\u2013795. https://doi.org/10.1145/765891.765996",
      "doi": "10.1145/765891.765996"
    },
    {
      "text": "Geoffrey\u00a0S Meltzner, James\u00a0T Heaton, Yunbin Deng, Gianluca\u00a0De Luca, Serge\u00a0H Roy, and Joshua\u00a0C Kline. 2018. Development of sEMG sensors and algorithms for silent speech recognition. Journal of Neural Engineering 15, 4 (jun 2018), 046031. https://doi.org/10.1088/1741-2552/aac965",
      "doi": ""
    },
    {
      "text": "Daniel Michelsanti, Olga Slizovskaia, Gloria Haro, Emilia G\u00f3mez, Zheng-Hua Tan, and Jesper Jensen. 2020. Vocoder-Based Speech Synthesis from Silent Videos. (2020). https://doi.org/10.48550/ARXIV.2004.02541",
      "doi": ""
    },
    {
      "text": "Rodrigo Mira, Konstantinos Vougioukas, Pingchuan Ma, Stavros Petridis, Bj\u00f6rn\u00a0W. Schuller, and Maja Pantic. 2022. End-to-End Video-to-Speech Synthesis Using Generative Adversarial Networks. IEEE Transactions on Cybernetics(2022), 1\u201313. https://doi.org/10.1109/TCYB.2022.3162495",
      "doi": ""
    },
    {
      "text": "Laxmi Pandey and Ahmed\u00a0Sabbir Arif. 2021. LipType: A Silent Speech Recognizer Augmented with an Independent Repair Model. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (Yokohama, Japan) (CHI \u201921). Association for Computing Machinery, New York, NY, USA, Article 1, 19\u00a0pages. https://doi.org/10.1145/3411764.3445565",
      "doi": "10.1145/3411764.3445565"
    },
    {
      "text": "Laxmi Pandey, Khalad Hasan, and Ahmed\u00a0Sabbir Arif. 2021. Acceptability of Speech and Silent Speech Input Methods in Private and Public. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (Yokohama, Japan) (CHI \u201921). Association for Computing Machinery, New York, NY, USA, Article 251, 13\u00a0pages. https://doi.org/10.1145/3411764.3445430",
      "doi": "10.1145/3411764.3445430"
    },
    {
      "text": "Laxmi Pandey and Ahmed Sabbir\u00a0Arif. 2021. Silent Speech and Emotion Recognition from Vocal Tract Shape Dynamics in Real-Time MRI. In ACM SIGGRAPH 2021 Posters. Association for Computing Machinery, New York, NY, USA. https://doi.org/10.1145/3450618.3469176",
      "doi": "10.1145/3450618.3469176"
    },
    {
      "text": "E.K. Patterson, S. Gurbuz, Z. Tufekci, and J.N. Gowdy. 2002. CUAVE: A new audio-visual database for multimodal human-computer interface research. In 2002 IEEE International Conference on Acoustics, Speech, and Signal Processing, Vol.\u00a02. II\u20132017\u2013II\u20132020. https://doi.org/10.1109/ICASSP.2002.5745028",
      "doi": ""
    },
    {
      "text": "Ahmed Rekik, Achraf Ben-Hamadou, and Walid Mahdi. 2014. A New Visual Speech Recognition Approach for RGB-D Cameras. In Image Analysis and Recognition, Aur\u00e9lio Campilho and Mohamed Kamel (Eds.). Springer International Publishing, Cham, 21\u201328.",
      "doi": ""
    },
    {
      "text": "Jun Rekimoto and Yu Nishimura. 2021. Derma: Silent Speech Interaction Using Transcutaneous Motion Sensing. In Augmented Humans Conference 2021 (Rovaniemi, Finland) (AHs\u201921). Association for Computing Machinery, New York, NY, USA, 91\u2013100. https://doi.org/10.1145/3458709.3458941",
      "doi": "10.1145/3458709.3458941"
    },
    {
      "text": "Christine Rzepka. 2019. Examining the use of voice assistants: A value-focused thinking approach. (2019).",
      "doi": ""
    },
    {
      "text": "Himanshu Sahni, Abdelkareem Bedri, Gabriel Reyes, Pavleen Thukral, Zehua Guo, Thad Starner, and Maysam Ghovanloo. 2014. The Tongue and Ear Interface: A Wearable System for Silent Speech Recognition. In Proceedings of the 2014 ACM International Symposium on Wearable Computers (Seattle, Washington) (ISWC \u201914). Association for Computing Machinery, New York, NY, USA, 47\u201354. https://doi.org/10.1145/2634317.2634322",
      "doi": "10.1145/2634317.2634322"
    },
    {
      "text": "George Saon, Gakuto Kurata, Tom Sercu, Kartik Audhkhasi, Samuel Thomas, Dimitrios Dimitriadis, Xiaodong Cui, Bhuvana Ramabhadran, Michael Picheny, Lynn-Li Lim, Bergul Roomi, and Phil Hall. 2017. English Conversational Telephone Speech Recognition by Humans and Machines. (2017). https://doi.org/10.48550/ARXIV.1703.02136",
      "doi": ""
    },
    {
      "text": "Tanja Schultz. 2010. ICCHP Keynote: Recognizing Silent and Weak Speech Based on Electromyography. In Computers Helping People with Special Needs, Klaus Miesenberger, Joachim Klaus, Wolfgang Zagler, and Arthur Karshmer (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 595\u2013604.",
      "doi": ""
    },
    {
      "text": "Tanmay Srivastava, Prerna Khanna, Shijia Pan, Phuc Nguyen, and Shubham Jain. 2022. MuteIt: Jaw Motion Based Unvoiced Command Recognition Using Earable. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 6, 3, Article 140 (sep 2022), 26\u00a0pages. https://doi.org/10.1145/3550281",
      "doi": "10.1145/3550281"
    },
    {
      "text": "Ke Sun, Chun Yu, Weinan Shi, Lan Liu, and Yuanchun Shi. 2018. Lip-Interact: Improving Mobile Device Interaction with Silent Speech Commands. In Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology(Berlin, Germany) (UIST \u201918). Association for Computing Machinery, New York, NY, USA, 581\u2013593. https://doi.org/10.1145/3242587.3242599",
      "doi": "10.1145/3242587.3242599"
    },
    {
      "text": "Abhinav Thanda and Shankar\u00a0M. Venkatesan. 2017. Audio Visual Speech Recognition Using Deep Recurrent Neural Networks. In Multimodal Pattern Recognition of Social Signals in Human-Computer-Interaction, Friedhelm Schwenker and Stefan Scherer (Eds.). Springer International Publishing, Cham, 98\u2013109.",
      "doi": ""
    },
    {
      "text": "L\u00e1szl\u00f3 T\u00f3th, G\u00e1bor Gosztolya, Tam\u00e1s Gr\u00f3sz, Alexandra Mark\u00f3, and Tam\u00e1s\u00a0G\u00e1bor Csap\u00f3. 2018. Multi-Task Learning of Speech Recognition and Speech Synthesis Parameters for Ultrasound-based Silent Speech Interfaces. In Proc. Interspeech 2018. 3172\u20133176. https://doi.org/10.21437/Interspeech.2018-1078",
      "doi": ""
    },
    {
      "text": "Darya Vorontsova, Ivan Menshikov, Aleksandr Zubov, Kirill Orlov, Peter Rikunov, Ekaterina Zvereva, Lev Flitman, Anton Lanikin, Anna Sokolova, Sergey Markov, and Alexandra Bernadotte. 2021. Silent EEG-Speech Recognition Using Convolutional and Recurrent Neural Network with 85% Accuracy of 9 Words Classification. Sensors 21, 20 (2021). https://doi.org/10.3390/s21206744",
      "doi": ""
    },
    {
      "text": "Konstantinos Vougioukas, Pingchuan Ma, Stavros Petridis, and Maja Pantic. 2019. Video-Driven Speech Reconstruction using Generative Adversarial Networks. (2019). https://doi.org/10.48550/ARXIV.1906.06301",
      "doi": ""
    },
    {
      "text": "Michael Wand, Jan Koutn\u00edk, and J\u00fcrgen Schmidhuber. 2016. Lipreading with long short-term memory. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 6115\u20136119. https://doi.org/10.1109/ICASSP.2016.7472852",
      "doi": "10.1109/ICASSP.2016.7472852"
    },
    {
      "text": "Jingxian Wang, Chengfeng Pan, Haojian Jin, Vaibhav Singh, Yash Jain, Jason\u00a0I. Hong, Carmel Majidi, and Swarun Kumar. 2020. RFID Tattoo: A Wireless Platform for Speech Recognition. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 3, 4, Article 155 (sep 2020), 24\u00a0pages. https://doi.org/10.1145/3369812",
      "doi": "10.1145/3369812"
    },
    {
      "text": "You Wang, Ming Zhang, RuMeng Wu, Han Gao, Meng Yang, Zhiyuan Luo, and Guang Li. 2020. Silent Speech Decoding Using Spectrogram Features Based on Neuromuscular Activities. Brain Sciences 10, 7 (2020). https://doi.org/10.3390/brainsci10070442",
      "doi": ""
    },
    {
      "text": "Zi Wang, Yili Ren, Yingying Chen, and Jie Yang. 2022. ToothSonic: Earable Authentication via Acoustic Toothprint. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 6, 2, Article 78 (jul 2022), 24\u00a0pages. https://doi.org/10.1145/3534606",
      "doi": "10.1145/3534606"
    },
    {
      "text": "Philip Weber and Thomas Ludwig. 2020. (Non-)Interacting with Conversational Agents: Perceptions and Motivations of Using Chatbots and Voice Assistants. In Proceedings of Mensch Und Computer 2020(Magdeburg, Germany) (MuC \u201920). Association for Computing Machinery, New York, NY, USA, 321\u2013331. https://doi.org/10.1145/3404983.3405513",
      "doi": "10.1145/3404983.3405513"
    },
    {
      "text": "W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, and G. Zweig. 2016. Achieving Human Parity in Conversational Speech Recognition. (2016). https://doi.org/10.48550/ARXIV.1610.05256",
      "doi": ""
    },
    {
      "text": "Kai Xu, Dawei Li, Nick Cassimatis, and Xiaolong Wang. 2018. LCANet: End-to-End Lipreading with Cascaded Attention-CTC. In 2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018). 548\u2013555. https://doi.org/10.1109/FG.2018.00088",
      "doi": "10.1109/FG.2018.00088"
    },
    {
      "text": "Kele Xu, Yuxiang Wu, and Zhifeng Gao. 2019. Ultrasound-Based Silent Speech Interface Using Sequential Convolutional Auto-Encoder. In Proceedings of the 27th ACM International Conference on Multimedia (Nice, France) (MM \u201919). Association for Computing Machinery, New York, NY, USA, 2194\u20132195. https://doi.org/10.1145/3343031.3350596",
      "doi": "10.1145/3343031.3350596"
    },
    {
      "text": "Shuang Yang, Yuanhang Zhang, Dalu Feng, Mingmin Yang, Chenhao Wang, Jingyun Xiao, Keyu Long, Shiguang Shan, and Xilin Chen. 2019. LRW-1000: A Naturally-Distributed Large-Scale Benchmark for Lip Reading in the Wild. In 2019 14th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2019). 1\u20138. https://doi.org/10.1109/FG.2019.8756582",
      "doi": "10.1109/FG.2019.8756582"
    },
    {
      "text": "Qian Zhang, Dong Wang, Run Zhao, and Yinggang Yu. 2021. SoundLip: Enabling Word and Sentence-Level Lip Interaction for Smart Devices. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 5, 1, Article 43 (mar 2021), 28\u00a0pages. https://doi.org/10.1145/3448087",
      "doi": "10.1145/3448087"
    },
    {
      "text": "Ruidong Zhang, Mingyang Chen, Benjamin Steeper, Yaxuan Li, Zihan Yan, Yizhuo Chen, Songyun Tao, Tuochao Chen, Hyunchul Lim, and Cheng Zhang. 2022. SpeeChin: A Smart Necklace for Silent Speech Recognition. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 5, 4, Article 192 (dec 2022), 23\u00a0pages. https://doi.org/10.1145/3494987",
      "doi": "10.1145/3494987"
    },
    {
      "text": "Yongzhao Zhang, Yi-Chao Chen, Haonan Wang, and Xingyu Jin. 2021. CELIP: Ultrasonic-Based Lip Reading with Channel Estimation Approach for Virtual Reality Systems. In Adjunct Proceedings of the 2021 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2021 ACM International Symposium on Wearable Computers (Virtual, USA) (UbiComp \u201921). Association for Computing Machinery, New York, NY, USA, 580\u2013585. https://doi.org/10.1145/3460418.3480163",
      "doi": "10.1145/3460418.3480163"
    },
    {
      "text": "Yongzhao Zhang, Wei-Hsiang Huang, Chih-Yun Yang, Wen-Ping Wang, Yi-Chao Chen, Chuang-Wen You, Da-Yuan Huang, Guangtao Xue, and Jiadi Yu. 2020. Endophasia: Utilizing Acoustic-Based Imaging for Issuing Contact-Free Silent Speech Commands. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 4, 1, Article 37 (mar 2020), 26\u00a0pages. https://doi.org/10.1145/3381008",
      "doi": "10.1145/3381008"
    },
    {
      "text": "Guoying Zhao, Mark Barnard, and Matti Pietikainen. 2009. Lipreading With Local Spatiotemporal Descriptors. IEEE Transactions on Multimedia 11, 7 (2009), 1254\u20131265. https://doi.org/10.1109/TMM.2009.2030637",
      "doi": "10.1109/TMM.2009.2030637"
    }
  ]
}