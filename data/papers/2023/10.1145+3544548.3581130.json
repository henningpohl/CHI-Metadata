{
  "doi": "10.1145/3544548.3581130",
  "title": "Visible Nuances: A Caption System to Visualize Paralinguistic Speech Cues for Deaf and Hard-of-Hearing Individuals",
  "published": "2023-04-19",
  "proctitle": "CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems",
  "pages": "1-15",
  "year": 2023,
  "badges": [],
  "abstract": "Captions help deaf and hard-of-hearing (DHH) individuals visually communicate voice information to better understand video content. In speech, the literal content and paralinguistic cues (e.g., pitch and nuance) work together to create real intention. However, current captions are limited in their capacity to deliver fine nuances because they cannot fully convey these paralinguistic cues. This paper proposes an audio-visualized caption system that automatically visualizes paralinguistic cues into various caption elements (thickness, height, font type and motion). A comparative study with 20 DHH participants demonstrates how our system supports DHH individuals to be better accessible to paralinguistic cues while watching videos. Particularly in the case of formal talks, they could accurately identify the speaker\u2019s nuance more often compared to current captions, without any practice or training. Addressing some issues on legibility and familiarity, the proposed caption system has potentials to enrich DHH individuals\u2019 video watching experience more as hearing people enjoy.",
  "tags": [
    "Speech accessibility",
    "Paralinguistic cues",
    "Deaf and hard-of-hearing individuals",
    "Caption design"
  ],
  "authors": [
    {
      "name": "JooYeong Kim",
      "institution": "School of Integrated Technology/Soft Computing & Interaction Laboratory, Gwangju Institute of Science and Technology, Korea, Republic of",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99660239862",
      "orcid": "0000-0002-4721-8475"
    },
    {
      "name": "SooYeon Ahn",
      "institution": "School of Integrated Technology / Soft Computing & Interaction Laboratory, Gwangju Institute of Science and Technology, Korea, Republic of",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99660780059",
      "orcid": "0000-0002-3862-0614"
    },
    {
      "name": "Jin-Hyuk Hong",
      "institution": "School of Integrated Technology/Soft Computing & Interaction Laboratory, Gwangju Institute of Science and Technology, Korea, Republic of and Artificial Intelligence Graduate School, Gwangju Institute of Science and Technology, Korea, Republic of",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659702157",
      "orcid": "0000-0002-8838-5667"
    }
  ],
  "references": [
    {
      "text": "Chanchal Agrawal and Roshan\u00a0L Peiris. 2021. I See What You\u2019re Saying: A Literature Review of Eye Tracking Research in Communication of Deaf or Hard of Hearing Users. In Proceedings of the 23rd International ACM SIGACCESS Conference on Computers and Accessibility (Virtual Event, USA) (ASSETS \u201921). Association for Computing Machinery, New York, NY, USA, Article 41, 13\u00a0pages. https://doi.org/10.1145/3441852.3471209",
      "doi": "10.1145/3441852.3471209"
    },
    {
      "text": "Suha\u00a0S. Al-Thanyyan and Aqil\u00a0M. Azmi. 2021. Automated Text Simplification: A Survey. ACM Comput. Surv. 54, 2 (2021), Article 43. https://doi.org/10.1145/3442695",
      "doi": "10.1145/3442695"
    },
    {
      "text": "Oliver Alonzo, Matthew Seita, Abraham Glasser, and Matt Huenerfauth. 2020. Automatic Text Simplification Tools for Deaf and Hard of Hearing Adults: Benefits of Lexical Simplification and Providing Users with Autonomy. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI \u201920). Association for Computing Machinery, New York, NY, USA, 1\u201313. https://doi.org/10.1145/3313831.3376563",
      "doi": "10.1145/3313831.3376563"
    },
    {
      "text": "Oliver Alonzo, Jessica Trussell, Becca Dingman, and Matt Huenerfauth. 2021. Comparison of Methods for Evaluating Complexity of Simplified Texts among Deaf and Hard-of-Hearing Adults at Different Literacy Levels. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (Yokohama, Japan) (CHI \u201921). Association for Computing Machinery, New York, NY, USA, Article 279, 12\u00a0pages. https://doi.org/10.1145/3411764.3445038",
      "doi": "10.1145/3411764.3445038"
    },
    {
      "text": "Larwan Berke, Christopher Caulfield, and Matt Huenerfauth. 2017. Deaf and Hard-of-Hearing Perspectives on Imperfect Automatic Speech Recognition for Captioning One-on-One Meetings. In Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility (Baltimore, Maryland, USA) (ASSETS \u201917). Association for Computing Machinery, New York, NY, USA, 155\u2013164. https://doi.org/10.1145/3132525.3132541",
      "doi": "10.1145/3132525.3132541"
    },
    {
      "text": "Ann Bessemans, Maarten Renckens, Kevin Bormans, Erik Nuyts, and Kevin Larson. 2019. Visual prosody supports reading aloud expressively. Visible Language 53, 3 (2019).",
      "doi": ""
    },
    {
      "text": "Anna\u00a0C. Cavender, Jeffrey\u00a0P. Bigham, and Richard\u00a0E. Ladner. 2009. ClassInFocus: Enabling Improved Visual Attention Strategies for Deaf and Hard of Hearing Students. In Proceedings of the 11th International ACM SIGACCESS Conference on Computers and Accessibility (Pittsburgh, Pennsylvania, USA) (Assets \u201909). Association for Computing Machinery, New York, NY, USA, 67\u201374. https://doi.org/10.1145/1639642.1639656",
      "doi": "10.1145/1639642.1639656"
    },
    {
      "text": "Calu\u00e3 de Lacerda\u00a0Pataca and Paula Dornhofer\u00a0Paro Costa. 2022. Hidden bawls, whispers, and yelps: can text be made to sound more than just its words?arXiv e-prints arXiv: 2202.10631(2022).",
      "doi": ""
    },
    {
      "text": "[9] Emotional conversation corpus data.2022. AI Hub. Retrieved from https://aihub.or.kr.",
      "doi": ""
    },
    {
      "text": "Thibault Fabre, Adrien Verhulst, Alfonso Balandra, Maki Sugimoto, and Masahiko Inami. 2021. Investigating Textual Visual Sound Effects in a Virtual Environment and their impacts on Object Perception and Sound Perception. In 2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR). 320\u2013328. https://doi.org/10.1109/ISMAR52148.2021.00048",
      "doi": ""
    },
    {
      "text": "Jodi Forlizzi, Johnny Lee, and Scott Hudson. 2003. The Kinedit System: Affective Messages Using Dynamic Texts. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (Ft. Lauderdale, Florida, USA) (CHI \u201903). Association for Computing Machinery, New York, NY, USA, 377\u2013384. https://doi.org/10.1145/642611.642677",
      "doi": "10.1145/642611.642677"
    },
    {
      "text": "Sandra\u00a0G Hart and Lowell\u00a0E Staveland. 1988. Development of NASA-TLX (Task Load Index): Results of empirical and theoretical research. Vol.\u00a052. Elsevier, 139\u2013183.",
      "doi": ""
    },
    {
      "text": "Richard\u00a0L Hazlett, Kevin Larson, A\u00a0Dawn Shaikh, and Barbara\u00a0S Chaparo. 2013. Two studies on how a typeface congruent with content can enhance onscreen communication. Information Design Journal (IDJ) 20, 3 (2013).",
      "doi": ""
    },
    {
      "text": "Romain Hennequin, Anis Khlif, Felix Voituret, and Manuel Moussallam. 2020. Spleeter: a fast and efficient music source separation tool with pre-trained models. Journal of Open Source Software 5, 50 (2020), 2154. https://doi.org/10.21105/joss.02154",
      "doi": ""
    },
    {
      "text": "Richang Hong, Meng Wang, Mengdi Xu, Shuicheng Yan, and Tat-Seng Chua. 2010. Dynamic Captioning: Video Accessibility Enhancement for Hearing Impairment. In Proceedings of the 18th ACM International Conference on Multimedia (Firenze, Italy) (MM \u201910). Association for Computing Machinery, New York, NY, USA, 421\u2013430. https://doi.org/10.1145/1873951.1874013",
      "doi": "10.1145/1873951.1874013"
    },
    {
      "text": "Hun-Young Jung, Jong-Hyeok Lee, Eunju Min, and Seung-Hoon Na. 2019. Word reordering for translation into Korean sign language using syntactically-guided classification. ACM Transactions on Asian and Low-Resource Language Information Processing (TALLIP) 19, 2 (2019), 1\u201320.",
      "doi": ""
    },
    {
      "text": "Sushant Kafle, Peter Yeung, and Matt Huenerfauth. 2019. Evaluating the Benefit of Highlighting Key Words in Captions for People Who Are Deaf or Hard of Hearing. In Proceedings of the 21st International ACM SIGACCESS Conference on Computers and Accessibility (Pittsburgh, PA, USA) (ASSETS \u201919). Association for Computing Machinery, New York, NY, USA, 43\u201355. https://doi.org/10.1145/3308561.3353781",
      "doi": "10.1145/3308561.3353781"
    },
    {
      "text": "Tugba Kulahcioglu and Gerard de Melo. 2020. Fonts Like This but Happier: A New Way to Discover Fonts. In Proceedings of the 28th ACM International Conference on Multimedia (Seattle, WA, USA) (MM \u201920). Association for Computing Machinery, New York, NY, USA, 2973\u20132981. https://doi.org/10.1145/3394171.3413534",
      "doi": "10.1145/3394171.3413534"
    },
    {
      "text": "Tugba Kulahcioglu and Gerard de Melo. 2021. Semantics-aware typographical choices via affective associations. Language Resources and Evaluation 55, 1 (2021), 105\u2013126.",
      "doi": "10.1007/s10579-020-09499-0"
    },
    {
      "text": "Ludwig K\u00fcrzinger, Dominik Winkelbauer, Lujun Li, Tobias Watzel, and Gerhard Rigoll. 2020. CTC-Segmentation of Large Corpora for German End-to-End Speech Recognition. In Speech and Computer, Alexey Karpov and Rodmonga Potapova (Eds.). Springer International Publishing, Cham, 267\u2013278.",
      "doi": ""
    },
    {
      "text": "Raja\u00a0S. Kushalnagar, Gary\u00a0W. Behm, Joseph\u00a0S. Stanislow, and Vasu Gupta. 2014. Enhancing Caption Accessibility through Simultaneous Multimodal Information: Visual-Tactile Captions. In Proceedings of the 16th International ACM SIGACCESS Conference on Computers & Accessibility (Rochester, New York, USA) (ASSETS \u201914). Association for Computing Machinery, New York, NY, USA, 185\u2013192. https://doi.org/10.1145/2661334.2661381",
      "doi": "10.1145/2661334.2661381"
    },
    {
      "text": "Raja\u00a0S. Kushalnagar, Walter\u00a0S. Lasecki, and Jeffrey\u00a0P. Bigham. 2014. Accessibility Evaluation of Classroom Captions. ACM Trans. Access. Comput. 5, 3, Article 7 (jan 2014), 24\u00a0pages. https://doi.org/10.1145/2543578",
      "doi": "10.1145/2543578"
    },
    {
      "text": "Daniel\u00a0G. Lee, Deborah\u00a0I. Fels, and John\u00a0Patrick Udo. 2007. Emotive Captioning. Comput. Entertain. 5, 2, Article 11 (apr 2007), 15\u00a0pages. https://doi.org/10.1145/1279540.1279551",
      "doi": "10.1145/1279540.1279551"
    },
    {
      "text": "Joonhwan Lee, Dongwhan Kim, Jieun Wee, Sooyeun Jang, Seyong Ha, and Soojin Jun. 2014. Evaluating pre-defined kinetic typography effects to convey emotions. Journal of Korea Multimedia Society 17, 1 (2014), 77\u201393.",
      "doi": ""
    },
    {
      "text": "Johnny\u00a0C. Lee, Jodi Forlizzi, and Scott\u00a0E. Hudson. 2002. The Kinetic Typography Engine: An Extensible System for Animating Expressive Text. In Proceedings of the 15th Annual ACM Symposium on User Interface Software and Technology(Paris, France) (UIST \u201902). Association for Computing Machinery, New York, NY, USA, 81\u201390. https://doi.org/10.1145/571985.571997",
      "doi": "10.1145/571985.571997"
    },
    {
      "text": "Wootaek Lim, Inseon Jang, and Chunghyun Ahn. 2014. Dynamic Subtitle Authoring Method Based on Audio Analysis for the Hearing Impaired. In Computers Helping People with Special Needs, Klaus Miesenberger, Deborah Fels, Dominique Archambault, Petr Pe\u0148\u00e1z, and Wolfgang Zagler (Eds.). Springer International Publishing, Cham, 53\u201360.",
      "doi": ""
    },
    {
      "text": "Catarina Ma\u00e7\u00e3s, David Palma, and Artur Rebelo. 2020. TypEm: A Generative Typeface That Represents the Emotion of the Text. In Proceedings of the 9th International Conference on Digital and Interactive Arts (Braga, Portugal) (ARTECH 2019). Association for Computing Machinery, New York, NY, USA, Article 5, 10\u00a0pages. https://doi.org/10.1145/3359852.3359874",
      "doi": "10.1145/3359852.3359874"
    },
    {
      "text": "Sabrina Malik, Jonathan Aitken, and Judith\u00a0Kelly Waalen. 2009. Communicating emotion with animated text. visual communication 8, 4 (2009), 469\u2013479.",
      "doi": ""
    },
    {
      "text": "Brian McFee, Colin Raffel, Dawen Liang, Daniel\u00a0P Ellis, Matt McVicar, Eric Battenberg, and Oriol Nieto. 2015. librosa: Audio and music signal analysis in python. In Proceedings of the 14th python in science conference, Vol.\u00a08. 18\u201325.",
      "doi": ""
    },
    {
      "text": "Jos\u00e9lia Neves. 2008. 10 fallacies about Subtitling for the d/Deaf and the hard of hearing. The Journal of Specialised Translation 10 (2008), 128\u2013143.",
      "doi": ""
    },
    {
      "text": "James Ohene-Djan, Jenny Wright, and Kirsty Combie-Smith. 2007. Emotional Subtitles: A System and Potential Applications for Deaf and Hearing Impaired People.. In CVHI.",
      "doi": ""
    },
    {
      "text": "Kotaro Oomori, Akihisa Shitara, Tatsuya Minagawa, Sayan Sarcar, and Yoichi Ochiai. 2020. A Preliminary Study on Understanding Voice-Only Online Meetings Using Emoji-Based Captioning for Deaf or Hard of Hearing Users. In Proceedings of the 22nd International ACM SIGACCESS Conference on Computers and Accessibility (Virtual Event, Greece) (ASSETS \u201920). Association for Computing Machinery, New York, NY, USA, Article 54, 4\u00a0pages. https://doi.org/10.1145/3373625.3418032",
      "doi": "10.1145/3373625.3418032"
    },
    {
      "text": "[33] Paralanguage.2022. APA dictionary of psychology. Retrieved from https://dictionary.apa.org/paralanguage.",
      "doi": ""
    },
    {
      "text": "Ricardo Pereira, Carlos Roque, Ant\u00f3nio Martinho, and Carlos Martinho. 2018. Expressing emotions through animated speech balloons.",
      "doi": ""
    },
    {
      "text": "[35] Phoneme.2022. Standard Korean Language Dictionary. Retrieved from https://stdict.korean.go.kr/search/searchView.do?word_no=262488&searchKeywordTo=3.",
      "doi": ""
    },
    {
      "text": "[36] Raisa Rashid.2008. Representing Emotion with Animated Text. Master\u2019s thesis.",
      "doi": ""
    },
    {
      "text": "Raisa Rashid, Quoc Vy, Richard Hunt, and Deborah\u00a0I Fels. 2008. Dancing with words: Using animated text for captioning. Intl. Journal of Human\u2013Computer Interaction 24, 5(2008), 505\u2013519.",
      "doi": ""
    },
    {
      "text": "Bj\u00f6rn Schuller, Stefan Steidl, Anton Batliner, Felix Burkhardt, Laurence Devillers, Christian M\u00fcLler, and Shrikanth Narayanan. 2013. Paralinguistics in speech and language\u2014state-of-the-art and the challenge. Computer Speech & Language 27, 1 (2013), 4\u201339.",
      "doi": "10.1016/j.csl.2012.02.005"
    },
    {
      "text": "Shuichi Seto, Hiroshi Arai, Kimikazu Sugimori, Yuko Shimomura, and Hiroyuki Kawabe. 2010. Subtitle system visualizing non-verbal expressions in voice for hearing impaired-Ambient Font. In Proceeding of the 10th Asia-Pacific Industrial Engineering and Management Systems.",
      "doi": ""
    },
    {
      "text": "Brent\u00a0N. Shiver and Rosalee\u00a0J. Wolfe. 2015. Evaluating Alternatives for Better Deaf Accessibility to Selected Web-Based Multimedia. In Proceedings of the 17th International ACM SIGACCESS Conference on Computers & Accessibility (Lisbon, Portugal) (ASSETS \u201915). Association for Computing Machinery, New York, NY, USA, 231\u2013238. https://doi.org/10.1145/2700648.2809857",
      "doi": "10.1145/2700648.2809857"
    },
    {
      "text": "[41] Syllable.2022. Longman Dictionary. Retrieved from https://www.ldoceonline.com/dictionary/syllable.",
      "doi": ""
    },
    {
      "text": "[42] Syllable.2022. Standard Korean Language Dictionary. https://stdict.korean.go.kr/search/searchResult.do?pageSize=10&searchKeyword=%EC%9D%8C%EC%A0%88.",
      "doi": ""
    },
    {
      "text": "Yushi Ueda. 2021. ESPnet2 pretrained model, Yushi Ueda/ksponspeech_a sr_train_asr_conformer8_n_fft512_hop_length256_raw _kr_bpe2309_valid.acc.best, fs=16k, lang=kr. https://doi.org/10.5281/zenodo.5154341",
      "doi": ""
    },
    {
      "text": "Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki Hayashi, Jiro Nishitoba, Yuya Unno, Nelson Enrique\u00a0Yalta Soplin, Jahn Heymann, Matthew Wiesner, and Nanxin Chen. 2018. Espnet: End-to-end speech processing toolkit. arXiv preprint arXiv:1804.00015(2018).",
      "doi": ""
    },
    {
      "text": "Jennifer Wehrmeyer. 2014. Eye-tracking Deaf and hearing viewing of sign language interpreted news broadcasts. Journal of Eye Movement Research 7, 1 (2014).",
      "doi": ""
    },
    {
      "text": "Matthias W\u00f6lfel, Tim Schlippe, and Angelo Stitz. 2015. Voice driven type design. In 2015 International Conference on Speech Technology and Human-Computer Dialogue (SpeD). 1\u20139. https://doi.org/10.1109/SPED.2015.7343095",
      "doi": ""
    },
    {
      "text": "Soledad Z\u00e1rate. 2021. Captioning and Subtitling for D/deaf and Hard of Hearing Audiences. UCL Press.",
      "doi": ""
    }
  ]
}