{
  "doi": "10.1145/3544548.3581366",
  "title": "Watch Out for Updates: Understanding the Effects of Model Explanation Updates in AI-Assisted Decision Making",
  "published": "2023-04-19",
  "proctitle": "CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems",
  "pages": "1-19",
  "year": 2023,
  "badges": [],
  "abstract": "AI explanations have been increasingly used to help people better utilize AI recommendations in AI-assisted decision making. While AI explanations may change over time due to updates of the AI model, little is known about how these changes may affect people\u2019s perceptions and usage of the model. In this paper, we study how varying levels of similarity between the AI explanations before and after a model update affects people\u2019s trust in and satisfaction with the AI model. We conduct randomized human-subject experiments on two decision making contexts where people have different levels of domain knowledge. Our results show that changes in AI explanation during the model update do not affect people\u2019s tendency to adopt AI recommendations. However, they may change people\u2019s subjective trust in and satisfaction with the AI model via changing both their perceived model accuracy and perceived consistency of AI explanations with their prior knowledge.",
  "tags": [
    "AI updates",
    "Human-subject experiments",
    "Explainable AI"
  ],
  "authors": [
    {
      "name": "Xinru Wang",
      "institution": "Computer Science, Purdue University, United States",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659693789",
      "orcid": "0000-0002-0213-6425"
    },
    {
      "name": "Ming Yin",
      "institution": "Computer Science, Purdue University, United States",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81438595973",
      "orcid": "0000-0002-7364-139X"
    }
  ],
  "references": [
    {
      "text": "Amina Adadi and Mohammed Berrada. 2018. Peeking inside the black-box: A survey on Explainable Artificial Intelligence (XAI). IEEE Access 6(2018), 52138\u201352160.",
      "doi": ""
    },
    {
      "text": "Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim. 2018. Sanity checks for saliency maps. Advances in neural information processing systems 31 (2018).",
      "doi": ""
    },
    {
      "text": "Robert Agler and Paul De\u00a0Boeck. 2017. On the interpretation and use of mediation: multiple perspectives on mediation analysis. Frontiers in psychology 8 (2017), 1984.",
      "doi": ""
    },
    {
      "text": "Ariful\u00a0Islam Anik and Andrea Bunt. 2021. Data-centric explanations: explaining training data of machine learning systems to promote transparency. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 1\u201313.",
      "doi": "10.1145/3411764.3445736"
    },
    {
      "text": "James\u00a0L Arbuckle. 2019. Amos 26.0 User\u2019s Guide. Amos Development Corporation, SPSS Inc(2019).",
      "doi": ""
    },
    {
      "text": "Gagan Bansal, Besmira Nushi, Ece Kamar, Daniel\u00a0S Weld, Walter\u00a0S Lasecki, and Eric Horvitz. 2019. Updates in human-ai teams: Understanding and addressing the performance/compatibility tradeoff. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol.\u00a033. 2429\u20132437.",
      "doi": "10.1609/aaai.v33i01.33012429"
    },
    {
      "text": "Gagan Bansal, Tongshuang Wu, Joyce Zhou, Raymond Fok, Besmira Nushi, Ece Kamar, Marco\u00a0Tulio Ribeiro, and Daniel Weld. 2021. Does the whole exceed its parts? the effect of ai explanations on complementary team performance. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 1\u201316.",
      "doi": "10.1145/3411764.3445717"
    },
    {
      "text": "John\u00a0J Bartholdi, Craig\u00a0A Tovey, and Michael\u00a0A Trick. 1989. The computational difficulty of manipulating an election. Social Choice and Welfare 6, 3 (1989), 227\u2013241.",
      "doi": ""
    },
    {
      "text": "Kenneth\u00a0A Bollen. 1989. Structural equations with latent variables. Vol.\u00a0210. John Wiley & Sons.",
      "doi": ""
    },
    {
      "text": "Olivier Bousquet and Andr\u00e9 Elisseeff. 2000. Algorithmic stability and generalization performance. Advances in Neural Information Processing Systems 13 (2000).",
      "doi": ""
    },
    {
      "text": "Timothy\u00a0A Brown. 2015. Confirmatory factor analysis for applied research. Guilford publications.",
      "doi": ""
    },
    {
      "text": "Michael\u00a0W Browne and Gerhard Mels. 1992. RAMONA user\u2019s guide.",
      "doi": ""
    },
    {
      "text": "Cristian Bucilu\u01ce, Rich Caruana, and Alexandru Niculescu-Mizil. 2006. Model compression. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining. 535\u2013541.",
      "doi": "10.1145/1150402.1150464"
    },
    {
      "text": "Zana Bu\u00e7inca, Phoebe Lin, Krzysztof\u00a0Z Gajos, and Elena\u00a0L Glassman. 2020. Proxy tasks and subjective measures can be misleading in evaluating explainable AI systems. In Proceedings of the 25th international conference on intelligent user interfaces. 454\u2013464.",
      "doi": "10.1145/3377325.3377498"
    },
    {
      "text": "Adrian Bussone, Simone Stumpf, and Dympna O\u2019Sullivan. 2015. The role of explanations on trust and reliance in clinical decision support systems. In 2015 international conference on healthcare informatics. IEEE, 160\u2013169.",
      "doi": "10.1109/ICHI.2015.26"
    },
    {
      "text": "Samuel Carton, Qiaozhu Mei, and Paul Resnick. 2020. Feature-Based Explanations Don\u2019t Help People Detect Misclassifications of Online Toxicity. In Proceedings of the International AAAI Conference on Web and Social Media, Vol.\u00a014. 95\u2013106.",
      "doi": ""
    },
    {
      "text": "Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie Elhadad. 2015. Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission. In Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining. 1721\u20131730.",
      "doi": "10.1145/2783258.2788613"
    },
    {
      "text": "Valerie Chen, Umang Bhatt, Hoda Heidari, Adrian Weller, and Ameet Talwalkar. 2022. Perspectives on Incorporating Expert Feedback into Model Updates. arXiv preprint arXiv:2205.06905(2022).",
      "doi": ""
    },
    {
      "text": "Hao-Fei Cheng, Ruotong Wang, Zheng Zhang, Fiona O\u2019Connell, Terrance Gray, F\u00a0Maxwell Harper, and Haiyi Zhu. 2019. Explaining decision-making algorithms through UI: Strategies to help non-expert stakeholders. In Proceedings of the 2019 chi conference on human factors in computing systems. 1\u201312.",
      "doi": "10.1145/3290605.3300789"
    },
    {
      "text": "Chun-Wei Chiang and Ming Yin. 2021. You\u2019d better stop! Understanding human reliance on machine learning models under covariate shift. In 13th ACM Web Science Conference 2021. 120\u2013129.",
      "doi": "10.1145/3447535.3462487"
    },
    {
      "text": "Robert Cudeck. 1993. of Assessing Model Fit. Testing structural equation models 154 (1993), 136.",
      "doi": ""
    },
    {
      "text": "James\u00a0A Davis and Robert\u00a0Philip Weber. 1985. The logic of causal order. Vol.\u00a055. Sage.",
      "doi": ""
    },
    {
      "text": "Maria De-Arteaga, Riccardo Fogliato, and Alexandra Chouldechova. 2020. A case for humans-in-the-loop: Decisions in the presence of erroneous algorithmic scores. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. 1\u201312.",
      "doi": "10.1145/3313831.3376638"
    },
    {
      "text": "Murat Dikmen and Catherine Burns. 2022. The effects of domain knowledge on trust in explainable AI and task performance: A case of peer-to-peer lending. International Journal of Human-Computer Studies 162 (2022), 102792.",
      "doi": "10.1016/j.ijhcs.2022.102792"
    },
    {
      "text": "Finale Doshi-Velez and Been Kim. 2017. Towards a rigorous science of interpretable machine learning. arXiv preprint arXiv:1702.08608(2017).",
      "doi": ""
    },
    {
      "text": "Pierre Dragicevic. 2016. Fair statistical communication in HCI. In Modern statistical methods for HCI. Springer, 291\u2013330.",
      "doi": ""
    },
    {
      "text": "Mengnan Du, Ninghao Liu, and Xia Hu. 2019. Techniques for interpretable machine learning. Commun. ACM 63, 1 (2019), 68\u201377.",
      "doi": "10.1145/3359786"
    },
    {
      "text": "Dheeru Dua and Casey Graff. 2017. UCI Machine Learning Repository. http://archive.ics.uci.edu/ml",
      "doi": ""
    },
    {
      "text": "John\u00a0J Dudley and Per\u00a0Ola Kristensson. 2018. A review of user interface design for interactive machine learning. ACM Transactions on Interactive Intelligent Systems (TiiS) 8, 2(2018), 1\u201337.",
      "doi": "10.1145/3185517"
    },
    {
      "text": "Leon Festinger. 1962. A theory of cognitive dissonance. Vol.\u00a02. Stanford university press.",
      "doi": ""
    },
    {
      "text": "Aaron Fisher, Cynthia Rudin, and Francesca Dominici. 2018. Model class reliance: Variable importance measures for any machine learning model class, from the \u201cRashomon\u201d perspective. arXiv preprint arXiv:1801.01489 68 (2018).",
      "doi": ""
    },
    {
      "text": "Elliot G.\u00a0Mitchell, Elizabeth M.\u00a0Heitkemper, Marissa Burgermaster, Matthew E.\u00a0Levine, Yishen Miao, Maria L.\u00a0Hwang, Pooja M.\u00a0Desai, Andrea Cassells, Jonathan N.\u00a0Tobin, Esteban G.\u00a0Tabak, 2021. From reflection to action: Combining machine learning with expert knowledge for nutrition goal recommendations. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 1\u201317.",
      "doi": "10.1145/3411764.3445555"
    },
    {
      "text": "Efstathios\u00a0D Gennatas, Jerome\u00a0H Friedman, Lyle\u00a0H Ungar, Romain Pirracchio, Eric Eaton, Lara\u00a0G Reichmann, Yannet Interian, Jos\u00e9\u00a0Marcio Luna, Charles\u00a0B Simone, Andrew Auerbach, 2020. Expert-augmented machine learning. Proceedings of the National Academy of Sciences 117, 9 (2020), 4571\u20134577.",
      "doi": ""
    },
    {
      "text": "Bhavya Ghai, Q\u00a0Vera Liao, Yunfeng Zhang, Rachel Bellamy, and Klaus Mueller. 2021. Explainable active learning (xal) toward ai explanations as interfaces for machine teachers. Proceedings of the ACM on Human-Computer Interaction 4, CSCW3(2021), 1\u201328.",
      "doi": "10.1145/3432934"
    },
    {
      "text": "James\u00a0B Grace and Heli Jutila. 1999. The relationship between species density and community biomass in grazed and ungrazed coastal meadows. Oikos (1999), 398\u2013408.",
      "doi": ""
    },
    {
      "text": "Nina Grgi\u0107-Hla\u010da, Christoph Engel, and Krishna\u00a0P Gummadi. 2019. Human decision making with machine assistance: An experiment on bailing and jailing. Proceedings of the ACM on Human-Computer Interaction 3, CSCW(2019), 1\u201325.",
      "doi": ""
    },
    {
      "text": "Andrew\u00a0F Hayes. 2017. Introduction to mediation, moderation, and conditional process analysis: A regression-based approach. Guilford publications.",
      "doi": ""
    },
    {
      "text": "Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531(2015).",
      "doi": ""
    },
    {
      "text": "Fred Hohman, Kanit Wongsuphasawat, Mary\u00a0Beth Kery, and Kayur Patel. 2020. Understanding and visualizing data iteration in machine learning. In Proceedings of the 2020 CHI conference on human factors in computing systems. 1\u201313.",
      "doi": "10.1145/3313831.3376177"
    },
    {
      "text": "Donald Honeycutt, Mahsan Nourani, and Eric Ragan. 2020. Soliciting human-in-the-loop user feedback for interactive machine learning reduces user trust and impressions of model accuracy. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, Vol.\u00a08. 63\u201372.",
      "doi": ""
    },
    {
      "text": "Rick\u00a0H Hoyle. 1995. Structural equation modeling: Concepts, issues, and applications. Sage.",
      "doi": ""
    },
    {
      "text": "Karl\u00a0G J\u00f6reskog. 1969. A general approach to confirmatory maximum likelihood factor analysis. Psychometrika 34, 2 (1969), 183\u2013202.",
      "doi": ""
    },
    {
      "text": "Jongbin Jung, Connor Concannon, Ravi Shroff, Sharad Goel, and Daniel\u00a0G Goldstein. 2017. Simple rules for complex decisions. Available at SSRN 2919024(2017).",
      "doi": ""
    },
    {
      "text": "John\u00a0G Kemeny. 1959. Mathematics without numbers. Daedalus 88, 4 (1959), 577\u2013591.",
      "doi": ""
    },
    {
      "text": "Been Kim, Rajiv Khanna, and Oluwasanmi\u00a0O Koyejo. 2016. Examples are not enough, learn to criticize! criticism for interpretability. In Advances in neural information processing systems. 2280\u20132288.",
      "doi": ""
    },
    {
      "text": "Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, 2018. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). In International conference on machine learning. PMLR, 2668\u20132677.",
      "doi": ""
    },
    {
      "text": "James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei\u00a0A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, 2017. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences 114, 13(2017), 3521\u20133526.",
      "doi": ""
    },
    {
      "text": "Pang\u00a0Wei Koh and Percy Liang. 2017. Understanding black-box predictions via influence functions. arXiv preprint arXiv:1703.04730(2017).",
      "doi": ""
    },
    {
      "text": "Satyapriya Krishna, Tessa Han, Alex Gu, Javin Pombra, Shahin Jabbari, Steven Wu, and Himabindu Lakkaraju. 2022. The Disagreement Problem in Explainable Machine Learning: A Practitioner\u2019s Perspective. arXiv preprint arXiv:2202.01602(2022).",
      "doi": ""
    },
    {
      "text": "Todd Kulesza, Margaret Burnett, Weng-Keen Wong, and Simone Stumpf. 2015. Principles of explanatory debugging to personalize interactive machine learning. In Proceedings of the 20th international conference on intelligent user interfaces. 126\u2013137.",
      "doi": "10.1145/2678025.2701399"
    },
    {
      "text": "Vivian Lai, Jon\u00a0Z. Cai, and Chenhao Tan. 2019. Many Faces of Feature Importance: Comparing Built-in and Post-hoc Feature Importance in Text Classification. In Proceedings of EMNLP.",
      "doi": ""
    },
    {
      "text": "Vivian Lai, Han Liu, and Chenhao Tan. 2020. \" Why is\u2019 Chicago\u2019deceptive?\" Towards Building Model-Driven Tutorials for Humans. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. 1\u201313.",
      "doi": "10.1145/3313831.3376873"
    },
    {
      "text": "Himabindu Lakkaraju, Stephen\u00a0H Bach, and Jure Leskovec. 2016. Interpretable decision sets: A joint framework for description and prediction. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. 1675\u20131684.",
      "doi": "10.1145/2939672.2939874"
    },
    {
      "text": "Piyawat Lertvittayakumjorn and Francesca Toni. 2021. Explanation-based human debugging of nlp models: A survey. Transactions of the Association for Computational Linguistics 9 (2021), 1508\u20131528.",
      "doi": ""
    },
    {
      "text": "Ariel Levy, Monica Agrawal, Arvind Satyanarayan, and David Sontag. 2021. Assessing the impact of automated suggestions on decision making: Domain experts mediate model errors but take less initiative. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 1\u201313.",
      "doi": "10.1145/3411764.3445522"
    },
    {
      "text": "Han Liu, Vivian Lai, and Chenhao Tan. 2021. Understanding the effect of out-of-distribution examples and interactive explanations on human-ai decision making. Proceedings of the ACM on Human-Computer Interaction 5, CSCW2(2021), 1\u201345.",
      "doi": "10.1145/3479552"
    },
    {
      "text": "Zhuoran Lu and Ming Yin. 2021. Human Reliance on Machine Learning Models When Performance Feedback is Limited: Heuristics and Risks. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 1\u201316.",
      "doi": "10.1145/3411764.3445562"
    },
    {
      "text": "Scott\u00a0M Lundberg and Su-In Lee. 2017. A Unified Approach to Interpreting Model Predictions. In Advances in Neural Information Processing Systems 30, I.\u00a0Guyon, U.\u00a0V. Luxburg, S.\u00a0Bengio, H.\u00a0Wallach, R.\u00a0Fergus, S.\u00a0Vishwanathan, and R.\u00a0Garnett (Eds.). Curran Associates, Inc., 4765\u20134774. http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf",
      "doi": ""
    },
    {
      "text": "Scott\u00a0M Lundberg and Su-In Lee. 2017. A unified approach to interpreting model predictions. In Advances in neural information processing systems. 4765\u20134774.",
      "doi": ""
    },
    {
      "text": "Robert\u00a0C MacCallum and James\u00a0T Austin. 2000. Applications of structural equation modeling in psychological research. Annual review of psychology 51 (2000).",
      "doi": ""
    },
    {
      "text": "Gr\u00e9goire Montavon, Jacob Kauffmann, Wojciech Samek, and Klaus-Robert M\u00fcller. 2022. Explaining the predictions of unsupervised learning models. In International Workshop on Extending Explainable AI Beyond Deep Models and Classifiers. Springer, 117\u2013138.",
      "doi": "10.1007/978-3-031-04083-2_7"
    },
    {
      "text": "Michael Neely, Stefan\u00a0F Schouten, Maurits\u00a0JR Bleeker, and Ana Lucic. 2021. Order in the court: Explainable AI methods prone to disagreement. arXiv preprint arXiv:2105.03287(2021).",
      "doi": ""
    },
    {
      "text": "Mahsan Nourani, Samia Kabir, Sina Mohseni, and Eric\u00a0D Ragan. 2019. The effects of meaningful and meaningless explanations on trust and perceived system accuracy in intelligent systems. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, Vol.\u00a07. 97\u2013105.",
      "doi": ""
    },
    {
      "text": "Mahsan Nourani, Joanie King, and Eric Ragan. 2020. The role of domain expertise in user trust and the impact of first impressions with intelligent systems. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, Vol.\u00a08. 112\u2013121.",
      "doi": ""
    },
    {
      "text": "Jeroen Ooge, Shotallo Kato, and Katrien Verbert. 2022. Explaining Recommendations in E-Learning: Effects on Adolescents\u2019 Trust. In 27th International Conference on Intelligent User Interfaces. 93\u2013105.",
      "doi": ""
    },
    {
      "text": "Michael Pazzani, Severine Soltani, Robert Kaufman, Samson Qian, and Albert Hsiao. 2022. Expert-Informed, User-Centric Explanations for Machine Learning. (2022).",
      "doi": ""
    },
    {
      "text": "Forough Poursabzi-Sangdeh, Daniel\u00a0G Goldstein, Jake\u00a0M Hofman, Jennifer\u00a0Wortman Wortman\u00a0Vaughan, and Hanna Wallach. 2021. Manipulating and measuring model interpretability. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 1\u201352.",
      "doi": "10.1145/3411764.3445315"
    },
    {
      "text": "Amy Rechkemmer and Ming Yin. 2022. When Confidence Meets Accuracy: Exploring the Effects of Multiple Performance Indicators on Trust in Machine Learning Models. In CHI Conference on Human Factors in Computing Systems. 1\u201314.",
      "doi": ""
    },
    {
      "text": "General Data\u00a0Protection Regulation. 2016. Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46. Official Journal of the European Union (OJ) 59, 1-88 (2016), 294.",
      "doi": ""
    },
    {
      "text": "Marco\u00a0Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. \" Why should i trust you?\" Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. 1135\u20131144.",
      "doi": "10.1145/2939672.2939778"
    },
    {
      "text": "Marco\u00a0Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. \" Why should I trust you?\" Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. 1135\u20131144.",
      "doi": "10.1145/2939672.2939778"
    },
    {
      "text": "Edward\u00a0E Rigdon. 1998. Advanced structural equation modeling: issues and techniques. Applied Psychological Measurement 22, 1 (1998), 85\u201387.",
      "doi": ""
    },
    {
      "text": "William\u00a0H Riker. 1988. Liberalism against populism: A confrontation between the theory of democracy and the theory of social choice. Waveland press.",
      "doi": ""
    },
    {
      "text": "Yves Rosseel. 2012. lavaan: An R package for structural equation modeling. Journal of statistical software 48 (2012), 1\u201336.",
      "doi": ""
    },
    {
      "text": "Derek\u00a0D Rucker, Kristopher\u00a0J Preacher, Zakary\u00a0L Tormala, and Richard\u00a0E Petty. 2011. Mediation analysis in social psychology: Current practices and new recommendations. Social and personality psychology compass 5, 6 (2011), 359\u2013371.",
      "doi": ""
    },
    {
      "text": "Cynthia Rudin. 2019. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence 1, 5 (2019), 206\u2013215.",
      "doi": ""
    },
    {
      "text": "James Schaffer, John O\u2019Donovan, James Michaelis, Adrienne Raglin, and Tobias H\u00f6llerer. 2019. I can do better than your AI: expertise and explanations. In Proceedings of the 24th International Conference on Intelligent User Interfaces. 240\u2013251.",
      "doi": "10.1145/3301275.3302308"
    },
    {
      "text": "Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2013. Deep inside convolutional networks: Visualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034(2013).",
      "doi": ""
    },
    {
      "text": "Alison Smith-Renner, Ron Fan, Melissa Birchfield, Tongshuang Wu, Jordan Boyd-Graber, Daniel\u00a0S Weld, and Leah Findlater. 2020. No explainability without accountability: An empirical study of explanations and feedback in interactive ml. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. 1\u201313.",
      "doi": "10.1145/3313831.3376624"
    },
    {
      "text": "Thilo Spinner, Udo Schlegel, Hanna Sch\u00e4fer, and Mennatallah El-Assady. 2019. explAIner: A visual analytics framework for interactive and explainable machine learning. IEEE transactions on visualization and computer graphics 26, 1(2019), 1064\u20131074.",
      "doi": ""
    },
    {
      "text": "Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic attribution for deep networks. In International conference on machine learning. PMLR, 3319\u20133328.",
      "doi": ""
    },
    {
      "text": "Maxwell Szymanski, Katrien Verbert, and Vero Vanden\u00a0Abeele. 2022. Designing and evaluating explainable AI for non-AI experts: challenges and opportunities. In Proceedings of the 16th ACM Conference on Recommender Systems. 735\u2013736.",
      "doi": "10.1145/3523227.3547427"
    },
    {
      "text": "Stefano Teso and Kristian Kersting. 2019. Explanatory interactive machine learning. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society. 239\u2013245.",
      "doi": "10.1145/3306618.3314293"
    },
    {
      "text": "Suzanne Tolmeijer, Ujwal Gadiraju, Ramya Ghantasala, Akshit Gupta, and Abraham Bernstein. 2021. Second chance for a first impression? Trust development in intelligent system interaction. In Proceedings of the 29th ACM Conference on user modeling, adaptation and personalization. 77\u201387.",
      "doi": "10.1145/3450613.3456817"
    },
    {
      "text": "Sandra Wachter, Brent Mittelstadt, and Chris Russell. 2017. Counterfactual explanations without opening the black box: Automated decisions and the GDPR. Harv. JL & Tech. 31(2017), 841.",
      "doi": ""
    },
    {
      "text": "Fulton Wang and Cynthia Rudin. 2015. Falling rule lists. In Artificial Intelligence and Statistics. 1013\u20131022.",
      "doi": ""
    },
    {
      "text": "Xinru Wang and Ming Yin. 2021. Are explanations helpful? a comparative study of the effects of explanations in ai-assisted decision-making. In 26th International Conference on Intelligent User Interfaces. 318\u2013328.",
      "doi": "10.1145/3397481.3450650"
    },
    {
      "text": "Xinru Wang and Ming Yin. 2022. Effects of Explanations in AI-Assisted Decision Making: Principles and Comparisons. ACM Transactions on Interactive Intelligent Systems (TiiS) (2022).",
      "doi": ""
    },
    {
      "text": "Gesa Wiegand, Malin Eiband, Maximilian Haubelt, and Heinrich Hussmann. 2020. \u201cI\u2019d like an Explanation for That!\u201d Exploring Reactions to Unexpected Autonomous Driving. In 22nd International Conference on Human-Computer Interaction with Mobile Devices and Services. 1\u201311.",
      "doi": ""
    },
    {
      "text": "Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. 2015. Show, attend and tell: Neural image caption generation with visual attention. In International conference on machine learning. 2048\u20132057.",
      "doi": "10.5555/3045118.3045336"
    },
    {
      "text": "Fumeng Yang, Zhuanyi Huang, Jean Scholtz, and Dustin\u00a0L Arendt. 2020. How do visual explanations foster end users\u2019 appropriate trust in machine learning?. In Proceedings of the 25th International Conference on Intelligent User Interfaces. 189\u2013201.",
      "doi": "10.1145/3377325.3377480"
    },
    {
      "text": "Yash. 2020. Lending club 2007-2020q3 | Kaggle. https://www.kaggle.com/ethon0426/lending-club-20072020q1?select=Loan_status_2007-2020Q3.gzip",
      "doi": ""
    },
    {
      "text": "Ming Yin, Jennifer Wortman\u00a0Vaughan, and Hanna Wallach. 2019. Understanding the effect of accuracy on trust in machine learning models. In Proceedings of the 2019 chi conference on human factors in computing systems. 1\u201312.",
      "doi": "10.1145/3290605.3300509"
    },
    {
      "text": "H\u00a0Peyton Young. 1988. Condorcet\u2019s theory of voting. American Political science review 82, 4 (1988), 1231\u20131244.",
      "doi": ""
    },
    {
      "text": "Yunfeng Zhang, Q\u00a0Vera Liao, and Rachel\u00a0KE Bellamy. 2020. Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. 295\u2013305.",
      "doi": "10.1145/3351095.3372852"
    }
  ]
}