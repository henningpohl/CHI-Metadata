{
  "doi": "10.1145/3544548.3581463",
  "title": "Out of Context: Investigating the Bias and Fairness Concerns of \u201cArtificial Intelligence as a Service\u201d",
  "published": "2023-04-19",
  "proctitle": "CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems",
  "pages": "1-17",
  "year": 2023,
  "badges": [],
  "abstract": "\u201cAI as a Service\u201d (AIaaS) is a rapidly growing market, offering various plug-and-play AI services and tools. AIaaS enables its customers (users)\u2014who may lack the expertise, data, and/or resources to develop their own systems\u2014to easily build and integrate AI capabilities into their applications. Yet, it is known that AI systems can encapsulate biases and inequalities that can have societal impact. This paper argues that the context-sensitive nature of fairness is often incompatible with AIaaS\u2019 \u2018one-size-fits-all\u2019 approach, leading to issues and tensions. Specifically, we review and systematise the AIaaS space by proposing a taxonomy of AI services based on the levels of autonomy afforded to the user. We then critically examine the different categories of AIaaS, outlining how these services can lead to biases or be otherwise harmful in the context of end-user applications. In doing so, we seek to draw research attention to the challenges of this emerging area.",
  "tags": [
    "bias",
    "data-driven",
    "machine learning",
    "cloud",
    "fairness",
    "MLaaS",
    "algorithmic supply chains",
    "accountability",
    "AIaaS",
    "artificial intelligence"
  ],
  "authors": [
    {
      "name": "Kornel Lewicki",
      "institution": "University of Cambridge, United Kingdom",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99660779876",
      "orcid": "0000-0002-7127-6894"
    },
    {
      "name": "Michelle Seng Ah Lee",
      "institution": "Compliant & Accountable Systems Group, University of Cambridge, United Kingdom",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659681738",
      "orcid": "0000-0001-7725-2503"
    },
    {
      "name": "Jennifer Cobbe",
      "institution": "Compliant & Accountable Systems Group, University of Cambridge, United Kingdom",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659321699",
      "orcid": "0000-0001-8912-4760"
    },
    {
      "name": "Jatinder Singh",
      "institution": "Compliant & Accountable Systems Group, University of Cambridge, United Kingdom",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659320789",
      "orcid": "0000-0002-5102-6564"
    }
  ],
  "references": [
    {
      "text": "4Paradigm. 2022. 4Paradigm Sage HyperCycle ML. https://www.4paradigm.com/product/hypercycleml.",
      "doi": ""
    },
    {
      "text": "Jermias Adams-Prassl, Reuben Binns, and Aislinn Kelly-Lyth. 2022. Directly Discriminatory Algorithms. Modern Law Review (2022). https://doi.org/10.1111/1468-2230.12759",
      "doi": ""
    },
    {
      "text": "Alekh Agarwal, Alina Beygelzimer, Miroslav Dud\u00edk, John Langford, and Hanna Wallach. 2018. A reductions approach to fair classification. In International Conference on Machine Learning. PMLR, 60\u201369.",
      "doi": ""
    },
    {
      "text": "Nur Ahmed and Muntasir Wahed. 2020. The De-democratization of AI: Deep Learning and the Compute Divide in Artificial Intelligence Research. arXiv preprint arXiv:2010.15581(2020).",
      "doi": ""
    },
    {
      "text": "Aidence. 2022. AI-powered clinical applications for the oncology pathway. https://www.aidence.com/.",
      "doi": ""
    },
    {
      "text": "Amazon. 2021. Amazon SageMaker Autopilot. https://aws.amazon.com/sagemaker/autopilot/.",
      "doi": ""
    },
    {
      "text": "Amazon. 2021. Artificial Intelligence Services. https://aws.amazon.com/machine-learning/ai-services/.",
      "doi": ""
    },
    {
      "text": "Skin Analytics. 2022. Providing AI supported dermatology solutions in partnership with the NHS.https://skin-analytics.com.",
      "doi": ""
    },
    {
      "text": "Mike Ananny and Kate Crawford. 2018. Seeing without knowing: Limitations of the transparency ideal and its application to algorithmic accountability. New Media and Society 20, 3 (2018), 973\u2013989. https://doi.org/10.1177/1461444816676645",
      "doi": ""
    },
    {
      "text": "Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2019. Machine bias: There\u2019s software used across the country to predict future criminals. and it\u2019s biased against blacks. 2016. URL https://www. propublica. org/article/machine-bias-risk-assessments-in-criminal-sentencing(2019).",
      "doi": ""
    },
    {
      "text": "Ariful\u00a0Islam Anik and Andrea Bunt. 2021. Data-centric explanations: explaining training data of machine learning systems to promote transparency. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 1\u201313.",
      "doi": "10.1145/3411764.3445736"
    },
    {
      "text": "Matthew Arnold, Rachel\u00a0KE Bellamy, Michael Hind, Stephanie Houde, Sameep Mehta, Aleksandra Mojsilovi\u0107, Ravi Nair, K\u00a0Natesan Ramamurthy, Alexandra Olteanu, David Piorkowski, 2019. FactSheets: Increasing trust in AI services through supplier\u2019s declarations of conformity. IBM Journal of Research and Development 63, 4/5 (2019), 6\u20131.",
      "doi": ""
    },
    {
      "text": "Baidu. 2021. Baidu AI Open Platform. https://ai.baidu.com/tech/face/detect.",
      "doi": ""
    },
    {
      "text": "Rachel\u00a0KE Bellamy, Kuntal Dey, Michael Hind, Samuel\u00a0C Hoffman, Stephanie Houde, Kalapriya Kannan, Pranay Lohia, Jacquelyn Martino, Sameep Mehta, Aleksandra Mojsilovi\u0107, 2019. AI Fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias. IBM Journal of Research and Development 63, 4/5 (2019), 4\u20131.",
      "doi": ""
    },
    {
      "text": "Emily\u00a0M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. 610\u2013623.",
      "doi": "10.1145/3442188.3445922"
    },
    {
      "text": "Reuben Binns, Max Van\u00a0Kleek, Michael Veale, Ulrik Lyngs, Jun Zhao, and Nigel Shadbolt. 2018. \u2018It\u2019s Reducing a Human Being to a Percentage\u2019 Perceptions of Justice in Algorithmic Decisions. In Proceedings of the 2018 CHI conference on Human Factors in Computing Systems. 1\u201314.",
      "doi": "10.1145/3173574.3173951"
    },
    {
      "text": "Clare Birchall. 2014. Radical transparency?Cultural Studies? Critical Methodologies 14, 1 (2014), 77\u201388.",
      "doi": ""
    },
    {
      "text": "Sarah Bird, Miro Dud\u00edk, Richard Edgar, Brandon Horn, Roman Lutz, Vanessa Milan, Mehrnoosh Sameki, Hanna Wallach, and Kathleen Walker. 2020. Fairlearn: A toolkit for assessing and improving fairness in AI. Microsoft, Tech. Rep. MSR-TR-2020-32(2020).",
      "doi": ""
    },
    {
      "text": "Philip Bobko and Philip\u00a0L Roth. 2004. The four-fifths rule for assessing adverse impact: An arithmetic, intuitive, and logical analysis of the rule and implications for future research and practice. In Research in personnel and human resources management. Emerald Group Publishing Limited.",
      "doi": ""
    },
    {
      "text": "Tolga Bolukbasi, Kai-Wei Chang, James\u00a0Y Zou, Venkatesh Saligrama, and Adam\u00a0T Kalai. 2016. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. Advances in neural information processing systems 29 (2016).",
      "doi": ""
    },
    {
      "text": "Tom\u00a0B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165(2020).",
      "doi": ""
    },
    {
      "text": "Joy Buolamwini and Timnit Gebru. 2018. Gender Shades: Intersectional accuracy disparities in commercial gender classification. In Conference on Fairness, Accountability and Transparency, Vol.\u00a081. PMLR, 77\u201391.",
      "doi": ""
    },
    {
      "text": "Corinne Cath. 2018. Governing artificial intelligence: ethical, legal and technical opportunities and challenges., 20180080\u00a0pages.",
      "doi": ""
    },
    {
      "text": "Robert Challen, Joshua Denny, Martin Pitt, Luke Gompels, Tom Edwards, and Krasimira Tsaneva-Atanasova. 2019. Artificial Intelligence, bias and clinical safety. BMJ Quality & Safety 28, 3 (2019), 231\u2013237.",
      "doi": ""
    },
    {
      "text": "Joseph Chin, Aifaz Gowani, Gabriel James, and Matthew Peng. 2020. The death of data scientists - will automl replace them?https://www.kdnuggets.com/2020/02/data-scientists-automl-replace.html",
      "doi": ""
    },
    {
      "text": "Alexandra Chouldechova. 2017. Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. Big data 5, 2 (2017), 153\u2013163.",
      "doi": ""
    },
    {
      "text": "Jennifer Cobbe, Michelle Seng\u00a0Ah Lee, and Jatinder Singh. 2021. Reviewable Automated Decision-Making: A Framework for Accountable Algorithmic Systems. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. 598\u2013609.",
      "doi": "10.1145/3442188.3445921"
    },
    {
      "text": "Jennifer Cobbe, Chris Norval, and Jatinder Singh. 2020. What lies beneath: transparency in online service supply chains. Journal of Cyber Policy 5, 1 (2020), 65\u201393. https://doi.org/10.1080/23738871.2020.1745860",
      "doi": ""
    },
    {
      "text": "Jennifer Cobbe and Jatinder Singh. 2021. Artificial intelligence as a Service: Legal responsibilities, liabilities, and policy challenges. Computer Law & Security Review 42 (2021), 105573.",
      "doi": ""
    },
    {
      "text": "European Commission. 2021. Proposal for regulation of the European parliament and of the council - Laying down harmonised rules on Artificial Intelligence (Artificial Intelligence Act) and amending certain Union legislative acts. https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52021PC0206.",
      "doi": ""
    },
    {
      "text": "EU Commission 2021. Proposal for a regulation of the European Parliament and of the Council laying down harmonised rules on artificial intelligence (Artificial Intelligence Act) and amending certain Union legislative acts. COM (2021) 206(2021).",
      "doi": ""
    },
    {
      "text": "McKinsey\u00a0& Company. 2020. The state of AI in 2020. https://www.mckinsey.com/Business-Functions/McKinsey-Analytics/Our-Insights/Global-survey-The-state-of-AI-in-2020.",
      "doi": ""
    },
    {
      "text": "Kate Crawford. 2016. Artificial intelligence\u2019s white guy problem. The New York Times 25, 06 (2016).",
      "doi": ""
    },
    {
      "text": "Andr\u00e9\u00a0F Cruz, Pedro Saleiro, Catarina Bel\u00e9m, Carlos Soares, and Pedro Bizarro. 2020. A Bandit-Based Algorithm for Fairness-Aware Hyperparameter Optimization. arXiv preprint arXiv:2010.03665(2020).",
      "doi": ""
    },
    {
      "text": "Marija Cubric. 2020. Drivers, barriers and social considerations for AI adoption in business and management: A tertiary study. Technology in Society 62(2020), 101257.",
      "doi": ""
    },
    {
      "text": "Allan Dafoe. 2018. AI governance: a research agenda. Governance of AI Program, Future of Humanity Institute, University of Oxford: Oxford, UK (2018).",
      "doi": ""
    },
    {
      "text": "Thomas Davenport and Ravi Kalakota. 2019. The potential for artificial intelligence in healthcare. Future healthcare journal 6, 2 (2019), 94.",
      "doi": ""
    },
    {
      "text": "Terrance De\u00a0Vries, Ishan Misra, Changhan Wang, and Laurens Van\u00a0der Maaten. 2019. Does object recognition work for everyone?. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. 52\u201359.",
      "doi": ""
    },
    {
      "text": "Wesley\u00a0Hanwen Deng, Bill\u00a0Boyuan Guo, Alicia Devos, Hong Shen, Motahhare Eslami, and Kenneth Holstein. 2022. Understanding Practices, Challenges, and Opportunities for User-Driven Algorithm Auditing in Industry Practice. arXiv preprint arXiv:2210.03709(2022).",
      "doi": ""
    },
    {
      "text": "Wesley\u00a0Hanwen Deng, Manish Nagireddy, Michelle Seng\u00a0Ah Lee, Jatinder Singh, Zhiwei\u00a0Steven Wu, Kenneth Holstein, and Haiyi Zhu. 2022. Exploring How Machine Learning Practitioners (Try To) Use Fairness Toolkits. (2022), 473\u2013484. https://doi.org/10.1145/3531146.3533113",
      "doi": "10.1145/3531146.3533113"
    },
    {
      "text": "Emily Denton, Ben Hutchinson, Margaret Mitchell, Timnit Gebru, and Andrew Zaldivar. 2019. Image counterfactual sensitivity analysis for detecting unintended bias. arXiv preprint arXiv:1906.06439(2019).",
      "doi": ""
    },
    {
      "text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805(2018).",
      "doi": ""
    },
    {
      "text": "Alicia DeVos, Aditi Dhabalia, Hong Shen, Kenneth Holstein, and Motahhare Eslami. 2022. Toward User-Driven Algorithm Auditing: Investigating users\u2019 strategies for uncovering harmful algorithmic behavior. In CHI Conference on Human Factors in Computing Systems. 1\u201319.",
      "doi": "10.1145/3491102.3517441"
    },
    {
      "text": "Yogesh\u00a0K Dwivedi, Laurie Hughes, Elvira Ismagilova, Gert Aarts, Crispin Coombs, Tom Crick, Yanqing Duan, Rohita Dwivedi, John Edwards, Aled Eirug, 2019. Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy. International Journal of Information Management (2019), 101994.",
      "doi": ""
    },
    {
      "text": "Yogesh\u00a0K Dwivedi, Laurie Hughes, Elvira Ismagilova, Gert Aarts, Crispin Coombs, Tom Crick, Yanqing Duan, Rohita Dwivedi, John Edwards, Aled Eirug, 2021. Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy. International Journal of Information Management 57 (2021), 101994.",
      "doi": ""
    },
    {
      "text": "Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. 2012. Fairness through awareness. In Proceedings of the Third Innovations in Theoretical Computer Science Conference. 214\u2013226.",
      "doi": "10.1145/2090236.2090255"
    },
    {
      "text": "Upol Ehsan, Q\u00a0Vera Liao, Michael Muller, Mark\u00a0O Riedl, and Justin\u00a0D Weisz. 2021. Expanding explainability: Towards social transparency in AI systems. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 1\u201319.",
      "doi": "10.1145/3411764.3445188"
    },
    {
      "text": "Hugging Face. 2022. The AI community building the future.https://huggingface.co.",
      "doi": ""
    },
    {
      "text": "Federal Trade Commission, USA. 2021. Aiming for truth, fairness, and equity in your company\u2019s use of AI | Federal Trade Commission. https://www.ftc.gov/news-events/blogs/business-blog/2021/04/aiming-truth-fairness-equity-your-companys-use-ai.",
      "doi": ""
    },
    {
      "text": "Michael Feldman, Sorelle\u00a0A Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubramanian. 2015. Certifying and removing disparate impact. In proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining. 259\u2013268.",
      "doi": "10.1145/2783258.2783311"
    },
    {
      "text": "Sorelle\u00a0A Friedler, Carlos Scheidegger, and Suresh Venkatasubramanian. 2016. On the (im) possibility of fairness. arXiv preprint arXiv:1609.07236(2016).",
      "doi": ""
    },
    {
      "text": "Megan Garcia. 2016. Racist in the machine: The disturbing implications of algorithmic bias. World Policy Journal 33, 4 (2016), 111\u2013117.",
      "doi": ""
    },
    {
      "text": "Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer\u00a0Wortman Vaughan, Hanna Wallach, Hal Daum\u00e9\u00a0III, and Kate Crawford. 2018. Datasheets for datasets. arXiv preprint arXiv:1803.09010(2018).",
      "doi": ""
    },
    {
      "text": "Markos Georgopoulos, James Oldfield, Mihalis\u00a0A Nicolaou, Yannis Panagakis, and Maja Pantic. 2021. Mitigating demographic bias in facial datasets with style-based multi-attribute transfer. International Journal of Computer Vision 129, 7 (2021), 2288\u20132307.",
      "doi": "10.1007/s11263-021-01448-w"
    },
    {
      "text": "Google. 2021. Cloud AI Building Blocks. https://cloud.google.com/products/ai/building-blocks.",
      "doi": ""
    },
    {
      "text": "Google. 2022. Cloud AutoML Custom Machine Learning Models | Google Cloud. https://cloud.google.com/automl.",
      "doi": ""
    },
    {
      "text": "Ben Green and Lily Hu. 2018. The Myth in the Methodology: Towards a Recontextualization of Fairness in Machine Learning. Machine Learning: The Debates workshop at the 35th International Conference on Machine Learning (ICML).",
      "doi": ""
    },
    {
      "text": "Patrick Grother, Mei Ngan, and Kayee Hanaoka. 2019. Face Recognition Vendor Test (FVRT): Part 3, Demographic Effects. National Institute of Standards and Technology.",
      "doi": ""
    },
    {
      "text": "Stanford HAI. 2022. The Geographic Bias in Medical AI Tools. https://hai.stanford.edu/news/geographic-bias-medical-ai-tools.",
      "doi": ""
    },
    {
      "text": "Mark Haranas. 2022. AWS, Microsoft, Google Top Cloud Ai Developer Market: Gartner. https://www.crn.com/slide-shows/cloud/aws-microsoft-google-top-cloud-ai-developer-market-gartner",
      "doi": ""
    },
    {
      "text": "Moritz Hardt, Eric Price, and Nathan Srebro. 2016. Equality of opportunity in supervised learning. arXiv preprint arXiv:1610.02413(2016).",
      "doi": ""
    },
    {
      "text": "Headspace. 2022. Headspace Health. https://www.headspace.com/health.",
      "doi": ""
    },
    {
      "text": "Amy\u00a0K Heger, Liz\u00a0B Marquis, Mihaela Vorvoreanu, Hanna Wallach, and Jennifer Wortman\u00a0Vaughan. 2022. Understanding Machine Learning Practitioners\u2019 Data Documentation Perceptions, Needs, Challenges, and Desiderata. Proceedings of the ACM on Human-Computer Interaction 6, CSCW2(2022), 1\u201329.",
      "doi": "10.1145/3555760"
    },
    {
      "text": "Lisa\u00a0Anne Hendricks, Kaylee Burns, Kate Saenko, Trevor Darrell, and Anna Rohrbach. 2018. Women also snowboard: Overcoming bias in captioning models. In Proceedings of the European Conference on Computer Vision (ECCV). 771\u2013787.",
      "doi": "10.1007/978-3-030-01219-9_47"
    },
    {
      "text": "HireVue. 2022. End-to-End Hiring Experience Platform: Video Interviewing, Conversational AI & More | HireVue. https://www.hirevue.com/.",
      "doi": ""
    },
    {
      "text": "Kenneth Holstein, Jennifer Wortman\u00a0Vaughan, Hal Daum\u00e9\u00a0III, Miro Dudik, and Hanna Wallach. 2019. Improving fairness in machine learning systems: What do industry practitioners need?. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. 1\u201316.",
      "doi": "10.1145/3290605.3300830"
    },
    {
      "text": "Ayanna Howard and Jason Borenstein. 2018. The ugly truth about ourselves and our robot creations: the problem of bias and social inequity. Science and Engineering Ethics 24, 5 (2018), 1521\u20131536.",
      "doi": ""
    },
    {
      "text": "IBM. 2021. IBM Watson products and solutions. https://www.ibm.com/uk-en/watson/products-services.",
      "doi": ""
    },
    {
      "text": "IBM. 2022. Watson OpenScale on Cloud Pak for Data. https://www.ibm.com/docs/en/cloud-paks/cp-data/3.5.0?topic=services-watson-openscale.",
      "doi": ""
    },
    {
      "text": "Infermedica. 2022. Call Center Triage. https://infermedica.com/product/call-center-triage.",
      "doi": ""
    },
    {
      "text": "Seyyed\u00a0Ahmad Javadi, Richard Cloete, Jennifer Cobbe, Michelle Seng\u00a0Ah Lee, and Jatinder Singh. 2020. Monitoring Misuse for Accountable \u2018Artificial Intelligence as a Service\u2019. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society. 300\u2013306.",
      "doi": "10.1145/3375627.3375873"
    },
    {
      "text": "Seyyed\u00a0Ahmad Javadi, Chris Norval, Richard Cloete, and Jatinder Singh. 2021. Monitoring AI Services for Misuse. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society. 597\u2013607.",
      "doi": "10.1145/3461702.3462566"
    },
    {
      "text": "Haifeng Jin, Qingquan Song, and Xia Hu. 2019. Auto-keras: An efficient neural architecture search system. In Proceedings of the 25th ACM SIGKDD international Conference on Knowledge Discovery & Data Mining. 1946\u20131956.",
      "doi": "10.1145/3292500.3330648"
    },
    {
      "text": "Anna Jobin, Marcello Ienca, and Effy Vayena. 2019. The global landscape of AI ethics guidelines. Nature Machine Intelligence 1, 9 (2019), 389\u2013399.",
      "doi": ""
    },
    {
      "text": "Matthew Joseph, Michael Kearns, Jamie Morgenstern, and Aaron Roth. 2016. Fairness in learning: Classic and contextual bandits. arXiv preprint arXiv:1605.07139(2016).",
      "doi": ""
    },
    {
      "text": "Dimitra Kamarinou, Christopher Millard, Jatinder Singh, and R Leenes. 2017. Machine learning with personal data. In Data protection and privacy: the age of intelligent machines. Hart Publishing.",
      "doi": ""
    },
    {
      "text": "Zaid Khan and Yun Fu. 2021. One Label, One Billion Faces: Usage and Consistency of Racial Categories in Computer Vision. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. 587\u2013597.",
      "doi": ""
    },
    {
      "text": "Udayan Khurana, Horst Samulowitz, and Deepak Turaga. 2018. Feature engineering for predictive modeling using reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol.\u00a032.",
      "doi": ""
    },
    {
      "text": "Ross\u00a0D. King, Cao Feng, and Alistair Sutherland. 1995. Statlog: comparison of classification algorithms on large real-world problems. Applied Artificial Intelligence an International Journal 9, 3(1995), 289\u2013333.",
      "doi": ""
    },
    {
      "text": "Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. 2016. Inherent trade-offs in the fair determination of risk scores. arXiv preprint arXiv:1609.05807(2016).",
      "doi": ""
    },
    {
      "text": "Allison Koenecke, Andrew Nam, Emily Lake, Joe Nudell, Minnie Quartey, Zion Mengesha, Connor Toups, John\u00a0R Rickford, Dan Jurafsky, and Sharad Goel. 2020. Racial disparities in automated speech recognition. Proceedings of the National Academy of Sciences 117, 14(2020), 7684\u20137689.",
      "doi": ""
    },
    {
      "text": "Ron Kohavi. 1996. Scaling up the accuracy of naive-bayes classifiers: A decision-tree hybrid.. In Kdd, Vol.\u00a096. 202\u2013207.",
      "doi": ""
    },
    {
      "text": "Lars Kotthoff, Chris Thornton, Holger\u00a0H Hoos, Frank Hutter, and Kevin Leyton-Brown. 2019. Auto-WEKA: Automatic model selection and hyperparameter optimization in WEKA. In Automated Machine Learning. Springer, Cham, 81\u201395.",
      "doi": ""
    },
    {
      "text": "PM Krafft, Meg Young, Michael Katell, Jennifer\u00a0E Lee, Shankar Narayan, Micah Epstein, Dharma Dailey, Bernease Herman, Aaron Tam, Vivian Guetler, 2021. An Action-Oriented AI Policy Toolkit for Technology Audits by Community Advocates and Activists. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. 772\u2013781.",
      "doi": "10.1145/3442188.3445938"
    },
    {
      "text": "Joshua A. Kroll, Joanna Huey, Solon Barocas, Edward W. Felten, Joel R. Reidenberg, David G. Robinson, and Harlan Yu. 2017. Accountable algorithms. University of Pennsylvania Law Review 165, 3 (Feb. 2017), 633\u2013705.",
      "doi": ""
    },
    {
      "text": "Joshua\u00a0A. Kroll. 2021. Outlining Traceability: A Principle for Operationalizing Accountability in Computing Systems. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (Virtual Event, Canada) (FAccT \u201921). Association for Computing Machinery, New York, NY, USA, 758\u2013771. https://doi.org/10.1145/3442188.3445937",
      "doi": "10.1145/3442188.3445937"
    },
    {
      "text": "Hoang\u00a0Thanh Lam, Johann-Michael Thiebaut, Mathieu Sinn, Bei Chen, Tiep Mai, and Oznur Alkan. 2017. One button machine for automating feature engineering in relational databases. arXiv preprint arXiv:1706.00327(2017).",
      "doi": ""
    },
    {
      "text": "Doris Jung\u00a0Lin Lee, Stephen Macke, Doris Xin, Angela Lee, Silu Huang, and Aditya\u00a0G Parameswaran. 2019. A Human-in-the-loop Perspective on AutoML: Milestones and the Road Ahead.IEEE Data Eng. Bull. 42, 2 (2019), 59\u201370.",
      "doi": ""
    },
    {
      "text": "Michelle Seng\u00a0Ah Lee and Luciano Floridi. 2021. Algorithmic fairness in mortgage lending: from absolute conditions to relational trade-offs. Minds and Machines 31(2021), 165\u2013191.",
      "doi": "10.1007/s11023-020-09529-4"
    },
    {
      "text": "Michelle Seng\u00a0Ah Lee, Luciano Floridi, and Jatinder Singh. 2021. Formalising trade-offs beyond algorithmic fairness: Lessons from ethical philosophy and welfare economics. AI and Ethics 1, 4 (2021), 529\u2013544.",
      "doi": ""
    },
    {
      "text": "Michelle Seng\u00a0Ah Lee and Jat Singh. 2021. The landscape and gaps in open source fairness toolkits. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 1\u201313.",
      "doi": "10.1145/3411764.3445261"
    },
    {
      "text": "Daphne Leprince-Ringuet. 2021. Low-code and no-code development is changing how software is built - and who builds it. https://www.zdnet.com/article/low-code-and-no-code-development-is-changing-how-software-is-built-and-who-builds-it/",
      "doi": ""
    },
    {
      "text": "Yang Liu, Goran Radanovic, Christos Dimitrakakis, Debmalya Mandal, and David\u00a0C Parkes. 2017. Calibrated fairness in bandits. arXiv preprint arXiv:1707.01875(2017).",
      "doi": ""
    },
    {
      "text": "Luminance. 2022. The Artificial Intelligence platform for the legal profession. https://www.luminance.com.",
      "doi": ""
    },
    {
      "text": "Lunit. 2022. AI will be the new standard of care. By Lunit.https://www.lunit.io/en.",
      "doi": ""
    },
    {
      "text": "Michael Madaio, Lisa Egede, Hariharan Subramonyam, Jennifer Wortman\u00a0Vaughan, and Hanna Wallach. 2022. Assessing the Fairness of AI Systems: AI Practitioners\u2019 Processes, Challenges, and Needs for Support. Proceedings of the ACM on Human-Computer Interaction 6, CSCW1(2022), 1\u201326.",
      "doi": "10.1145/3512899"
    },
    {
      "text": "Michael\u00a0A Madaio, Luke Stark, Jennifer Wortman\u00a0Vaughan, and Hanna Wallach. 2020. Co-designing checklists to understand organizational challenges and opportunities around fairness in ai. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. 1\u201314.",
      "doi": "10.1145/3313831.3376445"
    },
    {
      "text": "Gustavo Malkomes, Chip Schaff, and Roman Garnett. 2016. Bayesian optimization for automated model selection. In Workshop on Automatic Machine Learning. PMLR, 41\u201347.",
      "doi": ""
    },
    {
      "text": "Natalia Martinez, Martin Bertran, and Guillermo Sapiro. 2020. Minimax pareto fairness: A multi objective perspective. In International Conference on Machine Learning. PMLR, 6755\u20136764.",
      "doi": ""
    },
    {
      "text": "MEGVII. 2021. Megvii Face++ Artificial Intelligence Open Platform. https://www.faceplusplus.com.cn/beauty/.",
      "doi": ""
    },
    {
      "text": "Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. 2019. A survey on bias and fairness in machine learning. arXiv preprint arXiv:1908.09635(2019).",
      "doi": ""
    },
    {
      "text": "Dana\u00eb Metaxa, Joon\u00a0Sung Park, Ronald\u00a0E Robertson, Karrie Karahalios, Christo Wilson, Jeff Hancock, Christian Sandvig, 2021. Auditing algorithms: Understanding algorithmic systems from the outside in. Foundations and Trends\u00ae in Human\u2013Computer Interaction 14, 4(2021), 272\u2013344.",
      "doi": ""
    },
    {
      "text": "Microsoft. 2021. Cognitive Services. https://azure.microsoft.com/en-gb/services/cognitive-services/.",
      "doi": ""
    },
    {
      "text": "Microsoft. 2022. Automated Machine Learning | Microsoft Azure. https://azure.microsoft.com/en-gb/services/machine-learning/automatedml/.",
      "doi": ""
    },
    {
      "text": "Microsoft. 2022. Our approach to responsible AI at Microsoft. https://www.microsoft.com/en-us/ai/our-approach?activetab=pivot1:primaryr5.",
      "doi": ""
    },
    {
      "text": "Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa\u00a0Deborah Raji, and Timnit Gebru. 2019. Model cards for model reporting. In Proceedings of the conference on fairness, accountability, and transparency. 220\u2013229.",
      "doi": "10.1145/3287560.3287596"
    },
    {
      "text": "ModelZoo. 2022. Discover open source deep learning code and pretrained models.https://modelzoo.co.",
      "doi": ""
    },
    {
      "text": "Jakob M\u00f6kander, Prathm Juneja, David\u00a0S Watson, and Luciano Floridi. 2022. The US Algorithmic Accountability Act of 2022 vs. The EU Artificial Intelligence Act: what can they learn from each other?Minds and Machines (2022), 1\u20138.",
      "doi": ""
    },
    {
      "text": "Deirdre\u00a0K Mulligan, Joshua\u00a0A Kroll, Nitin Kohli, and Richmond\u00a0Y Wong. 2019. This thing called fairness: Disciplinary confusion realizing a value in technology. Proceedings of the ACM on Human-Computer Interaction 3, CSCW(2019), 1\u201336.",
      "doi": "10.1145/3359221"
    },
    {
      "text": "Chris Norval, Kristin Cornelius, Jennifer Cobbe, and Jatinder Singh. 2022. Disclosure by Design: Designing information disclosures to support meaningful transparency and accountability. In 2022 ACM Conference on Fairness, Accountability, and Transparency. 679\u2013690.",
      "doi": "10.1145/3531146.3533133"
    },
    {
      "text": "Nuance. 2022. AI Marketplace for Diagnostic Imaging - AI for Radiology. https://www.nuance.com/healthcare/diagnostics-solutions/ai-marketplace.html.",
      "doi": ""
    },
    {
      "text": "Ziad Obermeyer, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. Dissecting racial bias in an algorithm used to manage the health of populations. Science 366, 6464 (2019), 447\u2013453.",
      "doi": ""
    },
    {
      "text": "OECD. 2019. Recommendation of the Council on Artificial Intelligence. https://legalinstruments.oecd.org/en/instruments/OECD-LEGAL-0449.",
      "doi": ""
    },
    {
      "text": "United Nations.\u00a0Statistical Office. 1982. Provisional guidelines on standard international age classifications. New York: United Nations.",
      "doi": ""
    },
    {
      "text": "Randal\u00a0S Olson and Jason\u00a0H Moore. 2016. TPOT: A tree-based pipeline optimization tool for automating machine learning. In Workshop on Automatic Machine Learning. PMLR, 66\u201374.",
      "doi": ""
    },
    {
      "text": "Cathy O\u2019neil. 2016. Weapons of math destruction: How big data increases inequality and threatens democracy. Broadway Books.",
      "doi": "10.5555/3002861"
    },
    {
      "text": "OpenAI. 2022. ChatGPT. https://openai.com/blog/chatgpt/.",
      "doi": ""
    },
    {
      "text": "OpenAI. 2022. DALL-E 2. https://openai.com/dall-e-2/.",
      "doi": ""
    },
    {
      "text": "Seong\u00a0Ho Park and Kyunghwa Han. 2018. Methodologic guide for evaluating clinical performance and effect of artificial intelligence technology for medical diagnosis and prediction. Radiology 286, 3 (2018), 800\u2013809.",
      "doi": ""
    },
    {
      "text": "Valerio Perrone, Michele Donini, Muhammad\u00a0Bilal Zafar, Robin Schmucker, Krishnaram Kenthapadi, and C\u00e9dric Archambeau. 2021. Fair bayesian optimization. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society. 854\u2013863.",
      "doi": "10.1145/3461702.3462629"
    },
    {
      "text": "Florian Pfisterer, Stefan Coors, Janek Thomas, and Bernd Bischl. 2019. Multi-objective automatic machine learning with autoxgboostmc. arXiv preprint arXiv:1908.10796(2019).",
      "doi": ""
    },
    {
      "text": "Geoff Pleiss, Manish Raghavan, Felix Wu, Jon Kleinberg, and Kilian\u00a0Q Weinberger. 2017. On fairness and calibration. arXiv preprint arXiv:1709.02012(2017).",
      "doi": ""
    },
    {
      "text": "Pymetrics. 2021. Talent Matching Platform. https://www.pymetrics.ai/.",
      "doi": ""
    },
    {
      "text": "qure.ai. 2022. AI to enable accessible, affordable & timely care across the globe.https://qure.ai/.",
      "doi": ""
    },
    {
      "text": "Manish Raghavan, Solon Barocas, Jon Kleinberg, and Karen Levy. 2020. Mitigating bias in algorithmic hiring: Evaluating claims and practices. In Proceedings of the 2020 conference on fairness, accountability, and transparency. 469\u2013481.",
      "doi": "10.1145/3351095.3372828"
    },
    {
      "text": "Inioluwa\u00a0Deborah Raji and Joy Buolamwini. 2019. Actionable auditing: Investigating the impact of publicly naming biased performance results of commercial ai products. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society. 429\u2013435.",
      "doi": "10.1145/3306618.3314244"
    },
    {
      "text": "Inioluwa\u00a0Deborah Raji, Timnit Gebru, Margaret Mitchell, Joy Buolamwini, Joonseok Lee, and Emily Denton. 2020. Saving face: Investigating the ethical concerns of facial recognition auditing. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society. 145\u2013151.",
      "doi": "10.1145/3375627.3375820"
    },
    {
      "text": "Inioluwa\u00a0Deborah Raji, Andrew Smart, Rebecca\u00a0N White, Margaret Mitchell, Timnit Gebru, Ben Hutchinson, Jamila Smith-Loud, Daniel Theron, and Parker Barnes. 2020. Closing the AI accountability gap: defining an end-to-end framework for internal algorithmic auditing. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. 33\u201344.",
      "doi": "10.1145/3351095.3372873"
    },
    {
      "text": "Bogdana Rakova, Jingying Yang, Henriette Cramer, and Rumman Chowdhury. 2021. Where responsible AI meets reality: Practitioner perspectives on enablers for shifting organizational practices. Proceedings of the ACM on Human-Computer Interaction 5, CSCW1(2021), 1\u201323.",
      "doi": "10.1145/3449081"
    },
    {
      "text": "Grand\u00a0View Research. 2021. Artificial Intelligence as a Service: Market Research Report. https://www.grandviewresearch.com/industry-analysis/artificial-intelligence-as-a-service-market-report.",
      "doi": ""
    },
    {
      "text": "Nithya Sambasivan, Erin Arnesen, Ben Hutchinson, Tulsee Doshi, and Vinodkumar Prabhakaran. 2021. Re-imagining algorithmic fairness in india and beyond. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. 315\u2013328.",
      "doi": "10.1145/3442188.3445896"
    },
    {
      "text": "Javier S\u00e1nchez-Monedero, Lina Dencik, and Lilian Edwards. 2020. What does it mean to\u2019solve\u2019the problem of discrimination in hiring? Social, technical and legal perspectives from the UK on automated hiring systems. In Proceedings of the 2020 conference on Fairness, Accountability, and Transparency. 458\u2013468.",
      "doi": "10.1145/3351095.3372849"
    },
    {
      "text": "Morgan\u00a0Klaus Scheuerman, Jacob\u00a0M Paul, and Jed\u00a0R Brubaker. 2019. How computers see gender: An evaluation of gender classification in commercial facial analysis services. Proceedings of the ACM on Human-Computer Interaction 3, CSCW(2019), 1\u201333.",
      "doi": "10.1145/3359246"
    },
    {
      "text": "Morgan\u00a0Klaus Scheuerman, Kandrea Wade, Caitlin Lustig, and Jed\u00a0R Brubaker. 2020. How We\u2019ve Taught Algorithms to See Identity: Constructing Race and Gender in Image Databases for Facial Analysis. Proceedings of the ACM on Human-Computer Interaction 4, CSCW1(2020), 1\u201335.",
      "doi": "10.1145/3392866"
    },
    {
      "text": "Daniel Schiff, Justin Biddle, Jason Borenstein, and Kelly Laas. 2020. What\u2019s Next for AI Ethics, Policy, and Governance? A Global Overview. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society. 153\u2013158.",
      "doi": "10.1145/3375627.3375804"
    },
    {
      "text": "Andrew\u00a0D Selbst, Danah Boyd, Sorelle\u00a0A Friedler, Suresh Venkatasubramanian, and Janet Vertesi. 2019. Fairness and abstraction in sociotechnical systems. In Proceedings of the conference on fairness, accountability, and transparency. 59\u201368.",
      "doi": "10.1145/3287560.3287598"
    },
    {
      "text": "Shreya Shankar, Yoni Halpern, Eric Breck, James Atwood, Jimbo Wilson, and D Sculley. 2017. No classification without representation: Assessing geodiversity issues in open data sets for the developing world. arXiv preprint arXiv:1711.08536(2017).",
      "doi": ""
    },
    {
      "text": "Hong Shen, Alicia DeVos, Motahhare Eslami, and Kenneth Holstein. 2021. Everyday algorithm auditing: Understanding the power of everyday users in surfacing harmful algorithmic behaviors. Proceedings of the ACM on Human-Computer Interaction 5, CSCW2(2021), 1\u201329.",
      "doi": "10.1145/3479577"
    },
    {
      "text": "Jatinder Singh, Jennifer Cobbe, and Chris Norval. 2019. Decision Provenance: Harnessing Data Flow for Accountable Systems. IEEE Access 7(2019), 6562\u20136574. https://doi.org/10.1109/ACCESS.2018.2887201",
      "doi": ""
    },
    {
      "text": "Jatinder Singh, Ian Walden, Jon Crowcroft, and Jean Bacon. 2016. Responsibility & machine learning: Part of a process. Available at SSRN 2860048(2016).",
      "doi": ""
    },
    {
      "text": "Megha Srivastava, Hoda Heidari, and Andreas Krause. 2019. Mathematical notions vs. human perception of fairness: A descriptive approach to fairness for machine learning. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 2459\u20132468.",
      "doi": "10.1145/3292500.3330664"
    },
    {
      "text": "Stability.AI. 2022. Stable Diffusion. https://stability.ai/blog/stable-diffusion-v2-release.",
      "doi": ""
    },
    {
      "text": "Tony Sun, Andrew Gaut, Shirlyn Tang, Yuxin Huang, Mai ElSherief, Jieyu Zhao, Diba Mirza, Elizabeth Belding, Kai-Wei Chang, and William\u00a0Yang Wang. 2019. Mitigating gender bias in natural language processing: Literature review. arXiv preprint arXiv:1906.08976(2019).",
      "doi": ""
    },
    {
      "text": "Harini Suresh and John\u00a0V Guttag. 2019. A framework for understanding unintended consequences of machine learning. arXiv preprint arXiv:1901.10002(2019).",
      "doi": ""
    },
    {
      "text": "Feel Therapeutics. 2022. Decoding Mental Health.https://www.feeltherapeutics.com.",
      "doi": ""
    },
    {
      "text": "Shari Trewin, Sara Basson, Michael Muller, Stacy Branham, Jutta Treviranus, Daniel Gruen, Daniel Hebert, Natalia Lyckowski, and Erich Manser. 2019. Considerations for AI fairness for people with disabilities. AI Matters 5, 3 (2019), 40\u201363.",
      "doi": "10.1145/3362077.3362086"
    },
    {
      "text": "US House of Representatives. 2022. H.R.6580 - Algorithmic Accountability Act of 2022. https://www.congress.gov/bill/117th-congress/house-bill/6580.",
      "doi": ""
    },
    {
      "text": "Sahil Verma and Julia Rubin. 2018. Fairness definitions explained. In 2018 IEEE/ACM International Workshop on Software Fairness (FAIRWARE). IEEE, 1\u20137.",
      "doi": "10.1145/3194770.3194776"
    },
    {
      "text": "Sandra Wachter, Brent Mittelstadt, and Chris Russell. 2020. Why fairness cannot be automated: Bridging the gap between EU non-discrimination law and AI. arXiv preprint arXiv:2005.05906(2020).",
      "doi": ""
    },
    {
      "text": "Dakuo Wang, Justin\u00a0D Weisz, Michael Muller, Parikshit Ram, Werner Geyer, Casey Dugan, Yla Tausczik, Horst Samulowitz, and Alexander Gray. 2019. Human-ai collaboration in data science: Exploring data scientists\u2019 perceptions of automated ai. Proceedings of the ACM on Human-Computer Interaction 3, CSCW(2019), 1\u201324.",
      "doi": "10.1145/3359313"
    },
    {
      "text": "Qianwen Wang, Yao Ming, Zhihua Jin, Qiaomu Shen, Dongyu Liu, Micah\u00a0J Smith, Kalyan Veeramachaneni, and Huamin Qu. 2019. Atmseer: Increasing transparency and controllability in automated machine learning. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. 1\u201312.",
      "doi": "10.1145/3290605.3300911"
    },
    {
      "text": "Daniel Karl\u00a0I Weidele, Justin\u00a0D Weisz, Erick Oduor, Michael Muller, Josh Andres, Alexander Gray, and Dakuo Wang. 2020. AutoAIViz: opening the blackbox of automated artificial intelligence with conditional parallel coordinates. In Proceedings of the 25th International Conference on Intelligent User Interfaces. 308\u2013312.",
      "doi": "10.1145/3377325.3377538"
    },
    {
      "text": "Maranke Wieringa. 2020. What to account for when accounting for algorithms: a systematic literature review on algorithmic accountability. In Proceedings of the 2020 conference on fairness, accountability, and transparency. 1\u201318.",
      "doi": "10.1145/3351095.3372833"
    },
    {
      "text": "Christo Wilson, Avijit Ghosh, Shan Jiang, Alan Mislove, Lewis Baker, Janelle Szary, Kelly Trindel, and Frida Polli. 2021. Building and auditing fair algorithms: A case study in candidate screening. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. 666\u2013677.",
      "doi": "10.1145/3442188.3445928"
    },
    {
      "text": "Allison Woodruff, Sarah\u00a0E Fox, Steven Rousso-Schindler, and Jeffrey Warshaw. 2018. A qualitative exploration of perceptions of algorithmic fairness. In Proceedings of the 2018 chi conference on human factors in computing systems. 1\u201314.",
      "doi": "10.1145/3173574.3174230"
    },
    {
      "text": "Qingyun Wu and Chi Wang. 2021. Fair AutoML. arXiv preprint arXiv:2111.06495(2021).",
      "doi": ""
    },
    {
      "text": "Doris Xin, Eva\u00a0Yiwei Wu, Doris Jung-Lin Lee, Niloufar Salehi, and Aditya Parameswaran. 2021. Whither automl? understanding the role of automation in machine learning workflows. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 1\u201316.",
      "doi": "10.1145/3411764.3445306"
    },
    {
      "text": "Renee Yao. 2022. Lunit, maker of FDA-cleared AI for cancer analysis, goes public. https://blogs.nvidia.com/blog/2022/07/21/lunit-healthcare-ai-ipo/",
      "doi": ""
    },
    {
      "text": "Muhammad\u00a0Bilal Zafar, Isabel Valera, Manuel Gomez\u00a0Rodriguez, and Krishna\u00a0P Gummadi. 2017. Fairness beyond disparate treatment & disparate impact: Learning classification without disparate mistreatment. In Proceedings of the 26th International Conference on World Wide Web (WWW). 1171\u20131180.",
      "doi": "10.1145/3038912.3052660"
    },
    {
      "text": "Brian\u00a0Hu Zhang, Blake Lemoine, and Margaret Mitchell. 2018. Mitigating unwanted biases with adversarial learning. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society. 335\u2013340.",
      "doi": "10.1145/3278721.3278779"
    },
    {
      "text": "Daniel Zhang, Saurabh Mishra, Erik Brynjolfsson, John Etchemendy, Deep Ganguli, Barbara Grosz, Terah Lyons, James Manyika, Juan\u00a0Carlos Niebles, Michael Sellitto, 2021. The AI Index 2021 Annual Report. arXiv preprint arXiv:2103.06312(2021).",
      "doi": ""
    },
    {
      "text": "Song\u00a0Yang Zhang\u00a0Zhifei and Qi Hairong. 2017. Age Progression/Regression by Conditional Adversarial Autoencoder. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE.",
      "doi": ""
    },
    {
      "text": "Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. 2017. Men also like shopping: Reducing gender bias amplification using corpus-level constraints. arXiv preprint arXiv:1707.09457(2017).",
      "doi": ""
    }
  ]
}