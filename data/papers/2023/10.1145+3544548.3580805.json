{
  "doi": "10.1145/3544548.3580805",
  "title": "Is this AI trained on Credible Data? The Effects of Labeling Quality and Performance Bias on User Trust",
  "published": "2023-04-19",
  "proctitle": "CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems",
  "pages": "1-11",
  "year": 2023,
  "badges": [],
  "abstract": "To promote data transparency, frameworks such as CrowdWorkSheets encourage documentation of annotation practices on the interfaces of AI systems, but we do not know how they affect user experience. Will the quality of labeling affect perceived credibility of training data? Does the source of annotation matter? Will a credible dataset persuade users to trust a system even if it shows racial biases in its predictions? To find out, we conducted a user study (N = 430) with a prototype of a classification system, using a 2 (labeling quality: high vs. low) \u00d7 4 (source: others-as-source vs. self-as-source cue vs. self-as-source voluntary action, vs. self-as-source forced action) \u00d7 3 (AI performance: none vs. biased vs. unbiased) experiment. We found that high-quality labeling leads to higher perceived training data credibility, which in turn enhances users\u2019 trust in AI, but not when the system shows bias. Practical implications for explainable and ethical AI interfaces are discussed.",
  "tags": [
    "data labeling quality",
    "algorithmic bias",
    "labeling source",
    "training data credibility",
    "trust in AI"
  ],
  "authors": [
    {
      "name": "Cheng Chen",
      "institution": "School of Communications, Elon University, United States",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99660783835",
      "orcid": "0000-0002-9127-3893"
    },
    {
      "name": "S. Shyam Sundar",
      "institution": "Media Effects Research Laboratory, The Pennsylvania State University, United States",
      "img": "/do/10.1145/contrib-81456626968/rel-imgonly/shyam.jpeg",
      "acmid": "81456626968",
      "orcid": "0000-0002-5779-8864"
    }
  ],
  "references": [
    {
      "text": "Simon Albrecht and Anthony Travaglione. 2003. Trust in public-sector senior management. International Journal of Human Resource Management 14, 1(2003), 76\u201392. https://doi.org/10.1080/09585190210158529",
      "doi": ""
    },
    {
      "text": "Ariful\u00a0Islam Anik and Andrea Bunt. 2021. Data-centric explanations: explaining training data of machine learning systems to promote transparency. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. Association for Computing Machinery, New York, NY; United States, 1\u201313. https://doi.org/10.1145/3411764.3445736",
      "doi": "10.1145/3411764.3445736"
    },
    {
      "text": "Alejandro\u00a0Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del\u00a0Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garc\u00eda, Sergio Gil-L\u00f3pez, Daniel Molina, Richard Benjamins, 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information fusion 58(2020), 82\u2013115. https://doi.org/10.1016/j.inffus.2019.12.012",
      "doi": "10.1016/j.inffus.2019.12.012"
    },
    {
      "text": "Michelle Bao, Angela Zhou, Samantha Zottola, Brian Brubach, Sarah Desmarais, Aaron Horowitz, Kristian Lum, and Suresh Venkatasubramanian. 2021. It\u2019s COMPASlicated: The Messy Relationship between RAI Datasets and Algorithmic Fairness Benchmarks. In Proceedings of the 35th Conference on Neutral Information Proceesing Systems. https://doi.org/10.48550/arXiv.2106.05498",
      "doi": ""
    },
    {
      "text": "Emily\u00a0M Bender and Batya Friedman. 2018. Data statements for natural language processing: Toward mitigating system bias and enabling better science. Transactions of the Association for Computational Linguistics 6 (2018), 587\u2013604. https://doi.org/10.1162/tacl_a_00041",
      "doi": ""
    },
    {
      "text": "Reuben Binns, Max Van\u00a0Kleek, Michael Veale, Ulrik Lyngs, Jun Zhao, and Nigel Shadbolt. 2018. \u2019It\u2019s reducing a human being to a percentage\u2019 perceptions of justice in algorithmic decisions. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems. Association for Computing Machinery, New York, NY, United States, 1\u201314. https://doi.org/10.1145/3173574.3173951",
      "doi": "10.1145/3173574.3173951"
    },
    {
      "text": "Judee\u00a0K Burgoon. 1993. Interpersonal expectations, expectancy violations, and emotional communication. Journal of language and social psychology 12, 1-2 (1993), 30\u201348. https://doi.org/10.1177/0261927X93121003",
      "doi": ""
    },
    {
      "text": "Judee\u00a0K Burgoon, Joseph\u00a0A Bonito, Paul\u00a0Benjamin Lowry, Sean\u00a0L Humpherys, Gregory\u00a0D Moody, James\u00a0E Gaskin, and Justin\u00a0Scott Giboney. 2016. Application of expectancy violations theory to communication with and judgments about embodied agents during a decision-making task. International Journal of Human-Computer Studies 91 (2016), 24\u201336. https://doi.org/10.1016/j.ijhcs.2016.02.002",
      "doi": "10.1016/j.ijhcs.2016.02.002"
    },
    {
      "text": "Cheng Chen. 2022. Communicating racial bias in AI algorithms: Effects of training data diversity and user feedback on AI trust. Ph.\u00a0D. Dissertation. Pennsylvania State University, State College, PA.",
      "doi": ""
    },
    {
      "text": "Cheng Chen and S.\u00a0Shyam Sundar. 2021. Combating algorithmic bias: Should AI media show and tell to gain user trust? (2021). Poster presented at the 2021 ICDS Symposium on Fairness of Machine Learning.",
      "doi": ""
    },
    {
      "text": "Irene Chen, Fredrik\u00a0D Johansson, and David Sontag. 2018. Why is my classifier discriminatory?Proceedings of the 32nd International Conference on Neural Information Processing Systems 31(2018), 3543\u20133554. https://doi.org/10.5555/3327144.3327272",
      "doi": "10.5555/3327144.3327272"
    },
    {
      "text": "Alexandra Chouldechova and Aaron Roth. 2020. A snapshot of the frontiers of fairness in machine learning. Commun. ACM 63, 5 (2020), 82\u201389. https://doi.org/10.1145/3376898",
      "doi": "10.1145/3376898"
    },
    {
      "text": "Mark D\u00edaz, Ian Kivlichan, Rachel Rosen, Dylan Baker, Razvan Amironesei, Vinodkumar Prabhakaran, and Emily Denton. 2022. Crowdworksheets: Accounting for individual and collective identities underlying crowdsourced dataset annotation. In 2022 ACM Conference on Fairness, Accountability, and Transparency. Association for Computing Machinery, New York, NY, USA, 2342\u20132351. https://doi.org/10.1145/3531146.3534647",
      "doi": "10.1145/3531146.3534647"
    },
    {
      "text": "Berkeley\u00a0J Dietvorst, Joseph\u00a0P Simmons, and Cade Massey. 2015. Algorithm aversion: people erroneously avoid algorithms after seeing them err.Journal of Experimental Psychology: General 144, 1 (2015), 114. https://doi.org/10.1037/xge0000033",
      "doi": ""
    },
    {
      "text": "Berkeley\u00a0J Dietvorst, Joseph\u00a0P Simmons, and Cade Massey. 2018. Overcoming algorithm aversion: People will use imperfect algorithms if they can (even slightly) modify them. Management Science 64, 3 (2018), 1155\u20131170. https://doi.org/10.1287/mnsc.2016.2643",
      "doi": "10.1287/mnsc.2016.2643"
    },
    {
      "text": "Jonathan Dodge, Q\u00a0Vera Liao, Yunfeng Zhang, Rachel\u00a0KE Bellamy, and Casey Dugan. 2019. Explaining models: An empirical study of how explanations impact fairness judgment. In Proceedings of the 24th International Conference on Intelligent User Interfaces. Association for Computing Machinery, New York, NY, United States, 275\u2013285. https://doi.org/10.1145/3301275.3302310",
      "doi": "10.1145/3301275.3302310"
    },
    {
      "text": "Motahhare Eslami, Kristen Vaccaro, Karrie Karahalios, and Kevin Hamilton. 2017. \u201cBe careful; things can be worse than they appear\u201d: Understanding Biased Algorithms and Users\u2019 Behavior around Them in Rating Platforms. In Proceedings of the International AAAI Conference on Web and Social Media, Vol.\u00a011. AAAI Press, Palo Alto, California, USA, 62\u201371. https://doi.org/10.1609/icwsm.v11i1.14898",
      "doi": ""
    },
    {
      "text": "Franz Faul, Edgar Erdfelder, Albert-Georg Lang, and Axel Buchner. 2007. G* Power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences. Behavior Research Methods 39, 2 (2007), 175\u2013191. https://doi.org/10.3758/bf03193146",
      "doi": ""
    },
    {
      "text": "Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer\u00a0Wortman Vaughan, Hanna Wallach, Hal\u00a0Daum\u00e9 Iii, and Kate Crawford. 2021. Datasheets for datasets. Commun. ACM 64, 12 (2021), 86\u201392. https://doi.org/10.48550/arXiv.1803.09010",
      "doi": "10.1145/3458723"
    },
    {
      "text": "Karen Hao. 2019. AI is sending people to jail\u2014and getting it wrong. MIT Technology Review. Retrieved December 8, 2022 from https://www.technologyreview.com/2019/01/21/137783/algorithms-criminal-justice-ai/",
      "doi": ""
    },
    {
      "text": "Andrew\u00a0F Hayes. 2017. Introduction to mediation, moderation, and conditional process analysis: A regression-based approach. Guilford publications, New York, NY, United States.",
      "doi": ""
    },
    {
      "text": "Sarah Holland, Ahmed Hosny, Sarah Newman, Joshua Joseph, and Kasia Chmielinski. 2020. The dataset nutrition label. Data Protection and Privacy, Volume 12: Data Protection and Democracy 12 (2020), 1. https://doi.org/10.48550/arXiv.1805.03677",
      "doi": ""
    },
    {
      "text": "Joo-Wha Hong, Ignacio Cruz, and Dmitri Williams. 2021. AI, you can drive my car: How we evaluate human drivers vs. self-driving cars. Computers in Human Behavior 125 (2021), 106944. https://doi.org/10.1016/j.chb.2021.106944",
      "doi": "10.1016/j.chb.2021.106944"
    },
    {
      "text": "Yan Huang and S\u00a0Shyam Sundar. 2022. Do we trust the crowd? Effects of crowdsourcing on perceived credibility of online health information. Health Communication 37, 1 (2022), 93\u2013102. https://doi.org/10.1080/10410236.2020.1824662",
      "doi": ""
    },
    {
      "text": "Jiun-Yin Jian, Ann\u00a0M Bisantz, and Colin\u00a0G Drury. 2000. Foundations for an empirically determined scale of trust in automated systems. International Journal of Cognitive Ergonomics 4, 1 (2000), 53\u201371. https://doi.org/10.1207/S15327566IJCE0401_04",
      "doi": ""
    },
    {
      "text": "Devon Johnson and Kent Grayson. 2005. Cognitive and affective trust in service relationships. Journal of Business Research 58, 4 (2005), 500\u2013507. https://doi.org/10.1016/S0148-2963(03)00140-1",
      "doi": ""
    },
    {
      "text": "Ren\u00e9\u00a0F Kizilcec. 2016. How much information? Effects of transparency on trust in an algorithmic interface. In Proceedings of the 2016 CHI conference on human factors in computing systems. Association for Computing Machinery, New York, NY, USA, 2390\u20132395. https://doi.org/10.1145/2858036.2858402",
      "doi": "10.1145/2858036.2858402"
    },
    {
      "text": "Heidi Ledford. 2019. Millions of black people affected by racial bias in health-care algorithms. Nature 574, 7780 (2019), 608\u2013610. https://www.nature.com/articles/d41586-019-03228-6",
      "doi": ""
    },
    {
      "text": "John\u00a0D Lee and Katrina\u00a0A See. 2004. Trust in automation: Designing for appropriate reliance. Human factors 46, 1 (2004), 50\u201380. https://doi.org/10.1518/hfes.46.1.50_30392",
      "doi": ""
    },
    {
      "text": "Q\u00a0Vera Liao, Moninder Singh, Yunfeng Zhang, and Rachel Bellamy. 2021. Introduction to explainable AI. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems. Association for Computing Machinery, New York, NY, USA, 1\u20133. https://doi.org/10.1145/3411763.3445016",
      "doi": "10.1145/3411763.3445016"
    },
    {
      "text": "Q\u00a0Vera Liao and S\u00a0Shyam Sundar. 2022. Designing for Responsible Trust in AI Systems: A Communication Perspective. In FAccT \u201922: 2022 ACM Conference on Fairness, Accountability, and Transparency. Association for Computing Machinery, New York, NY, United States, 1257\u20131268. https://doi.org/10.1145/3531146.3533182",
      "doi": "10.1145/3531146.3533182"
    },
    {
      "text": "Leib Litman, Jonathan Robinson, and Tzvi Abberbock. 2017. TurkPrime.com: A versatile crowdsourcing data acquisition platform for the behavioral sciences. Behavior Research Methods 49, 2 (2017), 433\u2013442. https://doi.org/10.3758/s13428-016-0727-z",
      "doi": ""
    },
    {
      "text": "Daniel\u00a0J McAllister. 1995. Affect-and cognition-based trust as foundations for interpersonal cooperation in organizations. Academy of Management Journal 38, 1 (1995), 24\u201359. https://doi.org/10.2307/256727",
      "doi": ""
    },
    {
      "text": "Miriam\u00a0J Metzger and Andrew\u00a0J Flanagin. 2013. Credibility and trust of information in online environments: The use of cognitive heuristics. Journal of Pragmatics 59(2013), 210\u2013220. https://doi.org/10.1016/j.pragma.2013.07.012",
      "doi": ""
    },
    {
      "text": "David Meyer. 2018. Amazon reportedly killed an AI recruitment system because it couldn\u2019t stop the tool from discriminating against women. Fortune. Retrieved September 13, 2022 from https://fortune.com/2018/10/10/amazon-ai-recruitment-bias-women-sexist/",
      "doi": ""
    },
    {
      "text": "Dale\u00a0T Miller. 1976. Ego involvement and attributions for success and failure.Journal of Personality and Social Psychology 34, 5(1976), 901. https://doi.org/10.1037/0022-3514.34.5.901",
      "doi": ""
    },
    {
      "text": "Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa\u00a0Deborah Raji, and Timnit Gebru. 2019. Model cards for model reporting. In Proceedings of the Conference on Fairness, Accountability, and Transparency. Association for Computing Machinery, New York, NY, USA, 220\u2013229. https://doi.org/10.1145/3287560.3287596",
      "doi": "10.1145/3287560.3287596"
    },
    {
      "text": "Maria\u00a0D Molina and S\u00a0Shyam Sundar. 2022. When AI moderates online content: effects of human collaboration and interactive transparency on user trust. Journal of Computer-Mediated Communication 27, 4 (2022), zmac010. https://doi.org/10.1093/jcmc/zmac010",
      "doi": ""
    },
    {
      "text": "Kathleen\u00a0L Mosier, Linda\u00a0J Skitka, Susan Heers, and Mark Burdick. 2017. Automation bias: Decision making and performance in high-tech cockpits. In Decision Making in Aviation. Routledge, 271\u2013288. https://doi.org/10.1207/s15327108ijap0801_3",
      "doi": ""
    },
    {
      "text": "Daniel\u00a0J O\u2019Keefe. 2003. Message properties, mediating states, and manipulation checks: Claims, evidence, and data analysis in experimental persuasive message effects research. Communication Theory 13, 3 (2003), 251\u2013274. https://doi.org/10.1111/j.1468-2885.2003.tb00292.x",
      "doi": ""
    },
    {
      "text": "Amandalynne Paullada, Inioluwa\u00a0Deborah Raji, Emily\u00a0M Bender, Emily Denton, and Alex Hanna. 2021. Data and its (dis) contents: A survey of dataset development and use in machine learning research. Patterns 2, 11 (2021), 100336. https://doi.org/10.1016/j.patter.2021.100336",
      "doi": ""
    },
    {
      "text": "A Phaneuf. 2020. Artificial intelligence in financial services: Applications and benefits of AI in finance. Insider Intelligence. Retrieved December 8, 2022 from https://www.insiderintelligence.com/insights/ai-in-finance/#:\u00a0:text=AI%20is%20particularly%20helpful%20in,underwriting%20and%20reduce%20financial%20risk.",
      "doi": ""
    },
    {
      "text": "Amy Rechkemmer and Ming Yin. 2022. When confidence meets accuracy: Exploring the effects of multiple performance indicators on trust in machine learning models. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems. Association for Computing Machinery, New York, NY, United States, 1\u201314. https://doi.org/10.1145/3491102.3501967",
      "doi": "10.1145/3491102.3501967"
    },
    {
      "text": "Wojciech Samek and Klaus-Robert M\u00fcller. 2019. Towards explainable artificial intelligence. In Explainable AI: Interpreting, explaining and visualizing deep learning. Springer, 5\u201322. https://doi.org/10.1145/3491102.3501967",
      "doi": "10.1145/3491102.3501967"
    },
    {
      "text": "Andrew\u00a0D Selbst, Danah Boyd, Sorelle\u00a0A Friedler, Suresh Venkatasubramanian, and Janet Vertesi. 2019. Fairness and abstraction in sociotechnical systems. In Proceedings of the Conference on Fairness, Accountability, and Transparency. Association for Computing Machinery, New York, NY, USA, 59\u201368. https://doi.org/10.1145/3287560.3287598",
      "doi": "10.1145/3287560.3287598"
    },
    {
      "text": "Alison Smith-Renner, Ron Fan, Melissa Birchfield, Tongshuang Wu, Jordan Boyd-Graber, Daniel\u00a0S Weld, and Leah Findlater. 2020. No explainability without accountability: An empirical study of explanations and feedback in interactive ML. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. Association for Computing Machinery, New York, NY, United States, 1\u201313. https://doi.org/10.1145/3313831.3376624",
      "doi": "10.1145/3313831.3376624"
    },
    {
      "text": "Hyeonjin Soh, Leonard\u00a0N Reid, and Karen\u00a0Whitehill King. 2009. Measuring trust in advertising. Journal of Advertising 38, 2 (2009), 83\u2013104. https://doi.org/10.2753/JOA0091-3367380206",
      "doi": ""
    },
    {
      "text": "Yuan Sun and S\u00a0Shyam Sundar. 2022. Exploring the effects of interactive dialogue in improving user control for explainable online symptom checkers. In CHI Conference on Human Factors in Computing Systems Extended Abstracts. Association for Computing Machinery, New York, NY, United States, 1\u20137. https://doi.org/10.1145/3491101.3519668",
      "doi": "10.1145/3491101.3519668"
    },
    {
      "text": "S\u00a0Shyam Sundar. 2008. The MAIN model: A heuristic approach to understanding technology effects on credibility. MacArthur Foundation Digital Media and Learning Initiative, Cambridge, MA. https://doi.org/10.1162/dmal.9780262562324.073",
      "doi": ""
    },
    {
      "text": "S\u00a0Shyam Sundar. 2020. Rise of machine agency: A framework for studying the psychology of human\u2013AI interaction (HAII). Journal of Computer-Mediated Communication 25, 1 (2020), 74\u201388. https://doi.org/10.1093/jcmc/zmz026",
      "doi": ""
    },
    {
      "text": "S\u00a0Shyam Sundar, Haiyan Jia, T\u00a0Franklin Waddell, and Yan Huang. 2015. Toward a theory of interactive media effects (TIME) four models for explaining how interface features affect user psychology. The Handbook of the Psychology of Communication Technology (2015), 47\u201386. https://doi.org/10.1002/9781118426456.ch3",
      "doi": ""
    },
    {
      "text": "S\u00a0Shyam Sundar and Clifford Nass. 2001. Conceptualizing sources in online news. Journal of Communication 51, 1 (2001), 52\u201372. https://doi.org/10.1111/j.1460-2466.2001.tb02872.x",
      "doi": ""
    },
    {
      "text": "Chun-Hua Tsai, Yue You, Xinning Gui, Yubo Kou, and John\u00a0M Carroll. 2021. Exploring and promoting diagnostic transparency and explainability in online symptom checkers. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. Association for Computing Machinery, New York, NY, United States, 1\u201317. https://doi.org/10.1145/3411764.3445101",
      "doi": "10.1145/3411764.3445101"
    },
    {
      "text": "Endel Tulving, Daniel\u00a0L Schacter, and Heather\u00a0A Stark. 1982. Priming effects in word-fragment completion are independent of recognition memory. Journal of Experimental Psychology: Learning, Memory, and Cognition 8, 4(1982), 336. https://doi.org/10.1037/0278-7393.8.4.336",
      "doi": ""
    },
    {
      "text": "Amos Tversky and Daniel Kahneman. 1974. Judgment under uncertainty: Heuristics and biases. science 185, 4157 (1974), 1124\u20131131. https://doi.org/10.1126/science.185.4157.1124",
      "doi": ""
    },
    {
      "text": "Sahil Verma and Julia Rubin. 2018. Fairness definitions explained. In 2018 IEEE/ACM International Workshop on Software Fairness (FairWare). IEEE, 1\u20137. https://doi.org/10.1145/3194770.3194776",
      "doi": "10.1145/3194770.3194776"
    },
    {
      "text": "Ruotong Wang, F\u00a0Maxwell Harper, and Haiyi Zhu. 2020. Factors influencing perceived fairness in algorithmic decision-making: Algorithm outcomes, development procedures, and individual differences. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. Association for Computing Machinery, New York, NY, United States, 1\u201314. https://doi.org/10.1145/3313831.3376813",
      "doi": "10.1145/3313831.3376813"
    },
    {
      "text": "Allison Woodruff, Sarah\u00a0E Fox, Steven Rousso-Schindler, and Jeffrey Warshaw. 2018. A qualitative exploration of perceptions of algorithmic fairness. In Proceedings of the 2018 CHI conference on Human Factors in Computing Systems. Association for Computing Machinery, New York, NY, United States, 1\u201314. https://doi.org/10.1145/3173574.3174230",
      "doi": "10.1145/3173574.3174230"
    },
    {
      "text": "Wencan Zhang and Brian\u00a0Y Lim. 2022. Towards relatable explainable AI with the perceptual process. In CHI Conference on Human Factors in Computing Systems. Association for Computing Machinery, New York, NY, United States, 1\u201324. https://doi.org/10.1145/3491102.3501826",
      "doi": "10.1145/3491102.3501826"
    },
    {
      "text": "Dolf Zillmann. 2002. Exemplification theory of media influence. In Media effects. Routledge, 29\u201352.",
      "doi": ""
    }
  ]
}