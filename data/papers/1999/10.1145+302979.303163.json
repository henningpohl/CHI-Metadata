{
  "doi": "10.1145/302979.303163",
  "title": "Mutual disambiguation of recognition errors in a multimodel architecture",
  "published": "1999-05-01",
  "proctitle": "CHI '99: Proceedings of the SIGCHI conference on Human Factors in Computing Systems",
  "pages": "576-583",
  "year": 1999,
  "badges": [],
  "abstract": "As a new generation of multimodal/media systems begins to define\nitself, researchers are attempting to learn how to combine\ndifferent modes into strategically integrated whole systems. In\ntheory, well designed multimodal systems should be able to\nintegrate complementary modalities in a manner that supports mutual\ndisambiguation (MD) of errors and leads to more robust performance.\nIn this study, over 2,000 multimodal utterances by both native and\naccented speakers of English were processed by a multimodal system,\nand then logged and analyzed. The results confirmed that multimodal\nsystems can indeed support significant levels of MD, and also\nhigher levels of MD for the more challenging accented users. As a\nresult, although speech recognition as a stand-alone performed far\nmore poorly for accented speakers, their multimodal recognition\nrates did not differ from those of native speakers. Implications\nare discussed for the development of future multimodal\narchitectures that can perform in a more robust and stable manner\nthan individual recognition technologies. Also discussed is the\ndesign of interfaces that support diversity in tangible ways, and\nthat function well under challenging real-world usage\nconditions,",
  "authors": [
    {
      "name": "Sharon Oviatt",
      "institution": "Center for Human-Computer Communication, Oregon Graduate Institute of Science and Technology, P.O. Box 91000, Portland, OR",
      "img": "/do/10.1145/contrib-81100656112/rel-imgonly/handbookv1.png",
      "acmid": "81100656112",
      "orcid": "missing"
    }
  ],
  "references": [
    {
      "text": "Bolt, R.A. Put that there: Voice and gesture at the graphics interface. Computer Graphics, 1980, 14 (3): 262-270.  ",
      "doi": "10.1145/965105.807503"
    },
    {
      "text": "Carpenter, R. The logic of typed feature structures. Cambridge, MA.: Cambridge University Press, 1992. ",
      "doi": "10.5555/131907"
    },
    {
      "text": "Clow, J. &amp; Oviatt, S. L. STAMP: A suite of tools for analyzing multimodal system processing, Proceedings of the International Conference on Spoken Language Processing, in press.",
      "doi": ""
    },
    {
      "text": "Cohen, P., Dalrymple, M., Moran, D., Pereira, F. Synergistic use of direct manipulation and natural language, CHI '89 Conference Proceedings, ACM/Addison Wesley: New York, NY, 1989, 227-234.  ",
      "doi": "10.1145/67449.67494"
    },
    {
      "text": "Cohen, P., Johnston, M., McGee, D., Oviatt, S., Pittman, J., Smith, I., Chen, L. and Clow, J. Quickset: Multimodal interaction for distributed applications. Proceedings of the Fifth ACM International Multimedia Conference, New York, NY: ACM Press, 1997, 31-40.  ",
      "doi": "10.1145/266180.266328"
    },
    {
      "text": "Johnston, M., Cohen, P.R., McGee, D., Oviatt, S.L., Pittman, J.A. &amp; Smith, I. Unification-based multimodal integration. Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, San Francisco, CA.: Morgan Kaufmann, 1997, 281-288.  ",
      "doi": "10.3115/976909.979653"
    },
    {
      "text": "Koons, D.B., Sparrell, C.J. &amp; Thorisson, K.R. Integrating simultaneous input from speech, gaze, and hand gestures. In Intelligent Multimedia Interfaces, M. Maybury, Ed. MIT Press: Menlo Park, CA, 1993, 257-276. ",
      "doi": "10.5555/162477.162508"
    },
    {
      "text": "Neal, J.G. &amp; Shapiro, S.C. Intelligent multi-media interface technology. In Intelligent User Interfaces, J. Sullivan &amp; S. Tyler, Eds. ACM: New York, 1991, 11-43.  ",
      "doi": "10.1145/107215.128690"
    },
    {
      "text": "Oviatt, S.L. Ten myths of multimodal interaction, Communications of the ACM, in press.  ",
      "doi": "10.1145/319382.319398"
    },
    {
      "text": "Oviatt, S.L. Multimodal interactive maps: Designing for human performance, Human-Computer Interaction, 1997, 12 (1 &amp; 2) 93-129.  ",
      "doi": "10.1207/s15327051hci1201%25262_4"
    },
    {
      "text": "Oviatt, S.L. Pen/voice: Complementary multimodal communication, Proceedings of Speech Tech 92, New York, NY.",
      "doi": ""
    },
    {
      "text": "Oviatt, S.L., Bernard, J. &amp; Levow, G. Linguistic adaptations during spoken and multimodal error resolution, Language and Speech, in press.",
      "doi": ""
    },
    {
      "text": "Oviatt, S.L., Cohen, P. &amp; Wang, M. Toward interface design for human language technology: Modality and structure as determinants of linguistic complexity, Speech Communication, 1994, 15 (3-4), 283-300.  ",
      "doi": "10.1016/0167-6393%2894%2990079-5"
    },
    {
      "text": "Oviatt, S. L., DeAngeli, A. &amp; Kuhn, K. Integration and synchronization of input modes during multimodal humancomputer interaction, Proceedings of the CHI 97 Conference, New York, NY: ACM Press, 415-422.  ",
      "doi": "10.1145/258549.258821"
    },
    {
      "text": "Oviatt, S. L. &amp; Kuhn, K. Referential features and linguistic indirection in multimodal language, Proceedings of the International Conference on Spoken Language Processing, in press.",
      "doi": ""
    },
    {
      "text": "Oviatt, S. L. &amp; Olsen, E. Integration themes in multimodal human-computer interaction, Proceedings of the International Conference on Spoken Language Processing, (ed. by Shirai, Furui &amp; Kakehi), Acoustical Society of Japan, 1994, vol. 2, 551-554.",
      "doi": ""
    }
  ]
}