{
  "doi": "10.1145/1978942.1978952",
  "title": "Exploring camera viewpoint control models for a multi-tasking setting in teleoperation",
  "published": "2011-05-07",
  "proctitle": "CHI '11: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems",
  "pages": "53-62",
  "year": 2011,
  "badges": [],
  "abstract": "Control of camera viewpoint plays a vital role in many teleoperation activities, as watching live video streams is still the fundamental way for operators to obtain situational awareness from remote environments. Motivated by a real-world industrial setting in mining teleoperation, we explore several possible solutions to resolve a common multi-tasking situation where an operator is required to control a robot and simultaneously perform remote camera operation. Conventional control interfaces are predominantly used in such teleoperation settings, but could overload an operator's hand-operation capability, and require frequent attention switches and thus could decrease productivity. We report on an empirical user study in a model multi-tasking teleoperation setting where the user has a main task which requires their attention. We compare three different camera viewpoint control models: (1) dual manual control, (2) natural interaction (combining eye gaze and head motion) and (3) autonomous tracking. The results indicate the advantages of using the natural interaction model, while the manual control model performed the worst.",
  "authors": [
    {
      "name": "Dingyun Zhu",
      "institution": "CSIRO / ANU , Canberra, Australia",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81385600519",
      "orcid": "missing"
    },
    {
      "name": "Tom Gedeon",
      "institution": "ANU, Canberra, Australia",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81100518865",
      "orcid": "missing"
    },
    {
      "name": "Ken Taylor",
      "institution": "CSIRO, Canberra, Australia",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81405592445",
      "orcid": "missing"
    }
  ],
  "references": [
    {
      "text": "Adams, N., Witkowski, M. and Spence, R. The inspection of very large images by eye-gaze control. In Proc. AVI 2008, ACM Press (2008), 1--8.  ",
      "doi": "10.1145/1385569.1385589"
    },
    {
      "text": "Chen, Y. C., Haas, E. C. and Barnes, M. J. Human performance issues and user interface design for teleoperated robots. IEEE Trans. on SMC -- Part C: Applications and Reviews, 37, 2 (2007), 1231--1245.  ",
      "doi": "10.1109/TSMCC.2007.905819"
    },
    {
      "text": "Cohen, C. J., Conway, L. and Kiditschek, D. Dynamical system representation, generation, and recognition of basic oscillatory motion gestures. In Proc. 2nd Int. Conf. Face Gesture Recog. 1996, 60--65. ",
      "doi": "10.5555/524467.796037"
    },
    {
      "text": "Cohen, I. and Medioni, G. Detecting and tracking moving objects for video surveillance. In Proc. CVPR 1999, 319--325.",
      "doi": ""
    },
    {
      "text": "Draper, M., Calhoun, G., Ruff, H., Williamson, D. and Barry, T. Manual versus speech input for unmanned aerial vehicle control station operations. In Proc. Hum. Factors Ergonom. Soc. 47th Annu. Meet. 2003, 109--113.",
      "doi": ""
    },
    {
      "text": "Duff, E., Caris, C., Bonchis, A., Taylor, K., Gunn, C. and Adcock, M. The development of a telerobotic rock breaker. In Proc. FSR 2009, 1--10.",
      "doi": ""
    },
    {
      "text": "Fong, T., and Thorpe, C. Vehicle Teleoperation interfaces. Autonomous Robots, 11, 1 (2001), 9--18.  ",
      "doi": "10.1023/A%3A1011295826834"
    },
    {
      "text": "Goh, A. H. W., Yong, Y. S., Chan, C. H., Then, S. J., Chu, P. L., Chau, S. W. and Hon, H. W. Interactive PTZ camera control system using Wii remote and infrared sensor bar. In Proc. World Academy of Science, Engineering and Technology (2008) 46, 127--132.",
      "doi": ""
    },
    {
      "text": "Gunn, C., Hutchins, M. Stevenson, D., Adcok, M. and Youngblood, P. Using collaborative haptics in remote surgical training. In Proc. WorldHaptics 2005, 481--482.  ",
      "doi": "10.1109/WHC.2005.141"
    },
    {
      "text": "Hainsworth, D. Teleoperation user interfaces for mining robotics. Autonomous Robots, 11, 1 (2001), 19--28.  ",
      "doi": "10.1023/A%3A1011299910904"
    },
    {
      "text": "1Hansen, D. W., Skovsgaard, H. and Mollenbach, E. Noise tolerant selection by gaze-controlled pan and zoom in 3D. In Proc. ETRA 2008, ACM Press (2008), 205--212.  ",
      "doi": "10.1145/1344471.1344521"
    },
    {
      "text": "Harrison, C. and Dey, A. K. Lean and zoom: proximity-aware user interface and content magnification. In Proc. CHI 2008, ACM Press (2008), 507--510.  ",
      "doi": "10.1145/1357054.1357135"
    },
    {
      "text": "Hu, C., Meng, M. Q., Liu, P. X. and Wang, X. Visual gesture recognition for human-machine interface of robot teleoperation. In Proc. IEEE/RSJ Int. Conf. Intell. Robot. Syst. 2003, 1560--1565.",
      "doi": ""
    },
    {
      "text": "Hughes, S. and Lewis, M. Robotic camera control for remote exploration. In Proc. CHI 2004, ACM Press (2004), 511--517.  ",
      "doi": "10.1145/985692.985757"
    },
    {
      "text": "Isokoski, P., Hyrskykari, A., Kotkaluoto, S. and Martin, B. Gamepad and eye tracker input in first person shooter games: data for the first 50 minutes. In Proc. COGAIN 2007, 11--15.",
      "doi": ""
    },
    {
      "text": "Jacob, R. J. K. The use of eye movements in human-computer interaction techniques: what you look at is what you get. ACM Transactions on Information Systems, 9, 3 (1991), 152--169.  ",
      "doi": "10.1145/123078.128728"
    },
    {
      "text": "Kumar, M., Paepcke, A. and Winograd, T. Eyepoint: practical pointing and selection using gaze and keyboard, In Proc. CHI 2007, ACM Press (2007), 421--430.  ",
      "doi": "10.1145/1240624.1240692"
    },
    {
      "text": "Lewis, J. R. IBM computer usability satisfaction questionnaires: psychometric evaluation and instructions for use. Int. J. Human-Computer Interaction, 7, 1, (1995), 57--78.  ",
      "doi": "10.1080/10447319509526110"
    },
    {
      "text": "Liu, Q., Kimber, D., Foote, J., Wilcox, L. and Boreczky, J. FlySPEC: a multi-user video camera system with hybrid human and automatic control. In Proc. MM 2002, ACM Press (2002), 484--492.  ",
      "doi": "10.1145/641007.641110"
    },
    {
      "text": "Logitech Dual Action Gamepad. http://www.logitech.com/en-us/gaming/controllers/devices/288.",
      "doi": ""
    },
    {
      "text": "Pelco ES30C. http://www.pelco.com/products.",
      "doi": ""
    },
    {
      "text": "Qvarforde, P. and Zhai, S. Conversing with the user based on eye-gaze patterns. In Proc. CHI 2005, ACM Press (2005), 221--230.  ",
      "doi": "10.1145/1054972.1055004"
    },
    {
      "text": "Saito, S. Does fatigue exist in quantitative measurement of eye movements?, Ergonomics, 35 (5/6), 607--615.",
      "doi": ""
    },
    {
      "text": "Salvucci, D. D. and Goldberg, J. H. Identifying fixations and saccades in eye-tracking protocols. In Proc. ETRA 2000, ACM Press (2000), 71--78.  ",
      "doi": "10.1145/355017.355028"
    },
    {
      "text": "Seeingmachines FaceLAB 4.5. http://www.seeingmachines.com/product/facelab.",
      "doi": ""
    },
    {
      "text": "Sensable Phantom Premium 1.5. http://www.sensable.com/phantom-premium-1--5.htm.",
      "doi": ""
    },
    {
      "text": "Tall, M. Alapetite, A., Agustin, J. S., Skovsgaard, H. H., Hansen, J. P., Hansen, D. W. and Mollenbach, E. Gaze-controlled driving. Ext. Abstracts CHI 2009, ACM Press (2009), 4387--4392.  ",
      "doi": "10.1145/1520340.1520671"
    },
    {
      "text": "Valli, A. The design of natural interaction. Multimedia Tools and Applications (2008) 38, 295--305.  ",
      "doi": "10.1007/s11042-007-0190-z"
    },
    {
      "text": "Wang, S., Xiong, X., Xu, Y. Wang, C., Zhang, W., Dai, X. and Zhang, D. Face tracking as an augmented input in video games: enhancing presence, role-playing and control. In Proc. CHI 2006, ACM Press (2006), 1097--1106.  ",
      "doi": "10.1145/1124772.1124936"
    },
    {
      "text": "Yamaguchi, K., Komuro, T. and Ishikawa, M. PTZ control with head tracking for video chat. Ext. Abstracts CHI 2009, ACM Press (2009), 3919--3924.  ",
      "doi": "10.1145/1520340.1520594"
    },
    {
      "text": "Yanco, H. Wheelesley: a robotic wheelchair system: indoor navigation and user interface. Assistive Technology and Artificial Intelligence, (1998), 256--268. ",
      "doi": "10.5555/646629.696524"
    },
    {
      "text": "Zhai, S., Morimoto, C. and Ihde, S. Manual and gaze input cascaded (MAGIC) pointing. In Proc. CHI 1999, ACM Press (1999), 246--253.  ",
      "doi": "10.1145/302979.303053"
    },
    {
      "text": "Zhu, D., Gedeon, T. D. and Taylor, K. Keyboard before head tracking depresses user success in remote camera control. In Proc. INTERACT 2009, Springer Press (2009), LNCS 5727, 319--331.  ",
      "doi": "10.1007/978-3-642-03658-3_37"
    },
    {
      "text": "Zhu, D., Gedeon, T. D. and Taylor, K. Natural interaction enhanced remote camera control for teleoperation. Ext. Abstracts CHI 2010, ACM Press (2010), 3229--3234.  ",
      "doi": "10.1145/1753846.1753963"
    }
  ]
}