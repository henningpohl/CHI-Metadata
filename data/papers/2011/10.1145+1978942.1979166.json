{
  "doi": "10.1145/1978942.1979166",
  "title": "An examination of two delivery modes for interactive search system experiments: remote and laboratory",
  "published": "2011-05-07",
  "proctitle": "CHI '11: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems",
  "pages": "1531-1540",
  "year": 2011,
  "badges": [],
  "abstract": "We compare two delivery modes for interactive search system (ISS) experiments: remote and laboratory. Our study was completed by two groups of subjects from the same population. The first group completed the study remotely and the second group completed the study in the laboratory. We compare differences in participants, participation behaviors, search behaviors and evaluation behaviors. Overall, for most measures no significant differences were found, but there were some notable differences. Greater variance was observed in time taken and number of documents opened and saved by remote subjects. Lab subjects provided more favorable responses to exit questionnaire items and reported significantly higher satisfaction. Lab subjects also provided significantly longer responses to open questions, while remote subjects provided more null responses. These results suggest that many behaviors do not change significantly according to study mode and that results from remote ISS experiments are similar to those from laboratory experiments.",
  "authors": [
    {
      "name": "Diane Kelly",
      "institution": "University of North Carolina at Chapel Hill, Chapel Hill, N. Carolina, USA",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81100523980",
      "orcid": "missing"
    },
    {
      "name": "Karl Gyllstrom",
      "institution": "Katholieke Universiteit Leuven, Leuven, Belgium",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81100532446",
      "orcid": "missing"
    }
  ],
  "references": [
    {
      "text": "Alonso, O. &amp;#38; Mizzaro, S. Relevance critieria for e-commerce: A crowdsourcing-based experimental analysis. In Proc. SIGIR 2009, ACM (2009), 760--761.  ",
      "doi": "10.1145/1571941.1572115"
    },
    {
      "text": "Alonso, O., Rose, D. E., &amp;#38; Stewart, B. Crowdsourcing for relevance evaluation. SIGIR Forum, 42(2) (2008).  ",
      "doi": "10.1145/1480506.1480508"
    },
    {
      "text": "Amazon Mechanical Turk. https://www.mturk.com.",
      "doi": ""
    },
    {
      "text": "Andreasen, M. S., Nielsen, H. V., Schr&amp;#248;der, S. O., &amp;#38; Stage, J. What happened to remote usability testing? An empirical study of three methods. In Proc. CHI 2007, ACM Press, (2007), 1405--1414.  ",
      "doi": "10.1145/1240624.1240838"
    },
    {
      "text": "Anick, P. Using terminological feedback for web search refinement: A log based study. In Proc. SIGIR 2003, ACM Press, 2003, 88--95.  ",
      "doi": "10.1145/860435.860453"
    },
    {
      "text": "Birnbaum, M. H. Psychological Experiments on the Internet. London, UK: Academic Press, 2000.",
      "doi": ""
    },
    {
      "text": "Bruun, A., Gull, P., Hofmeister, L. &amp;#38; Stage, J. Let your users do the testing: A comparison of three remote asynchronous usability testing methods. In Proc. CHI 2009, ACM Press (2009), 1619--1628.  ",
      "doi": "10.1145/1518701.1518948"
    },
    {
      "text": "Campbell, D. T. &amp;#38; Stanley, J. C. Experimental and quasi-experimental designs for research. Chicago: Rand McNally, 1966.",
      "doi": ""
    },
    {
      "text": "Downs, J. S., Holbrook, M. B., Sheng, S., &amp;#38; Cranor, L. F. Are your participants gaming the system? Screening Mechanical Turk workers. In Proc. CHI 2010, ACM Press (2010), 2399--2402.  ",
      "doi": "10.1145/1753326.1753688"
    },
    {
      "text": "Dumais, S. T. &amp;#38; Belkin, N. J. The TREC Interactive Tracks: Putting the user into search. In TREC: Experiment and Evaluation in Information Retrieval (pp. 123--153), Cambridge: MIT Press, 2005.",
      "doi": ""
    },
    {
      "text": "1ESP Online Game. http://www.gwap.com/gwap/gamesPreview/espgame/.",
      "doi": ""
    },
    {
      "text": "Hammontree, M., Weiler, M. &amp;#38; Nayak, N. Remote usability testing. Interactions 1(3) (1994), 21--25.  ",
      "doi": "10.1145/182966.182969"
    },
    {
      "text": "Hartson, H. R., Castillo, J.C., Kelso, J., &amp;#38; Neale, W. C. Remote evaluation: The network as an extension of the usability laboratory. In Proc. of CHI 1996, ACM Press (1996), 228--235.  ",
      "doi": "10.1145/238386.238511"
    },
    {
      "text": "Kazai, G., Milic-Frayling, N. &amp;#38; Costello, J. Towards methods for the collective gathering and quality control of relevance assessments. In Proc. SIGIR 2009, ACM Press (2009), 452--459.  ",
      "doi": "10.1145/1571941.1572019"
    },
    {
      "text": "Kelly, D. Methods for evaluating interactive information retrieval systems with users. Foundations &amp;#38; Trends in Information Retrieval, 3(1-2) (2009), 1--224.  ",
      "doi": "10.1561/1500000012"
    },
    {
      "text": "Kelly, D., Gyllstrom, K., &amp;#38; Bailey, E. (2009). A comparison of term and query suggestion features for interactive searching. In Proc. of SIGIR 2009, ACM Press (2009), 371--378.  ",
      "doi": "10.1145/1571941.1572006"
    },
    {
      "text": "Kittur, A., Chi, E. H., &amp;#38; Suh, B. Crowdsourcing user studies with Mechanical Turk. In Proc. CHI 2008, ACM Press (2008), 453--456.  ",
      "doi": "10.1145/1357054.1357127"
    },
    {
      "text": "Lemur IR Toolkit. http://www.lemurproject.org/.",
      "doi": ""
    },
    {
      "text": "McFadden, E., Hager, D. R., Elie, C. J., &amp;#38; Blackwell, J. M. Remote usability evaluation: Overview and case studies. International Journal of Human-Computer Interaction, 14(3--4) (2002), 489--502.",
      "doi": ""
    },
    {
      "text": "Mulaik, S. A. Foundations of factor analysis. Boca Raton, FL: CRC Press, 1972.",
      "doi": ""
    },
    {
      "text": "Radlinski, F., Kurup, M. &amp;#38; Joachims, T. (2008). How does clickthrough data reflect retrieval quality? In Proc. of CIKM 2008, ACM Press (2008), 43--52.  ",
      "doi": "10.1145/1458082.1458092"
    },
    {
      "text": "Reips, U.-D. The Web's Experimental Psychology Lab. http://iscience.deusto.es/archive/ulf/Lab/WebExpPsyLab.html, 1995.",
      "doi": ""
    },
    {
      "text": "Robertson, S. E. On the history of evaluation in IR. Journal of Information Science, 34(4) (2008), 439--456.  ",
      "doi": "10.1177/0165551507086989"
    },
    {
      "text": "Ross, J., Irani, L., Silberman, M. S., Saldivar, A., &amp;#38; Tomlinson, B. Who are the crowdworkers? Shifting demographics in Mechanical Turk. In Proc. CHI 2010, ACM Press (2010), 2863--2872.  ",
      "doi": "10.1145/1753846.1753873"
    },
    {
      "text": "Scholtz, J. Adaptation of traditional usability testing method for remote testing. In Proc. HICSS 2001, 1--8. ",
      "doi": "10.5555/820757.821905"
    },
    {
      "text": "Scriven, M. Evaluation thesaurus (4th edition). Newbury Park, CA: Sage Publications, 1991.",
      "doi": ""
    },
    {
      "text": "Sp&amp;#228;rck-Jones, K. Information Retrieval Experiment. London, UK: Butterworths &amp;#38; Co. Ltd, 1981. ",
      "doi": "10.5555/539571"
    },
    {
      "text": "Tague-Sutcliffe, J. M. The pragmatics of information retrieval experimentation, revisted. Information Processing &amp;#38; Management, 28(4) (1992), 467--490.  ",
      "doi": "10.1016/0306-4573%2892%2990005-K"
    },
    {
      "text": "Toms, E. G., Freund, L. &amp;#38; Li, C. WISSE: The Web interactive information retrieval experimentation system prototype. Information Processing &amp;#38; Management, 40(5) (2004), 655--675.  ",
      "doi": "10.1016/j.ipm.2003.08.006"
    },
    {
      "text": "von Ahn, L. Games with a purpose. IEEE Computer Magazine, June 2006, 96--98.  ",
      "doi": "10.1109/MC.2006.196"
    },
    {
      "text": "Voorhees, E. M. Overview of the TREC 2005 Robust Retrieval Track. Proc. of TREC-14 (2006).",
      "doi": ""
    },
    {
      "text": "Voorhees, E. M. &amp;#38; Harman, D. K. TREC: Experiment and Evaluation in Information Retrieval, Cambridge, MA: MIT Press (2005). ",
      "doi": "10.5555/1121636"
    }
  ]
}