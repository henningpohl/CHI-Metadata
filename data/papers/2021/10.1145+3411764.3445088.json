{
  "doi": "10.1145/3411764.3445088",
  "title": "Beyond Expertise and Roles: A Framework to Characterize the Stakeholders of Interpretable Machine Learning and their Needs",
  "published": "2021-05-07",
  "proctitle": "CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems",
  "pages": "1-16",
  "year": 2021,
  "badges": [],
  "abstract": "To ensure accountability and mitigate harm, it is critical that diverse stakeholders can interrogate black-box automated systems and find information that is understandable, relevant, and useful to them. In this paper, we eschew prior expertise- and role-based categorizations of interpretability stakeholders in favor of a more granular framework that decouples stakeholders\u2019 knowledge from their interpretability needs. We characterize stakeholders by their formal, instrumental, and personal knowledge and how it manifests in the contexts of machine learning, the data domain, and the general milieu. We additionally distill a hierarchical typology of stakeholder needs that distinguishes higher-level domain goals from lower-level interpretability tasks. In assessing the descriptive, evaluative, and generative powers of our framework, we find our more nuanced treatment of stakeholders reveals gaps and opportunities in the interpretability literature, adds precision to the design and comparison of user studies, and facilitates a more reflexive approach to conducting this research.",
  "authors": [
    {
      "name": "Harini Suresh",
      "institution": "MIT, United States",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "87059106057",
      "orcid": "0000-0002-9769-4947"
    },
    {
      "name": "Steven R. Gomez",
      "institution": "MIT, United States",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659229933",
      "orcid": "missing"
    },
    {
      "name": "Kevin K. Nam",
      "institution": "MIT, United States",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659701523",
      "orcid": "missing"
    },
    {
      "name": "Arvind Satyanarayan",
      "institution": "MIT, United States",
      "img": "/do/10.1145/contrib-81500663321/rel-imgonly/arvind-satyanarayan.jpg",
      "acmid": "81500663321",
      "orcid": "0000-0001-5564-635X"
    }
  ],
  "references": [
    {
      "text": "2020. ML Interpretability for Scientific Discovery (MLI4SD) Workshop. https://sites.google.com/view/mli4sd-icml2020/home. Accessed: 2020-09-16.",
      "doi": ""
    },
    {
      "text": "Agnar Aamodt and Enric Plaza. 1994. Case-based reasoning: Foundational issues, methodological variations, and system approaches. AI communications 7, 1 (1994), 39\u201359.",
      "doi": "10.5555/196108.196115"
    },
    {
      "text": "Ashraf Abdul, Jo Vermeulen, Danding Wang, Brian\u00a0Y. Lim, and Mohan Kankanhalli. 2018. Trends and Trajectories for Explainable, Accountable and Intelligible Systems: An HCI Research Agenda. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems - CHI \u201918. ACM Press, Montreal QC, Canada, 1\u201318. https://doi.org/10.1145/3173574.3174156",
      "doi": "10.1145/3173574.3174156"
    },
    {
      "text": "Ashraf Abdul, Christian von\u00a0der Weth, Mohan Kankanhalli, and Brian\u00a0Y Lim. 2020. COGAM: Measuring and Moderating Cognitive Load in Machine Learning Model Explanations. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. 1\u201314.",
      "doi": "10.1145/3313831.3376615"
    },
    {
      "text": "Amina Adadi and Mohammed Berrada. 2018. Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI). IEEE Access 6(2018), 52138\u201352160. https://doi.org/10.1109/ACCESS.2018.2870052Conference Name: IEEE Access.",
      "doi": "10.1109/access.2018.2870052"
    },
    {
      "text": "Ali Alkhatib and Michael Bernstein. 2019. Street-level algorithms: A theory at the gaps between policy and decisions. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. 1\u201313.",
      "doi": "10.1145/3290605.3300760"
    },
    {
      "text": "Vijay Arya, Rachel K.\u00a0E. Bellamy, Pin-Yu Chen, Amit Dhurandhar, Michael Hind, Samuel\u00a0C. Hoffman, Stephanie Houde, Q.\u00a0Vera Liao, Ronny Luss, Aleksandra Mojsilovi\u0107, Sami Mourad, Pablo Pedemonte, Ramya Raghavendra, John Richards, Prasanna Sattigeri, Karthikeyan Shanmugam, Moninder Singh, Kush\u00a0R. Varshney, Dennis Wei, and Yunfeng Zhang. 2019. One Explanation Does Not Fit All: A Toolkit and Taxonomy of AI Explainability Techniques. arXiv:1909.03012 [cs, stat] (Sept. 2019). http://arxiv.org/abs/1909.03012 arXiv:1909.03012.",
      "doi": ""
    },
    {
      "text": "Robert\u00a0K Atkinson, Sharon\u00a0J Derry, Alexander Renkl, and Donald Wortham. 2000. Learning from examples: Instructional principles from the worked examples research. Review of educational research 70, 2 (2000), 181\u2013214.",
      "doi": ""
    },
    {
      "text": "Sebastian Bach, Alexander Binder, Gr\u00e9goire Montavon, Frederick Klauschen, Klaus-Robert M\u00fcller, and Wojciech Samek. 2015. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS one 10, 7 (2015), e0130140.",
      "doi": ""
    },
    {
      "text": "Krisztian Balog and Filip Radlinski. 2020. Measuring Recommendation Explanation Quality:The Conflicting Goals of Explanations. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR \u201920). ACM, New York, NY, USA, Virtual Event, 10.",
      "doi": ""
    },
    {
      "text": "Gagan Bansal, Besmira Nushi, Ece Kamar, Walter Lasecki, Daniel\u00a0S Weld, and Eric Horvitz. [n.d.]. Beyond Accuracy: The Role of Mental Models in Human-AI Team Performance. ([n.\u00a0d.]), 10.",
      "doi": ""
    },
    {
      "text": "Gagan Bansal, Tongshuang Wu, Joyce Zhou, Raymond Fok, Besmira Nushi, Ece Kamar, Marco\u00a0Tulio Ribeiro, and Daniel\u00a0S. Weld. 2020. Does the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance. arXiv:2006.14779 [cs] (June 2020). http://arxiv.org/abs/2006.14779 arXiv:2006.14779.",
      "doi": ""
    },
    {
      "text": "Chelsea Barabas, Colin Doyle, JB Rubinovitz, and Karthik Dinakar. 2020. Studying up: reorienting the study of algorithmic fairness around issues of power. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. 167\u2013176.",
      "doi": "10.1145/3351095.3372859"
    },
    {
      "text": "Michel Beaudouin-Lafon. 2004. Designing interaction, not interfaces. In Proceedings of the working conference on Advanced visual interfaces. 15\u201322.",
      "doi": "10.1145/989863.989865"
    },
    {
      "text": "Cornelia Betsch. 2004. Pr\u00e4ferenz f\u00fcr intuition und deliberation (PID). Zeitschrift f\u00fcr Differentielle und Diagnostische Psychologie 25, 4(2004), 179\u2013197.",
      "doi": ""
    },
    {
      "text": "Aviruch Bhatia, Vishal Garg, Philip Haves, and Vikram Pudi. 2019. Explainable Clustering Using Hyper-Rectangles for Building Energy Simulation Data. E&ES 238, 1 (2019), 012068.",
      "doi": ""
    },
    {
      "text": "Umang Bhatt, Alice Xiang, Shubham Sharma, Adrian Weller, Ankur Taly, Yunhan Jia, Joydeep Ghosh, Ruchir Puri, Jos\u00e9 M.\u00a0F. Moura, and Peter Eckersley. 2020. Explainable machine learning in deployment. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency(FAT* \u201920). Association for Computing Machinery, Barcelona, Spain, 648\u2013657. https://doi.org/10.1145/3351095.3375624",
      "doi": "10.1145/3351095.3375624"
    },
    {
      "text": "Stephen Billett (Ed.). 2010. Learning Through Practice. Springer Netherlands, Dordrecht. https://doi.org/10.1007/978-90-481-3939-2",
      "doi": ""
    },
    {
      "text": "Nadia Boukhelifa, Anastasia Bezerianos, Ioan\u00a0Cristian Trelea, Nathalie\u00a0M\u00e9jean Perrot, and Evelyne Lutton. 2019. An Exploratory Study on Visual Exploration of Model Simulations by Multiple Types of Experts. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems - CHI \u201919. ACM Press, Glasgow, Scotland Uk, 1\u201314. https://doi.org/10.1145/3290605.3300874",
      "doi": "10.1145/3290605.3300874"
    },
    {
      "text": "Matthew Brehmer and Tamara Munzner. 2013. A multi-level typology of abstract visualization tasks. IEEE transactions on visualization and computer graphics 19, 12(2013), 2376\u20132385.",
      "doi": "10.1109/TVCG.2013.124"
    },
    {
      "text": "Andrea Brennen. 2020. What Do People Really Want When They Say They Want \u201dExplainable AI?\u201d We Asked 60 Stakeholders.. In Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems(CHI EA \u201920). Association for Computing Machinery, Honolulu, HI, USA, 1\u20137. https://doi.org/10.1145/3334480.3383047",
      "doi": "10.1145/3334480.3383047"
    },
    {
      "text": "Peter Buneman, Sanjeev Khanna, and Tan Wang-Chiew. 2001. Why and where: A characterization of data provenance. In International conference on database theory. Springer, 316\u2013330.",
      "doi": ""
    },
    {
      "text": "Adrian Bussone, Simone Stumpf, and Dympna O\u2019Sullivan. 2015. The Role of Explanations on Trust and Reliance in Clinical Decision Support Systems. In 2015 International Conference on Healthcare Informatics. IEEE, Dallas, TX, USA, 160\u2013169. https://doi.org/10.1109/ICHI.2015.26",
      "doi": "10.1109/ICHI.2015.26"
    },
    {
      "text": "Zana Bu\u00e7inca, Phoebe Lin, Krzysztof\u00a0Z. Gajos, and Elena\u00a0L. Glassman. 2020. Proxy Tasks and Subjective Measures Can Be Misleading in Evaluating Explainable AI Systems. Proceedings of the 25th International Conference on Intelligent User Interfaces (March 2020), 454\u2013464. https://doi.org/10.1145/3377325.3377498arXiv:2001.08298.",
      "doi": "10.1145/3377325.3377498"
    },
    {
      "text": "Ruth M.\u00a0J. Byrne. 2019. Counterfactuals in Explainable Artificial Intelligence (XAI): Evidence from Human Reasoning. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence Organization, Macao, China, 6276\u20136282. https://doi.org/10.24963/ijcai.2019/876",
      "doi": ""
    },
    {
      "text": "Carrie\u00a0J Cai, Jonas Jongejan, and Jess Holbrook. 2019. The effects of example-based explanations in a machine learning interface. In Proceedings of the 24th International Conference on Intelligent User Interfaces. 258\u2013262.",
      "doi": "10.1145/3301275.3302289"
    },
    {
      "text": "Carrie\u00a0J. Cai, Samantha Winter, David Steiner, Lauren Wilcox, and Michael Terry. 2019. \u201dHello AI\u201d: Uncovering the Onboarding Needs of Medical Practitioners for Human-AI Collaborative Decision-Making. Proceedings of the ACM on Human-Computer Interaction 3, CSCW (Nov. 2019), 1\u201324. https://doi.org/10.1145/3359206",
      "doi": "10.1145/3359206"
    },
    {
      "text": "Chaofan Chen, Oscar Li, Daniel Tao, Alina Barnett, Cynthia Rudin, and Jonathan\u00a0K Su. 2019. This looks like that: deep learning for interpretable image recognition. In Advances in neural information processing systems. 8930\u20138941.",
      "doi": ""
    },
    {
      "text": "Hao-Fei Cheng, Ruotong Wang, Zheng Zhang, Fiona O\u2019Connell, Terrance Gray, F\u00a0Maxwell Harper, and Haiyi Zhu. 2019. Explaining decision-making algorithms through UI: Strategies to help non-expert stakeholders. In Proceedings of the 2019 chi conference on human factors in computing systems. 1\u201312.",
      "doi": "10.1145/3290605.3300789"
    },
    {
      "text": "Michael Chromik, Malin Eiband, Sarah\u00a0Theres V\u00f6lkel, and Daniel Buschek. 2019. Dark Patterns of Explainability, Transparency, and User Control for Intelligent Systems. Los Angeles (2019), 6.",
      "doi": ""
    },
    {
      "text": "Danielle\u00a0Keats Citron and Frank Pasquale. 2014. The scored society: Due process for automated predictions. Wash. L. Rev. 89(2014), 1.",
      "doi": ""
    },
    {
      "text": "Michael Correll. 2019. Ethical dimensions of visualization research. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. 1\u201313.",
      "doi": "10.1145/3290605.3300418"
    },
    {
      "text": "Amanda Cox. 2011. Shaping Data for News. https://vimeo.com/29391942",
      "doi": ""
    },
    {
      "text": "Gloria Dall\u2019Alba and J\u00f6rgen Sandberg. 2006. Unveiling professional development: A critical review of stage models. Review of educational research 76, 3 (2006), 383\u2013412.",
      "doi": ""
    },
    {
      "text": "Arun Das and Paul Rad. 2020. Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey. arXiv:2006.11371 [cs] (June 2020). http://arxiv.org/abs/2006.11371 arXiv:2006.11371.",
      "doi": ""
    },
    {
      "text": "Amit Dhurandhar, Vijay Iyengar, Ronny Luss, and Karthikeyan Shanmugam. 2017. A formal framework to characterize interpretability of procedures. (2017). arXiv:1707.03886.",
      "doi": ""
    },
    {
      "text": "Jonathan Dodge, Q\u00a0Vera Liao, Yunfeng Zhang, Rachel\u00a0KE Bellamy, and Casey Dugan. 2019. Explaining models: an empirical study of how explanations impact fairness judgment. In Proceedings of the 24th International Conference on Intelligent User Interfaces. 275\u2013285.",
      "doi": "10.1145/3301275.3302310"
    },
    {
      "text": "Finale Doshi-Velez and Been Kim. 2017. Towards A Rigorous Science of Interpretable Machine Learning. arXiv:1702.08608 [cs, stat] (March 2017). http://arxiv.org/abs/1702.08608 arXiv:1702.08608.",
      "doi": ""
    },
    {
      "text": "Finale Doshi-Velez, Mason Kortz, Ryan Budish, Chris Bavitz, Sam Gershman, David O\u2019Brien, Stuart Schieber, James Waldo, David Weinberger, and Alexandra Wood. 2017. Accountability of AI under the law: The role of explanation. (2017). arXiv:1711.01134.",
      "doi": ""
    },
    {
      "text": "Hubert\u00a0L Dreyfus and Stuart\u00a0E Dreyfus. 1986. The power of human intuition and expertise in the era of the computer. Mind over machine. Nueva York: The Free Press (1986).",
      "doi": "10.5555/7916"
    },
    {
      "text": "Johanna Drucker. 2012. Humanistic theory and digital scholarship. Debates in the digital humanities 150 (2012), 85\u201395.",
      "doi": ""
    },
    {
      "text": "Mengnan Du, Ninghao Liu, and Xia Hu. 2019. Techniques for interpretable machine learning. Commun. ACM 63, 1 (2019), 68\u201377.",
      "doi": "10.1145/3359786"
    },
    {
      "text": "Michael Eraut. 2010. Knowledge, working practices, and learning. In Learning through practice. Springer, 37\u201358.",
      "doi": ""
    },
    {
      "text": "Juliana\u00a0J. Ferreira and Mateus\u00a0S. Monteiro. 2020. What Are People Doing About XAI User Experience? A Survey on AI Explainability Research and Practice. In Design, User Experience, and Usability. Design for Contemporary Interactive Environments(Lecture Notes in Computer Science), Aaron Marcus and Elizabeth Rosenzweig (Eds.). Springer International Publishing, Cham, 56\u201373. https://doi.org/10.1007/978-3-030-49760-6_4",
      "doi": "10.1007/978-3-030-49760-6_4"
    },
    {
      "text": "James Fleck. 1998. Expertise: knowledge, power and tradeability. In Exploring expertise. Springer, 143\u2013171.",
      "doi": ""
    },
    {
      "text": "Ruth\u00a0C. Fong and Andrea Vedaldi. 2017. Interpretable Explanations of Black Boxes by Meaningful Perturbation. In Proceedings of the IEEE International Conference on Computer Vision (ICCV).",
      "doi": ""
    },
    {
      "text": "Leilani\u00a0H. Gilpin, David Bau, Ben\u00a0Z. Yuan, Ayesha Bajwa, Michael Specter, and Lalana Kagal. 2018. Explaining Explanations: An Overview of Interpretability of Machine Learning. In 2018 IEEE 5th International Conference on Data Science and Advanced Analytics (DSAA). 80\u201389. https://doi.org/10.1109/DSAA.2018.00018",
      "doi": ""
    },
    {
      "text": "Alyssa Glass, Deborah\u00a0L. McGuinness, and Michael Wolverton. 2008. Toward establishing trust in adaptive agents. In Proceedings of the 13th international conference on Intelligent user interfaces - IUI \u201908. ACM Press, Gran Canaria, Spain, 227. https://doi.org/10.1145/1378773.1378804",
      "doi": "10.1145/1378773.1378804"
    },
    {
      "text": "Yash Goyal, Ziyan Wu, Jan Ernst, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019. Counterfactual Visual Explanations. In Proceedings of the 36th International Conference on Machine Learning, Vol.\u00a097. Long Beach, California, USA. http://proceedings.mlr.press/v97/goyal19a.html arXiv:1904.07451.",
      "doi": ""
    },
    {
      "text": "Davydd Greenwood and Morten Levin. 2007. Introduction to Action Research. SAGE Publications, Inc., 2455 Teller Road,\u00a0Thousand Oaks\u00a0California\u00a091320\u00a0United States of America. https://doi.org/10.4135/9781412984614",
      "doi": ""
    },
    {
      "text": "Donna Haraway. 1988. Situated knowledges: The science question in feminism and the privilege of partial perspective. Feminist studies 14, 3 (1988), 575\u2013599.",
      "doi": ""
    },
    {
      "text": "Johanna Hartelius. 2008. The Rhetoric of Expertise. Ph.D. Dissertation. The University of Texas at Austin.",
      "doi": ""
    },
    {
      "text": "Johanna Hartelius. 2011. Rhetorics of Expertise. Social Epistemology 25, 3 (July 2011), 211\u2013215. https://doi.org/10.1080/02691728.2011.578301Publisher: Routledge _eprint: https://doi.org/10.1080/02691728.2011.578301.",
      "doi": ""
    },
    {
      "text": "Sam Hepenstal and David McNeish. 2020. Explainable Artificial Intelligence: What Do You Need to Know?. In Augmented Cognition. Theoretical and Technological Approaches, Dylan\u00a0D. Schmorrow and Cali\u00a0M. Fidopiastis (Eds.). Springer International Publishing, Cham, 266\u2013275.",
      "doi": ""
    },
    {
      "text": "Mireille Hildebrandt. 2012. The Dawn of a Critical Transparency Right for the Profiling Era. Astronomy & Astrophysics - ASTRON ASTROPHYS (06 2012). https://doi.org/10.3233/978-1-61499-057-4-41",
      "doi": ""
    },
    {
      "text": "Fred Hohman, Andrew Head, Rich Caruana, Robert DeLine, and Steven\u00a0M. Drucker. 2019. Gamut: A Design Probe to Understand How Data Scientists Understand Machine Learning Models. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (Glasgow, Scotland Uk) (CHI \u201919). Association for Computing Machinery, New York, NY, USA, 1\u201313. https://doi.org/10.1145/3290605.3300809",
      "doi": "10.1145/3290605.3300809"
    },
    {
      "text": "Fred Hohman, Minsuk Kahng, Robert Pienta, and Duen\u00a0Horng Chau. 2019. Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers. IEEE Transactions on Visualization and Computer Graphics 25, 8 (Aug. 2019), 2674\u20132693. https://doi.org/10.1109/TVCG.2018.2843369",
      "doi": "10.1109/TVCG.2018.2843369"
    },
    {
      "text": "Sungsoo\u00a0Ray Hong, Jessica Hullman, and Enrico Bertini. 2020. Human Factors in Model Interpretability: Industry Practices, Challenges, and Needs. Proceedings of the ACM on Human-Computer Interaction 4, CSCW1 (May 2020), 1\u201326. https://doi.org/10.1145/3392878",
      "doi": "10.1145/3392878"
    },
    {
      "text": "Jessica Hullman and Nick Diakopoulos. 2011. Visualization rhetoric: Framing effects in narrative visualization. IEEE transactions on visualization and computer graphics 17, 12(2011), 2231\u20132240.",
      "doi": ""
    },
    {
      "text": "Janet Jull, Audrey Giles, and Ian\u00a0D. Graham. 2017. Community-based participatory research and integrated knowledge translation: advancing the co-creation of knowledge. Implementation Science 12, 1 (Dec. 2017), 150. https://doi.org/10.1186/s13012-017-0696-3",
      "doi": ""
    },
    {
      "text": "Harmanpreet Kaur, Harsha Nori, Samuel Jenkins, Rich Caruana, Hanna Wallach, and Jennifer Wortman\u00a0Vaughan. 2020. Interpreting Interpretability: Understanding Data Scientists\u2019 Use of Interpretability Tools for Machine Learning. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. ACM, Honolulu HI USA, 1\u201314. https://doi.org/10.1145/3313831.3376219",
      "doi": "10.1145/3313831.3376219"
    },
    {
      "text": "Helen Kennedy, Rosemary\u00a0Lucy Hill, Giorgia Aiello, and William Allen. 2016. The work that visualisation conventions do. Information, Communication & Society 19, 6 (2016), 715\u2013735.",
      "doi": ""
    },
    {
      "text": "Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, 2018. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). In International conference on machine learning. PMLR, 2668\u20132677.",
      "doi": ""
    },
    {
      "text": "Ian\u00a0M. Kinchin and B. Cabot. 2010. Reconsidering the dimensions of expertise: from linear stages towards dual processing. London Review of Education (July 2010). https://doi.org/10.1080/14748460.2010.487334",
      "doi": ""
    },
    {
      "text": "Ren\u00e9\u00a0F Kizilcec. 2016. How much information? Effects of transparency on trust in an algorithmic interface. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems. 2390\u20132395.",
      "doi": "10.1145/2858036.2858402"
    },
    {
      "text": "Ha-Kyung Kong, Zhicheng Liu, and Karrie Karahalios. 2018. Frames and slants in titles of visualizations on controversial topics. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems. 1\u201312.",
      "doi": "10.1145/3173574.3174012"
    },
    {
      "text": "Ha-Kyung Kong, Zhicheng Liu, and Karrie Karahalios. 2019. Trust and recall of information across varying degrees of title-visualization misalignment. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. 1\u201313.",
      "doi": "10.1145/3290605.3300576"
    },
    {
      "text": "Luciana\u00a0Monteiro Krebs, Oscar\u00a0Luis Alvarado\u00a0Rodriguez, Pierre Dewitte, Jef Ausloos, David Geerts, Laurens Naudts, and Katrien Verbert. 2019. Tell me what you know: GDPR implications on designing transparency and accountability for news recommender systems. In Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems. 1\u20136.",
      "doi": "10.1145/3290607.3312808"
    },
    {
      "text": "Bogdan Kulynych, David Madras, Smitha Milli, Inioluwa\u00a0Deborah Raji, Angela Zhou, and Richard Zemel. 2020. Participatory Approaches to Machine Learning. International Conference on Machine Learning Workshop.",
      "doi": ""
    },
    {
      "text": "David\u00a0F Labaree. 2000. On the nature of teaching and teacher education: Difficult practices that look easy. Journal of teacher education 51, 3 (2000), 228\u2013233.",
      "doi": ""
    },
    {
      "text": "Isaac Lage, Emily Chen, Jeffrey He, Menaka Narayanan, Been Kim, Samuel\u00a0J. Gershman, and Finale Doshi-Velez. 2019. Human Evaluation of Models Built for Interpretability. Proceedings of the AAAI Conference on Human Computation and Crowdsourcing 7, 1 (Oct. 2019), 59\u201367. https://www.aaai.org/ojs/index.php/HCOMP/article/view/5280 Number: 1.",
      "doi": ""
    },
    {
      "text": "Vivian Lai and Chenhao Tan. 2019. On Human Predictions with Explanations and Predictions of Machine Learning Models: A Case Study on Deception Detection. In Proceedings of the Conference on Fairness, Accountability, and Transparency - FAT* \u201919. ACM Press, Atlanta, GA, USA, 29\u201338. https://doi.org/10.1145/3287560.3287590",
      "doi": "10.1145/3287560.3287590"
    },
    {
      "text": "Himabindu Lakkaraju and Osbert Bastani. 2020. \u201dHow do I fool you?\u201d: Manipulating User Trust via Misleading Black Box Explanations. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society(AIES \u201920). Association for Computing Machinery, New York, NY, USA, 79\u201385. https://doi.org/10.1145/3375627.3375833",
      "doi": "10.1145/3375627.3375833"
    },
    {
      "text": "Christopher\u00a0A. Le\u00a0Dantec and Sarah Fox. 2015. Strangers at the Gate: Gaining Access, Building Rapport, and Co-Constructing Community-Based Research. In Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work & Social Computing - CSCW \u201915. ACM Press, Vancouver, BC, Canada, 1348\u20131358. https://doi.org/10.1145/2675133.2675147",
      "doi": ""
    },
    {
      "text": "Bongshin Lee, Kate Isaacs, Danielle\u00a0Albers Szafir, G\u00a0Elisabeta Marai, Cagatay Turkay, Melanie Tory, Sheelagh Carpendale, and Alex Endert. 2019. Broadening intellectual diversity in visualization research papers. IEEE computer graphics and applications 39, 4 (2019), 78\u201385.",
      "doi": ""
    },
    {
      "text": "Q.\u00a0Vera Liao, Daniel Gruen, and Sarah Miller. 2020. Questioning the AI: Informing Design Practices for Explainable AI User Experiences. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems(CHI \u201920). Association for Computing Machinery, Honolulu, HI, USA, 1\u201315. https://doi.org/10.1145/3313831.3376590",
      "doi": "10.1145/3313831.3376590"
    },
    {
      "text": "Brian\u00a0Y. Lim and Anind\u00a0K. Dey. 2009. Assessing Demand for Intelligibility in Context-Aware Applications. In Proceedings of the 11th International Conference on Ubiquitous Computing (Orlando, Florida, USA) (UbiComp \u201909). Association for Computing Machinery, New York, NY, USA, 195\u2013204. https://doi.org/10.1145/1620545.1620576",
      "doi": "10.1145/1620545.1620576"
    },
    {
      "text": "Brian\u00a0Y. Lim and Anind\u00a0K. Dey. 2010. Toolkit to Support Intelligibility in Context-Aware Applications. In Proceedings of the 12th ACM International Conference on Ubiquitous Computing (Copenhagen, Denmark) (UbiComp \u201910). Association for Computing Machinery, New York, NY, USA, 13\u201322. https://doi.org/10.1145/1864349.1864353",
      "doi": ""
    },
    {
      "text": "Brian\u00a0Y. Lim, Anind\u00a0K. Dey, and Daniel Avrahami. 2009. Why and Why Not Explanations Improve the Intelligibility of Context-Aware Intelligent Systems. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (Boston, MA, USA) (CHI \u201909). Association for Computing Machinery, New York, NY, USA, 2119\u20132128. https://doi.org/10.1145/1518701.1519023",
      "doi": "10.1145/1518701.1519023"
    },
    {
      "text": "Zachary\u00a0C. Lipton. 2018. The mythos of model interpretability. Commun. ACM 61, 10 (Sept. 2018), 36\u201343. https://doi.org/10.1145/3233231",
      "doi": "10.1145/3233231"
    },
    {
      "text": "Scott\u00a0M Lundberg, Bala Nair, Monica\u00a0S Vavilala, Mayumi Horibe, Michael\u00a0J Eisses, Trevor Adams, David\u00a0E Liston, Daniel King-Wai Low, Shu-Fang Newman, Jerry Kim, 2018. Explainable machine-learning predictions for the prevention of hypoxaemia during surgery. Nature biomedical engineering 2, 10 (2018), 749\u2013760.",
      "doi": ""
    },
    {
      "text": "George\u00a0A Miller. 1956. The magical number seven, plus or minus two: Some limits on our capacity for processing information.Psychological review 63, 2 (1956), 81.",
      "doi": ""
    },
    {
      "text": "Tim Miller. 2019. Explanation in artificial intelligence: Insights from the social sciences. Artificial Intelligence 267 (Feb. 2019), 1\u201338. https://doi.org/10.1016/j.artint.2018.07.007",
      "doi": ""
    },
    {
      "text": "Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa\u00a0Deborah Raji, and Timnit Gebru. 2019. Model cards for model reporting. In Proceedings of the conference on fairness, accountability, and transparency. 220\u2013229.",
      "doi": "10.1145/3287560.3287596"
    },
    {
      "text": "Shakir Mohamed, Marie-Therese Png, and William Isaac. 2020. Decolonial AI: Decolonial Theory as Sociotechnical Foresight in Artificial Intelligence. Philosophy & Technology (July 2020). https://doi.org/10.1007/s13347-020-00405-8arXiv:2007.04068.",
      "doi": ""
    },
    {
      "text": "Sina Mohseni, Jeremy\u00a0E. Block, and Eric\u00a0D. Ragan. 2020. A Human-Grounded Evaluation Benchmark for Local Explanations of Machine Learning. arXiv:1801.05075 [cs] (June 2020). http://arxiv.org/abs/1801.05075 arXiv:1801.05075.",
      "doi": ""
    },
    {
      "text": "Sina Mohseni, Niloofar Zarei, and Eric\u00a0D. Ragan. 2020. A Multidisciplinary Survey and Framework for Design and Evaluation of Explainable AI Systems. arXiv:1811.11839 [cs] (April 2020). http://arxiv.org/abs/1811.11839 arXiv:1811.11839.",
      "doi": ""
    },
    {
      "text": "Menaka Narayanan, Emily Chen, Jeffrey He, Been Kim, Sam Gershman, and Finale Doshi-Velez. 2018. How do humans understand explanations from machine learning systems? an evaluation of the human-interpretability of explanation. arXiv:1802.00682 (2018).",
      "doi": ""
    },
    {
      "text": "Ingrid Nunes and Dietmar Jannach. 2017. A systematic review and taxonomy of explanations in decision support and recommender systems. User Modeling and User-Adapted Interaction 27, 3-5 (2017), 393\u2013444.",
      "doi": "10.1007/s11257-017-9195-0"
    },
    {
      "text": "Ihudiya\u00a0Finda Ogbonnaya-Ogburu, Angela\u00a0D.R. Smith, Alexandra To, and Kentaro Toyama. 2020. Critical Race Theory for HCI. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. ACM, Honolulu HI USA, 1\u201316. https://doi.org/10.1145/3313831.3376392",
      "doi": "10.1145/3313831.3376392"
    },
    {
      "text": "Chris Olah, Alexander Mordvintsev, and Ludwig Schubert. 2017. Feature visualization. Distill 2, 11 (2017), e7.",
      "doi": ""
    },
    {
      "text": "Chris Olah, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schubert, Katherine Ye, and Alexander Mordvintsev. 2018. The building blocks of interpretability. Distill 3, 3 (2018), e10.",
      "doi": ""
    },
    {
      "text": "Prajwal Paudyal, Junghyo Lee, Azamat Kamzin, Mohamad Soudki, Ayan Banerjee, and Sandeep\u00a0KS Gupta. 2019. Learn2Sign: Explainable AI for Sign Language Learning.. In IUI Workshops.",
      "doi": ""
    },
    {
      "text": "Evan\u00a0M Peck, Sofia\u00a0E Ayuso, and Omar El-Etr. 2019. Data is personal: Attitudes and perceptions of data visualization in rural pennsylvania. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. 1\u201312.",
      "doi": "10.1145/3290605.3300474"
    },
    {
      "text": "Forough Poursabzi-Sangdeh, Daniel\u00a0G. Goldstein, Jake\u00a0M. Hofman, Jennifer\u00a0Wortman Vaughan, and Hanna Wallach. 2019. Manipulating and Measuring Model Interpretability. arXiv:1802.07810 [cs] (Nov. 2019). http://arxiv.org/abs/1802.07810 arXiv:1802.07810.",
      "doi": ""
    },
    {
      "text": "Alun Preece, Dan Harborne, Dave Braines, Richard Tomsett, and Supriyo Chakraborty. 2018. Stakeholders in Explainable AI. arXiv:1810.00184 [cs] (Sept. 2018). http://arxiv.org/abs/1810.00184 arXiv:1810.00184.",
      "doi": ""
    },
    {
      "text": "Inioluwa\u00a0Deborah Raji, Andrew Smart, Rebecca\u00a0N White, Margaret Mitchell, Timnit Gebru, Ben Hutchinson, Jamila Smith-Loud, Daniel Theron, and Parker Barnes. 2020. Closing the AI accountability gap: defining an end-to-end framework for internal algorithmic auditing. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. 33\u201344.",
      "doi": "10.1145/3351095.3372873"
    },
    {
      "text": "Gabri\u00eblle Ras, Marcel van Gerven, and Pim Haselager. 2018. Explanation Methods in Deep Learning: Users, Values, Concerns and Challenges. In Explainable and Interpretable Models in Computer Vision and Machine Learning, Hugo\u00a0Jair Escalante, Sergio Escalera, Isabelle Guyon, Xavier Bar\u00f3, Ya\u011fmur G\u00fc\u00e7l\u00fct\u00fcrk, Umut G\u00fc\u00e7l\u00fc, and Marcel van Gerven(Eds.). Springer International Publishing, Cham, 19\u201336. https://doi.org/10.1007/978-3-319-98131-4_2",
      "doi": ""
    },
    {
      "text": "Alexander Renkl. 2014. Toward an instructionally oriented theory of example-based learning. Cognitive science 38, 1 (2014), 1\u201337.",
      "doi": ""
    },
    {
      "text": "Marco\u00a0Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. Model-agnostic interpretability of machine learning. (2016). arXiv:1606.05386.",
      "doi": ""
    },
    {
      "text": "Mireia Ribera and Agata Lapedriza. 2019. Can we do better explanations? A proposal of user-centered explainable AI.. In IUI Workshops.",
      "doi": ""
    },
    {
      "text": "Ribana Roscher, Bastian Bohn, Marco\u00a0F. Duarte, and Jochen Garcke. 2020. Explainable Machine Learning for Scientific Insights and Discoveries. IEEE Access 8(2020), 42200\u201342216. https://doi.org/10.1109/ACCESS.2020.2976199",
      "doi": "10.1109/access.2020.2976199"
    },
    {
      "text": "Wojciech Samek, Thomas Wiegand, and Klaus-Robert M\u00fcller. 2017. Explainable artificial intelligence: Understanding, visualizing and interpreting deep learning models. (2017). arXiv:1708.08296.",
      "doi": ""
    },
    {
      "text": "Udo Schlegel, Hiba Arnout, Mennatallah El-Assady, Daniela Oelke, and Daniel\u00a0A. Keim. 2019. Towards a Rigorous Evaluation of XAI Methods on Time Series. arXiv:1909.07082 [cs] (Sept. 2019). http://arxiv.org/abs/1909.07082 arXiv:1909.07082.",
      "doi": ""
    },
    {
      "text": "Johannes Schneider and Joshua Handali. 2019. PERSONALIZED EXPLANATION FOR MACHINE LEARNING: A CONCEPTUALIZATION. In Proceedings of the 27th European Conference on Information Systems (ECIS). Stockholm & Uppsala, Sweden. https://aisel.aisnet.org/ecis2019_rp/171",
      "doi": ""
    },
    {
      "text": "Ben Shneiderman. 2020. Human-centered artificial intelligence: Reliable, safe & trustworthy. International Journal of Human\u2013Computer Interaction 36, 6(2020), 495\u2013504.",
      "doi": ""
    },
    {
      "text": "Thilo Spinner, Udo Schlegel, Hanna Sch\u00e4fer, and Mennatallah El-Assady. 2020. explAIner: A Visual Analytics Framework for Interactive and Explainable Machine Learning. IEEE Transactions on Visualization and Computer Graphics 26, 1 (Jan. 2020), 1064\u20131074. https://doi.org/10.1109/TVCG.2019.2934629Conference Name: IEEE Transactions on Visualization and Computer Graphics.",
      "doi": ""
    },
    {
      "text": "Clay Spinuzzi. 2005. The methodology of participatory design. Technical communication 52, 2 (2005), 163\u2013174.",
      "doi": ""
    },
    {
      "text": "Mukund Sundararajan, Jinhua Xu, Ankur Taly, Rory Sayres, and Amir Najmi. 2019. Exploring Principled Visualizations for Deep Network Attributions.. In IUI Workshops, Vol.\u00a04.",
      "doi": ""
    },
    {
      "text": "Harini Suresh, Natalie Lao, and Ilaria Liccardi. 2020. Misplaced Trust: Measuring the Interference of Machine Learning in Human Decision-Making. In WebSci \u201920: 12th ACM Conference on Web Science, Southampton, UK, July 6-10, 2020, Emilio Ferrara, Pauline Leonard, and Wendy Hall (Eds.). ACM, 315\u2013324. https://doi.org/10.1145/3394231.3397922",
      "doi": "10.1145/3394231.3397922"
    },
    {
      "text": "Jim Thatcher, David O\u2019Sullivan, and Dillon Mahmoudi. 2016. Data colonialism through accumulation by dispossession: New metaphors for daily data. Environment and Planning D: Society and Space 34, 6 (Dec. 2016), 990\u20131006. https://doi.org/10.1177/0263775816633195",
      "doi": ""
    },
    {
      "text": "Andreas Theodorou, Robert\u00a0H Wortham, and Joanna\u00a0J Bryson. 2017. Designing and implementing transparency for real time inspection of autonomous robots. Connection Science 29, 3 (2017), 230\u2013241.",
      "doi": "10.1080/09540091.2017.1310182"
    },
    {
      "text": "Richard Tomsett, Dave Braines, Dan Harborne, Alun Preece, and Supriyo Chakraborty. 2018. Interpretable to Whom? A Role-based Model for Analyzing Interpretable Machine Learning Systems. In arXiv:1806.07552 [cs]. http://arxiv.org/abs/1806.07552 arXiv:1806.07552.",
      "doi": ""
    },
    {
      "text": "Sana Tonekaboni, Shalmali Joshi, Melissa\u00a0D. McCradden, and Anna Goldenberg. 2019. What Clinicians Want: Contextualizing Explainable Machine Learning for Clinical End Use. In Machine Learning for Healthcare Conference. 359\u2013380. http://proceedings.mlr.press/v106/tonekaboni19a.html ISSN: 1938-7228 Section: Machine Learning.",
      "doi": ""
    },
    {
      "text": "Joe Tullio, Anind\u00a0K. Dey, Jason Chalecki, and James Fogarty. 2007. How it works: a field study of non-technical users interacting with an intelligent system. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems(CHI \u201907). Association for Computing Machinery, San Jose, California, USA, 31\u201340. https://doi.org/10.1145/1240624.1240630",
      "doi": "10.1145/1240624.1240630"
    },
    {
      "text": "Berk Ustun, Alexander Spangher, and Yang Liu. 2019. Actionable Recourse in Linear Classification. In Proceedings of the Conference on Fairness, Accountability, and Transparency (Atlanta, GA, USA) (FAT* \u201919). Association for Computing Machinery, New York, NY, USA, 10\u201319. https://doi.org/10.1145/3287560.3287566",
      "doi": "10.1145/3287560.3287566"
    },
    {
      "text": "Effy Vayena, Alessandro Blasimme, and I.\u00a0Glenn Cohen. 2018. Machine learning in medicine: Addressing ethical challenges. PLOS Medicine 15, 11 (Nov. 2018), e1002689. https://doi.org/10.1371/journal.pmed.1002689",
      "doi": ""
    },
    {
      "text": "Sandra Wachter, Brent Mittelstadt, and Chris Russell. 2018. Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR. Harvard Journal of Law & Technology 31, 2 (March 2018), 841\u2013887. http://arxiv.org/abs/1711.00399 arXiv:1711.00399.",
      "doi": ""
    },
    {
      "text": "Danding Wang, Qian Yang, Ashraf Abdul, and Brian\u00a0Y Lim. 2019. Designing theory-driven user-centric explainable AI. In Proceedings of the 2019 CHI conference on human factors in computing systems. 1\u201315.",
      "doi": "10.1145/3290605.3300831"
    },
    {
      "text": "Ryan Weber. 2012. Review of The Rhetoric of Expertise. Rhetoric and Public Affairs 15, 1 (2012), 193\u2013196. https://www.jstor.org/stable/41955617 Publisher: Michigan State University Press.",
      "doi": ""
    },
    {
      "text": "Adrian Weller. 2019. Transparency: Motivations and Challenges. arXiv:1708.01870 [cs] (Aug. 2019). http://arxiv.org/abs/1708.01870 arXiv:1708.01870.",
      "doi": ""
    },
    {
      "text": "J. Wexler, M. Pushkarna, T. Bolukbasi, M. Wattenberg, F. Vi\u00e9gas, and J. Wilson. 2020. The What-If Tool: Interactive Probing of Machine Learning Models. IEEE Transactions on Visualization and Computer Graphics 26, 1(2020), 56\u201365.",
      "doi": ""
    },
    {
      "text": "Robin Williams, Wendy Faulkner, and James Fleck (Eds.). 1998. Exploring Expertise. Palgrave Macmillan UK, London. https://doi.org/10.1007/978-1-349-13693-3",
      "doi": ""
    },
    {
      "text": "Jacob\u00a0O Wobbrock and Julie\u00a0A Kientz. 2016. Research contributions in human-computer interaction. interactions 23, 3 (2016), 38\u201344.",
      "doi": "10.1145/2907069"
    },
    {
      "text": "Christine\u00a0T. Wolf. 2019. Explainability scenarios: towards scenario-based XAI design. In Proceedings of the 24th International Conference on Intelligent User Interfaces. ACM, Marina del Ray California, 252\u2013257. https://doi.org/10.1145/3301275.3302317",
      "doi": "10.1145/3301275.3302317"
    },
    {
      "text": "Yao Xie, Ge Gao, and Xiang\u00a0\u2019Anthony\u2019 Chen. 2019. Outlining the Design Space of Explainable Intelligent Systems for Medical Diagnosis. arxiv:1902.06019\u00a0[cs.HC]",
      "doi": ""
    },
    {
      "text": "Qian Yang, Aaron Steinfeld, Carolyn Ros\u00e9, and John Zimmerman. 2020. Re-examining Whether, Why, and How Human-AI Interaction Is Uniquely Difficult to Design. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems(CHI \u201920). Association for Computing Machinery, Honolulu, HI, USA, 1\u201313. https://doi.org/10.1145/3313831.3376301",
      "doi": "10.1145/3313831.3376301"
    },
    {
      "text": "Jill Yielder. 2001. Professional Expertise: A Model for Integration and Change. Thesis. ResearchSpace@Auckland. https://researchspace.auckland.ac.nz/handle/2292/2340Accepted: 2008-01-30T02:04:57Z.",
      "doi": ""
    },
    {
      "text": "Jill Yielder. 2004. An integrated model of professional expertise and its implications for higher education. International Journal of Lifelong Education 23, 1 (Jan. 2004), 60\u201380. https://doi.org/10.1080/0260137032000172060",
      "doi": ""
    },
    {
      "text": "Ming Yin, Jennifer Wortman\u00a0Vaughan, and Hanna Wallach. 2019. Understanding the Effect of Accuracy on Trust in Machine Learning Models. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems(CHI \u201919). Association for Computing Machinery, Glasgow, Scotland Uk, 1\u201312. https://doi.org/10.1145/3290605.3300509",
      "doi": "10.1145/3290605.3300509"
    },
    {
      "text": "Rulei Yu and Lei Shi. 2018. A user-based taxonomy for deep learning visualization. Visual Informatics 2, 3 (Sept. 2018), 147\u2013154. https://doi.org/10.1016/j.visinf.2018.09.001",
      "doi": ""
    },
    {
      "text": "Tal\u00a0Z Zarsky. 2013. Transparent predictions. U. Ill. L. Rev. (2013), 1503.",
      "doi": ""
    },
    {
      "text": "Matthew\u00a0D. Zeiler and Rob Fergus. 2014. Visualizing and Understanding Convolutional Networks. In Computer Vision \u2013 ECCV 2014, David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars (Eds.). Vol.\u00a08689. Springer International Publishing, Cham, 818\u2013833. https://doi.org/10.1007/978-3-319-10590-1_53Series Title: Lecture Notes in Computer Science.",
      "doi": ""
    },
    {
      "text": "Quanshi Zhang, Ying\u00a0Nian Wu, and Song-Chun Zhu. 2018. Interpretable Convolutional Neural Networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
      "doi": ""
    },
    {
      "text": "Yunfeng Zhang, Q\u00a0Vera Liao, and Rachel\u00a0KE Bellamy. 2020. Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. 295\u2013305.",
      "doi": "10.1145/3351095.3372852"
    },
    {
      "text": "Ruijing Zhao, Izak Benbasat, and Hasan Cavusoglu. 2019. Transparency in Advice-Giving Systems: A Framework and a Research Model for Transparency Provision.. In IUI Workshops.",
      "doi": ""
    },
    {
      "text": "Jan\u00a0Ruben Zilke, Eneldo\u00a0Loza Menc\u00eda, and Frederik Janssen. 2016. Deepred\u2013rule extraction from deep neural networks. In International Conference on Discovery Science. Springer, 457\u2013473.",
      "doi": ""
    }
  ]
}