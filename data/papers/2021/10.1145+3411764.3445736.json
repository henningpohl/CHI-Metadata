{
  "doi": "10.1145/3411764.3445736",
  "title": "Data-Centric Explanations: Explaining Training Data of Machine Learning Systems to Promote Transparency",
  "published": "2021-05-07",
  "proctitle": "CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems",
  "pages": "1-13",
  "year": 2021,
  "badges": [],
  "abstract": "Training datasets fundamentally impact the performance of machine learning (ML) systems. Any biases introduced during training (implicit or explicit) are often reflected in the system's behaviors leading to questions about fairness and loss of trust in the system. Yet, information on training data is rarely communicated to stakeholders. In this work, we explore the concept of data-centric explanations for ML systems that describe the training data to end-users. Through a formative study, we investigate the potential utility of such an approach, including the information about training data that participants find most compelling. In a second study, we investigate reactions to our explanations across four different system scenarios. Our results suggest that data-centric explanations have the potential to impact how users judge the trustworthiness of a system and to assist users in assessing fairness. We discuss the implications of our findings for designing explanations to support users\u2019 perceptions of ML systems.",
  "authors": [
    {
      "name": "Ariful Islam Anik",
      "institution": "Computer Science Department, University of Manitoba, Canada",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659701669",
      "orcid": "missing"
    },
    {
      "name": "Andrea Bunt",
      "institution": "Computer Science Department, University of Manitoba, Canada",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81100232130",
      "orcid": "0000-0002-7374-7629"
    }
  ],
  "references": [
    {
      "text": "Liliana Ardissono, Anna Goy, Giovanna Petrone, Marino Segnan, and Pietro Torasso. 2003. Intrigue: Personalized Recommendation of Tourist Attractions. Applied Artificial Intelligence: Special Issue on Artificial Intelligence for Cultural Heritage and Digital Libraries 17, 8\u20139: 687\u2013714.",
      "doi": ""
    },
    {
      "text": "M. Arnold, D. Piorkowski, D. Reimer, J. Richards, J. Tsay, K. R. Varshney, R. K.E. Bellamy, M. Hind, S. Houde, S. Mehta, A. Mojsilovic, R. Nair, K. Natesan Ramamurthy, and A. Olteanu. 2019. FactSheets: Increasing trust in AI services through supplier's declarations of conformity. IBM Journal of Research and Development 63, 4\u20135. https://doi.org/10.1147/JRD.2019.2942288",
      "doi": ""
    },
    {
      "text": "Solon Barocas and Andrew Selbst. 2016. Big Data's Disparate Impact. California Law Review 104, 3: 671. https://doi.org/10.15779/Z38BG31",
      "doi": ""
    },
    {
      "text": "Claudio Biancalana, Fabio Gasparetti, Alessandro Micarelli, Alfonso Miola, and Giuseppe Sansonetti. 2011. Context-aware movie recommendation based on signal processing and machine learning. Proceedings of the 2nd Challenge on Context-Aware Movie Recommendation: 5\u201310. https://doi.org/10.1145/2096112.2096114",
      "doi": "10.1145/2096112.2096114"
    },
    {
      "text": "Reuben Binns, Max Van Kleek, Michael Veale, Ulrik Lyngs, Jun Zhao, and Nigel Shadbolt. 2018. \u201cIt's reducing a human being to a percentage\u201d; perceptions of justice in algorithmic decisions. Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems 2018-April: 1\u201314. https://doi.org/10.1145/3173574.3173951",
      "doi": "10.1145/3173574.3173951"
    },
    {
      "text": "Tolga Bolukbasi, Kai Wei Chang, James Zou, Venkatesh Saligrama, and Adam Kalai. 2016. Man is to computer programmer as woman is to homemaker? Debiasing word embeddings. In Advances in Neural Information Processing Systems, 4356\u20134364.",
      "doi": ""
    },
    {
      "text": "Jeremy Boy, Ronald A. Rensink, Enrico Bertini, and Jean Daniel Fekete. 2014. A principled way of assessing visualization literacy. IEEE Transactions on Visualization and Computer Graphics 20, 12: 1963\u20131972. https://doi.org/10.1109/TVCG.2014.2346984",
      "doi": ""
    },
    {
      "text": "Andrea Bunt, Matthew Lount, and Catherine Lauzon. 2012. Are explanations always important? A study of deployed, low-cost intelligent interactive systems. Proceedings of the 2012 ACM International Conference on Intelligent User Interfaces: 169\u2013178. https://doi.org/10.1145/2166966.2166996",
      "doi": "10.1145/2166966.2166996"
    },
    {
      "text": "Andrea Bunt, Joanna McGrenere, and Cristina Conati. 2007. Understanding the Utility of Rationale in a Mixed-Initiative System for GUI Customization. In User Modeling 2007, 147\u2013156.",
      "doi": ""
    },
    {
      "text": "Joy Buolamwini and Timnit Gebru. 2018. Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classificatio. Proceedings of the 1st Conference on Fairness, Accountability and Transparency 10: 1889\u20131896. https://doi.org/10.2147/OTT.S126905",
      "doi": ""
    },
    {
      "text": "Jenna Burrell. 2016. How the machine \u2018thinks\u2019: Understanding opacity in machine learning algorithms. Big Data & Society 3, 1: 2053951715622512. https://doi.org/10.1177/2053951715622512",
      "doi": ""
    },
    {
      "text": "Niklas Bussmann, Paolo Giudici, Dimitri Marinelli, and Jochen Papenbrock. 2020. Explainable Machine Learning in Credit Risk Management. Computational Economics: 1\u201321. https://doi.org/10.1007/s10614-020-10042-0",
      "doi": ""
    },
    {
      "text": "John T. Cacioppo, Richard E. Petty, and Chuan Feng Kao. 1984. The Efficient Assessment of Need for Cognition. Journal of Personality Assessment 48, 3: 306\u2013307. https://doi.org/10.1207/s15327752jpa4803_13",
      "doi": ""
    },
    {
      "text": "Carrie J. Cai, Jonas Jongejan, and Jess Holbrook. 2019. The effects of example-based explanations in a machine learning interface. Proceedings of the 24th International Conference on Intelligent User Interfaces: 258\u2013262. https://doi.org/10.1145/3301275.3302289",
      "doi": "10.1145/3301275.3302289"
    },
    {
      "text": "Toon Calders and Indr\u0117 \u017dliobait\u0117. 2013. Why unbiased computational processes can lead to discriminative decision procedures. Studies in Applied Philosophy, Epistemology and Rational Ethics 3: 43\u201357. https://doi.org/10.1007/978-3-642-30487-3_3",
      "doi": ""
    },
    {
      "text": "Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie Elhadad. 2015. Intelligible Models for HealthCare. Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD \u201915: 1721\u20131730. https://doi.org/10.1145/2783258.2788613",
      "doi": "10.1145/2783258.2788613"
    },
    {
      "text": "Aaron Chalfin, Oren Danieli, Andrew Hillis, Zubin Jelveh, Michael Luca, Jens Ludwig, and Sendhil Mullainathan. 2016. Productivity and selection of human capital with machine learning. American Economic Review 106, 5: 124\u2013127. https://doi.org/10.1257/aer.p20161029",
      "doi": ""
    },
    {
      "text": "Zhengping Che, Sanjay Purushotham, Robinder Khemani, and Yan Liu. 2016. Interpretable Deep Models for ICU Outcome Prediction. AMIA ... Annual Symposium proceedings. AMIA Symposium 2016: 371\u2013380.",
      "doi": ""
    },
    {
      "text": "Lin Chen, Rui Li, Yige Liu, Ruixuan Zhang, and Diane Myung Kyung Woodbridge. 2018. Machine learning-based product recommendation using Apache Spark. 2017 IEEE SmartWorld Ubiquitous Intelligence and Computing, Advanced and Trusted Computed, Scalable Computing and Communications, Cloud and Big Data Computing, Internet of People and Smart City Innovation, SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI 2017 - : 1\u20136. https://doi.org/10.1109/UIC-ATC.2017.8397470",
      "doi": ""
    },
    {
      "text": "Hao Fei Cheng, Ruotong Wang, Zheng Zhang, Fiona O'Connell, Terrance Gray, F. Maxwell Harper, and Haiyi Zhu. 2019. Explaining decision-making algorithms through UI: Strategies to help non-expert stakeholders. Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems: 1\u201312. https://doi.org/10.1145/3290605.3300789",
      "doi": "10.1145/3290605.3300789"
    },
    {
      "text": "Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen Tau Yih, Yejin Choi, Percy Liang, and Luke Zettlemoyer. 2020. QUAC: Question answering in context. Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018: 2174\u20132184. https://doi.org/10.18653/v1/d18-1241",
      "doi": ""
    },
    {
      "text": "Alexandra Chouldechova and Aaron Roth. 2018. The Frontiers of Fairness in Machine Learning. 1\u201313. Retrieved from http://arxiv.org/abs/1810.08810",
      "doi": ""
    },
    {
      "text": "Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. 2018. VoxceleB2: Deep speaker recognition. Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH 2018-Septe, ii: 1086\u20131090. https://doi.org/10.21437/Interspeech.2018-1929",
      "doi": ""
    },
    {
      "text": "Danielle Keats Citron and Frank Pasquale. 2014. The scored society: Due process for automated predictions. Washington Law Review 89, 1: 1\u201333.",
      "doi": ""
    },
    {
      "text": "Jason A. Colquitt and Jessica B. Rodell. 2015. Measuring Justice and Fairness. The Oxford Handbook of Justice in the Workplace: 187\u2013202. https://doi.org/10.1093/oxfordhb/9780199981410.013.8",
      "doi": ""
    },
    {
      "text": "Sam Corbett-Davies, Emma Pierson, Avi Feller, and Sharad Goel. 2016. A computer program used for bail and sentencing decisions was labeled biased against blacks . It\u2019 s actually not that clear. The Washington Post: 1\u20137.",
      "doi": ""
    },
    {
      "text": "Juliet Corbin and Anselm Strauss. 2008. Strategies for qualitative data analysis. Basics of Qualitative Research. Techniques and procedures for developing grounded theory 3.",
      "doi": ""
    },
    {
      "text": "Henriette Cramer, Vanessa Evers, Satyan Ramlal, Maarten Van Someren, Lloyd Rutledge, Natalia Stash, Lora Aroyo, and Bob Wielinga. 2008. The effects of transparency on trust in and acceptance of a content-based art recommender. In User Modeling and User-Adapted Interaction, 455\u2013496. https://doi.org/10.1007/s11257-008-9051-3",
      "doi": ""
    },
    {
      "text": "Anupam Datta. 2017. Did Artificial Intelligence Deny You Credit? The Conversation. Retrieved January 20, 2019 from http://theconversation.com/did-artificial-intelligence-deny-you-credit-73259",
      "doi": ""
    },
    {
      "text": "Anupam Datta, Shayak Sen, and Yair Zick. 2017. Algorithmic Transparency via Quantitative Input Influence. Transparent Data Mining for Big and Small Data: 71\u201394. https://doi.org/10.1007/978-3-319-54024-5_4",
      "doi": ""
    },
    {
      "text": "Nicholas Diakopoulos. 2015. Algorithmic Accountability: Journalistic investigation of computational power structures. Digital Journalism 3, 3: 398\u2013415. https://doi.org/10.1080/21670811.2014.976411",
      "doi": ""
    },
    {
      "text": "Berkeley J. Dietvorst, Joseph P. Simmons, and Cade Massey. 2015. Algorithm aversion: People erroneously avoid algorithms after seeing them err. Journal of Experimental Psychology: General 144, 1: 114\u2013126. https://doi.org/10.1037/xge0000033",
      "doi": ""
    },
    {
      "text": "Jonathan Dodge, Q. Vera Liao, Yunfeng Zhang, Rachel K.E. Bellamy, and Casey Dugan. 2019. Explaining models: An empirical study of how explanations impact fairness judgment. Proceedings of the 24th International Conference on Intelligent User Interfaces: 275\u2013285. https://doi.org/10.1145/3301275.3302310",
      "doi": "10.1145/3301275.3302310"
    },
    {
      "text": "Tim Donkers, Benedikt Loepp, and J\u00fcrgen Ziegler. 2018. Explaining recommendations by means of user reviews. In CEUR Workshop Proceedings.",
      "doi": ""
    },
    {
      "text": "Donal Doyle, Alexey Tsymbal, and P\u00e1draig Cunningham. 2003. A Review of Explanation and Explanation in Case-Based Reasoning. Dublin, Trinity College Dublin, Department of Computer Science, TCD-CS-2003-41: 41.",
      "doi": ""
    },
    {
      "text": "Mengnan Du, Ninghao Liu, and Xia Hu. 2020. Techniques for interpretable machine learning. Communications of the ACM 63, 1: 68\u201377. https://doi.org/10.1145/3359786",
      "doi": "10.1145/3359786"
    },
    {
      "text": "Mary T. Dzindolet, Linda G. Pierce, Hall P. Beck, and Lloyd A. Dawe. 2002. The perceived utility of human and automated aids in a visual detection task. Human Factors 44, 1: 79\u201394. https://doi.org/10.1518/0018720024494856",
      "doi": ""
    },
    {
      "text": "Malin Eiband, Hanna Schneider, Mark Bilandzic, Julian Fazekas-Con, Mareike Haug, and Heinrich Hussmann. 2018. Bringing transparency design into practice. International Conference on Intelligent User Interfaces, Proceedings IUI: 211\u2013223. https://doi.org/10.1145/3172944.3172961",
      "doi": "10.1145/3172944.3172961"
    },
    {
      "text": "Hugo Jair Escalante, Isabelle Guyon, Sergio Escalera, Julio Jacques, Meysam Madadi, Xavier Baro, Stephane Ayache, Evelyne Viegas, Yagmur Gucluturk, Umut Guclu, Marcel A.J. Van Gerven, and Rob Van Lier. 2017. Design of an explainable machine learning challenge for video interviews. Proceedings of the International Joint Conference on Neural Networks 2017-May: 3688\u20133695. https://doi.org/10.1109/IJCNN.2017.7966320",
      "doi": ""
    },
    {
      "text": "Andre Esteva, Brett Kuprel, Roberto A. Novoa, Justin Ko, Susan M. Swetter, Helen M. Blau, and Sebastian Thrun. 2017. Dermatologist-level classification of skin cancer with deep neural networks. Nature 542, 7639: 115\u2013118. https://doi.org/10.1038/nature21056",
      "doi": ""
    },
    {
      "text": "Ea Eyjolfsdottir, Gaurangi Tilak, and Nan Li. 2010. MovieGEN: A Movie Recommendation System. Computer Science Department, \u2026. Retrieved from http://www.cs.ucsb.edu/\u223cnanli/projects/CS265-MovieGEN.pdf",
      "doi": ""
    },
    {
      "text": "Gerald Fahner. 2018. Developing Transparent Credit Risk Scorecards More Effectively: An Explainable Artificial Intelligence Approach. c: 7\u201314. Retrieved from https://www.thinkmind.org/index.php?view=article&articleid=data_analytics_2018_1_30_60077",
      "doi": ""
    },
    {
      "text": "Alex Fefegha. 2019. Racial Bias and Gender Bias Examples in AI systems So here it goes: Racial Bias. 1\u201314. Retrieved January 17, 2019 from https://medium.com/thoughts-and-reflections/racial-bias-and-gender-bias-examples-in-ai-systems-7211e4c166a1",
      "doi": ""
    },
    {
      "text": "Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daum\u00e9 Iii, and Kate Crawford. 2018. Datasheets for datasets. In 5th Workshop on Fairness, Accountability, and Transparency in Machine Learning. Retrieved from http://arxiv.org/abs/1803.09010",
      "doi": ""
    },
    {
      "text": "Rory Mc Grath, Luca Costabello, Chan Le Van, Paul Sweeney, Farbod Kamiab, Zhao Shen, and Freddy Lecue. 2018. Interpretable Credit Application Predictions With Counterfactual Explanations. 1\u20139. Retrieved from http://arxiv.org/abs/1811.05245",
      "doi": ""
    },
    {
      "text": "Ben Green and Lily Hu. 2018. The Myth in the Methodology: Towards a Recontextualization of Fairness in Machine Learning. Proceedings of the machine learning: the debates workshop.",
      "doi": ""
    },
    {
      "text": "Nina Grgic-Hlaca, Elissa M. Redmiles, Krishna P. Gummadi, and Adrian Weller. 2018. Human perceptions of fairness in algorithmic decision making: A case study of criminal risk prediction. The Web Conference 2018 - Proceedings of the World Wide Web Conference, WWW 2018: 903\u2013912. https://doi.org/10.1145/3178876.3186138",
      "doi": "10.1145/3178876.3186138"
    },
    {
      "text": "Moritz Hardt, Eric Price, and Nathan Srebro. 2016. Equality of opportunity in supervised learning. Advances in Neural Information Processing Systems: 3323\u20133331.",
      "doi": ""
    },
    {
      "text": "J. L. Herlocker, J. A. Konstan, and J. Riedl. 2000. Explaining collaborative filtering recommendations. In Proceedings of the ACM Conference on Computer Supported Cooperative Work, 241\u2013250. https://doi.org/10.1145/358916.358995",
      "doi": ""
    },
    {
      "text": "Fred Hohman, Andrew Head, Rich Caruana, Robert DeLine, and Steven M. Drucker. 2019. Gamut: A design probe to understand how data scientists understand machine learning models. Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems: 1\u201313. https://doi.org/10.1145/3290605.3300809",
      "doi": ""
    },
    {
      "text": "Kenneth Holstein, Jennifer Wortman Vaughan, Hal Daum\u00e9, Miroslav Dud\u00edk, and Hanna Wallach. 2019. Improving fairness in machine learning systems: What do industry practitioners need? Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems: 1\u201316. https://doi.org/10.1145/3290605.3300830",
      "doi": "10.1145/3290605.3300830"
    },
    {
      "text": "Andreas Holzinger, Bernd Malle, Peter Kieseberg, Peter M. Roth, Heimo M\u00fcller, Robert Reihs, and Kurt Zatloukal. 2017. Towards the Augmented Pathologist: Challenges of Explainable-AI in Digital Pathology. 1\u201334. Retrieved from http://arxiv.org/abs/1712.06657",
      "doi": ""
    },
    {
      "text": "Gary B Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller. 2007. Labeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments.",
      "doi": ""
    },
    {
      "text": "J. A.Colquitt. 2001. On the dimensionality of organizational justice: A construct validation of a measure. Journal of applied psychology 68, 386\u2013399.",
      "doi": ""
    },
    {
      "text": "Jiun-Yin Jian, Ann M Bisantz, Colin G Drury, and James Llinas. 1996. United States Air Force Research Laboratory Foundations for an Empirically Determined Scale of Trust in Automated Systems. International Journal of Cognitive Ergonomics 4, 1: 53\u201371.",
      "doi": ""
    },
    {
      "text": "Amir E Khandani, Adlar J Kim, and Andrew W Lo. 2010. Consumer credit-risk models via machine-learning algorithms. Journal of Banking & Finance 34, 11: 2767\u20132787. https://doi.org/10.1016/j.jbankfin.2010.06.001",
      "doi": ""
    },
    {
      "text": "Lauren Kirchner, Surya Mattu, Jeff Larson, and Julia Angwin. 2016. Machine Bias. Propublica 23: 1\u201326. Retrieved from https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing",
      "doi": ""
    },
    {
      "text": "Rene F. Kizilcec. 2016. How much information? Effects of transparency on trust in an algorithmic interface. Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems: 2390\u20132395. https://doi.org/10.1145/2858036.2858402",
      "doi": "10.1145/2858036.2858402"
    },
    {
      "text": "Rafal Kocielnik, Saleema Amershi, and Paul N. Bennett. 2019. Will you accept an imperfect AI? Exploring Designs for Adjusting End-user Expectations of AI Systems. Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems: 1\u201314. https://doi.org/10.1145/3290605.3300641",
      "doi": ""
    },
    {
      "text": "Josua Krause, Adam Perer, and Kenney Ng. 2016. Interacting with predictions: Visual inspection of black-box machine learning models. Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems: 5686\u20135697. https://doi.org/10.1145/2858036.2858529",
      "doi": "10.1145/2858036.2858529"
    },
    {
      "text": "Todd Kulesza, Margaret Burnett, Weng Keen Wong, and Simone Stumpf. 2015. Principles of Explanatory Debugging to personalize interactive machine learning. Proceedings of the 20th International Conference on Intelligent User Interfaces: 126\u2013137. https://doi.org/10.1145/2678025.2701399",
      "doi": "10.1145/2678025.2701399"
    },
    {
      "text": "Todd Kulesza, Simone Stumpf, Margaret Burnett, and Irwin Kwan. 2012. Tell me more? the effects of mental model soundness on personalizing an intelligent agent. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems: 1\u201310. https://doi.org/10.1145/2207676.2207678",
      "doi": "10.1145/2207676.2207678"
    },
    {
      "text": "Todd Kulesza, Simone Stumpf, Margaret Burnett, Sherry Yang, Irwin Kwan, and Weng Keen Wong. 2013. Too much, too little, or just right? Ways explanations impact end users\u2019 mental models. Proceedings of IEEE Symposium on Visual Languages and Human-Centric Computing, VL/HCC: 3\u201310. https://doi.org/10.1109/VLHCC.2013.6645235",
      "doi": ""
    },
    {
      "text": "Johannes Kunkel, Tim Donkers, Lisa Michael, Catalin Mihai Barbu, and J\u00fcrgen Ziegler. 2019. Let me explain: Impact of personal and impersonal explanations on trust in recommender systems. Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems: 1\u201312. https://doi.org/10.1145/3290605.3300717",
      "doi": "10.1145/3290605.3300717"
    },
    {
      "text": "Paul B. de Laat. 2018. Algorithmic Decision-Making Based on Machine Learning from Big Data: Can Transparency Restore Accountability? Philosophy and Technology 31, 4: 525\u2013541. https://doi.org/10.1007/s13347-017-0293-z",
      "doi": ""
    },
    {
      "text": "An\u00edsio Lacerda, Marco Cristo, Marcos Andr\u00e9 Gon\u00e7alves, Weiguo Fan, Nivio Ziviani, and Berthier Ribeiro-Neto. 2006. Learning to advertise. In Proceedings of the Twenty-Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 549\u2013556. https://doi.org/10.1145/1148170.1148265",
      "doi": "10.1145/1148170.1148265"
    },
    {
      "text": "Jeff Larson, Surya Mattu, Lauren Kirchner, and Julia Angwin. 2020. How We Analyzed the COMPAS Recidivism Algorithm. ProPublica. Retrieved from https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm",
      "doi": ""
    },
    {
      "text": "Min Kyung Lee. 2018. Understanding perception of algorithmic decisions: Fairness, trust, and emotion in response to algorithmic management. Big Data and Society 5, 1: 1\u201316. https://doi.org/10.1177/2053951718756684",
      "doi": ""
    },
    {
      "text": "Bruno Lepri, Nuria Oliver, Emmanuel Letouz\u00e9, Alex Pentland, and Patrick Vinck. 2018. Fair, Transparent, and Accountable Algorithmic Decision-making Processes. Philosophy & Technology 31, 4: 611\u2013627. https://doi.org/10.1007/s13347-017-0279-x",
      "doi": ""
    },
    {
      "text": "Benjamin Letham, Cynthia Rudin, Tyler H. McCormick, and David Madigan. 2015. Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model. Annals of Applied Statistics 9, 3: 1350\u20131371. https://doi.org/10.1214/15-AOAS848",
      "doi": ""
    },
    {
      "text": "Q. Vera Liao, Daniel Gruen, and Sarah Miller. 2020. Questioning the AI: Informing Design Practices for Explainable AI User Experiences. Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3313831.3376590",
      "doi": "10.1145/3313831.3376590"
    },
    {
      "text": "Cynthia C S Liem, Markus Langer, Andrew Demetriou, Annemarie M F Hiemstra, Achmadnoer Sukma Wicaksana, Marise Ph. Born, and Cornelius J K\u00f6nig. 2018. Psychology Meets Machine Learning: Interdisciplinary Perspectives on Algorithmic Job Candidate Screening. In Explainable and Interpretable Models in Computer Vision and Machine Learning. Springer International Publishing, Cham, 197\u2013253. https://doi.org/10.1007/978-3-319-98131-4_9",
      "doi": ""
    },
    {
      "text": "Brian Y. Lim and Anind K. Dey. 2009. Assessing demand for intelligibility in context-aware applications. UbiComp 2009: Ubiquitous Computing: 195. https://doi.org/10.1145/1620545.1620576",
      "doi": ""
    },
    {
      "text": "Brian Y. Lim, Anind K. Dey, and Daniel Avrahami. 2009. Why and why not explanations improve the intelligibility of context-aware intelligent systems. In Proceedings of the 27th international conference on Human factors in computing systems - CHI 09, 2119. https://doi.org/10.1145/1518701.1519023",
      "doi": "10.1145/1518701.1519023"
    },
    {
      "text": "Zachary C. Lipton. 2018. The Mythos of Model Interpretability. Queue 16, 3: 31\u201357. https://doi.org/10.1145/3236386.3241340",
      "doi": "10.1145/3236386.3241340"
    },
    {
      "text": "Gideon Mann and Cathy O'Neil. 2016. Hiring Algorithms Are Not Neutral. Harvard Business Review. Retrieved August 4, 2020 from https://hbr.org/2016/12/hiring-algorithms-are-not-neutral",
      "doi": ""
    },
    {
      "text": "Martijn Millecamp, Cristina Conati, Nyi Nyi Htun, and Katrien Verbert. 2019. To explain or not to explain: The effects of personal characteristics when explaining music recommendations. Proceedings of the 24th International Conference on Intelligent User Interfaces: 397\u2013407. https://doi.org/10.1145/3301275.3302313",
      "doi": "10.1145/3301275.3302313"
    },
    {
      "text": "Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. 2019. Model cards for model reporting. FAT* 2019 - Proceedings of the 2019 Conference on Fairness, Accountability, and Transparency, Figure 2: 220\u2013229. https://doi.org/10.1145/3287560.3287596",
      "doi": ""
    },
    {
      "text": "Conor Nugent and P\u00e1draig Cunningham. 2005. A case-based explanation system for black-box systems. Artificial Intelligence Review 24, 2: 163\u2013178. https://doi.org/10.1007/s10462-005-4609-5",
      "doi": "10.1007/s10462-005-4609-5"
    },
    {
      "text": "Alexandra Olteanu, Carlos Castillo, Fernando Diaz, and Emre Kiciman. 2019. Social data: Biases, methodological pitfalls, and ethical boundaries. Frontiers in Big Data 2: 13.",
      "doi": ""
    },
    {
      "text": "Frank Pasquale. 2015. The Black Box Society. Harvard University Press. https://doi.org/10.4159/harvard.9780674736061",
      "doi": ""
    },
    {
      "text": "Dino Pedreschi, Fosca Giannotti, Riccardo Guidotti, Anna Monreale, Luca Pappalardo, Salvatore Ruggieri, and Franco Turini. 2018. Open the Black Box Data-Driven Explanation of Black Box Decision Systems. 1, 1: 1\u201315. Retrieved from http://arxiv.org/abs/1806.09936",
      "doi": ""
    },
    {
      "text": "Forough Poursabzi-Sangdeh, Daniel G. Goldstein, Jake M. Hofman, Jennifer Wortman Vaughan, and Hanna Wallach. 2018. Manipulating and Measuring Model Interpretability. Retrieved from http://arxiv.org/abs/1802.07810",
      "doi": ""
    },
    {
      "text": "Pearl Pu and Li Chen. 2006. Trust building with explanation interfaces. Proceedings of the 11th International Conference on Intelligent User Interfaces 2006: 93\u2013100. https://doi.org/10.1145/1111449.1111475",
      "doi": "10.1145/1111449.1111475"
    },
    {
      "text": "Emilee Rader, Kelley Cotter, and Janghee Cho. 2018. Explanations as Mechanisms for Supporting Algorithmic Transparency. Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems - CHI \u201918: 1\u201313. https://doi.org/10.1145/3173574.3173677",
      "doi": "10.1145/3173574.3173677"
    },
    {
      "text": "Ashwin Ram. 1993. AQUA: Questions that Drive the Explanation Process. Georgia Institute of Technology.",
      "doi": ""
    },
    {
      "text": "Marco Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. \u201cWhy Should I Trust You?\u201d: Explaining the Predictions of Any Classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, 97\u2013101. https://doi.org/10.18653/v1/n16-3020",
      "doi": "10.1145/2939672.2939778"
    },
    {
      "text": "Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. Model-Agnostic Interpretability of Machine Learning. https://doi.org/10.1145/2858036.2858529",
      "doi": ""
    },
    {
      "text": "Andrew Slavin Ross, Michael C. Hughes, and Finale Doshi-Velez. 2017. Right for the right reasons: Training differentiable models by constraining their explanations. Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, {IJCAI-17} 0: 2662\u20132670. https://doi.org/10.24963/ijcai.2017/371",
      "doi": ""
    },
    {
      "text": "J. B. Rotter. 1966. Generalized expectancies for internal versus external control of reinforcement. Psychological monographs 80, 1: 1\u201328. https://doi.org/10.1037/h0092976",
      "doi": ""
    },
    {
      "text": "Wojciech Samek, Alexander Binder, Gr\u00e9goire Montavon, Sebastian Lapuschkin, and Klaus Robert M\u00fcller. 2017. Evaluating the visualization of what a deep neural network has learned. IEEE Transactions on Neural Networks and Learning Systems 28, 11: 2660\u20132673. https://doi.org/10.1109/TNNLS.2016.2599820",
      "doi": ""
    },
    {
      "text": "Isma\u00efla Seck, Khouloud Dahmane, Pierre Duthon, and Ga\u00eblle Loosli. 2018. Baselines and a datasheet for the Cerema AWP dataset. arXiv preprint arXiv:1806.04016. Retrieved from http://arxiv.org/abs/1806.04016",
      "doi": ""
    },
    {
      "text": "Mark Sendak, Madeleine Clare Elish, Michael Gao, Joseph Futoma, William Ratliff, Marshall Nichols, Armando Bedoya, Suresh Balu, and Cara O'Brien. 2020. \u201cThe human body is a black box\u201d: Supporting clinical decision-making with deep learning. FAT* 2020 - Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency: 99\u2013109. https://doi.org/10.1145/3351095.3372827",
      "doi": "10.1145/3351095.3372827"
    },
    {
      "text": "Eduardo Soares and Plamen Angelov. 2019. Fair-by-design explainable models for prediction of recidivism. 3\u20137. Retrieved from http://arxiv.org/abs/1910.02043",
      "doi": ""
    },
    {
      "text": "Megha Srivastava, Hoda Heidari, and Andreas Krause. 2019. Mathematical notions vs. Human perception of fairness: A descriptive approach to fairness for machine learning. Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining: 2459\u20132468. https://doi.org/10.1145/3292500.3330664",
      "doi": "10.1145/3292500.3330664"
    },
    {
      "text": "Paolo Tamagnini, Josua Krause, Aritra Dasgupta, and Enrico Bertini. 2017. Interpreting black-box classifiers using instance-level visual explanations. Proceedings of the 2nd Workshop on Human-In-the-Loop Data Analytics, HILDA 2017: 1\u20136. https://doi.org/10.1145/3077257.3077260",
      "doi": "10.1145/3077257.3077260"
    },
    {
      "text": "Caroline Wang, Bin Han, Bhrij Patel, Feroze Mohideen, and Cynthia Rudin. 2020. In Pursuit of Interpretable, Fair and Accurate Machine Learning for Criminal Recidivism Prediction. 1\u201358. Retrieved from http://arxiv.org/abs/2005.04176",
      "doi": ""
    },
    {
      "text": "Yuanyuan Wang, Stephen Chi Fai Chan, and Grace Ngai. 2012. Applicability of demographic recommender system to tourist attractions: A case study on TripAdvisor. Proceedings of the 2012 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology Workshops, WI-IAT 2012: 97\u2013101. https://doi.org/10.1109/WI-IAT.2012.133",
      "doi": "10.1109/WI-IAT.2012.133"
    },
    {
      "text": "Semih Yagcioglu, Aykut Erdem, Erkut Erdem, and Nazli Ikizler-Cinbis. 2020. RecipeQA: A challenge dataset for multimodal comprehension of cooking recipes. Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018: 1358\u20131368. https://doi.org/10.18653/v1/d18-1166",
      "doi": ""
    },
    {
      "text": "L. Richard Ye and Paul E. Johnson. 1995. The impact of explanation facilities on user acceptance of expert systems advice. MIS Quarterly: Management Information Systems 19, 2: 157\u2013172. https://doi.org/10.2307/249686",
      "doi": "10.2307/249686"
    },
    {
      "text": "Ming Yin, Jennifer Wortman Vaughan, and Hanna Wallach. 2019. Understanding the effect of accuracy on trust in machine learning models. Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems: 1\u201312. https://doi.org/10.1145/3290605.3300509",
      "doi": "10.1145/3290605.3300509"
    },
    {
      "text": "Jiaming Zeng, Berk Ustun, and Cynthia Rudin. 2017. Interpretable classification models for recidivism prediction. Journal of the Royal Statistical Society. Series A: Statistics in Society 180, 3: 689\u2013722. https://doi.org/10.1111/rssa.12227",
      "doi": ""
    },
    {
      "text": "Yong Zhang, Hongming Zhou, Nganmeng Tan, Saeed Bagheri, and Meng Joo Er. 2017. Targeted Advertising Based on Browsing History. CoRR abs/1711.0. Retrieved from http://arxiv.org/abs/1711.04498",
      "doi": ""
    },
    {
      "text": "Yunfeng Zhang, Q. Vera Liao, and Rachel K.E. Bellamy. 2020. Efect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making. Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency: 295\u2013305. https://doi.org/10.1145/3351095.3372852",
      "doi": ""
    },
    {
      "text": "Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. 2017. Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, 2979\u20132989. https://doi.org/10.18653/v1/D17-1323",
      "doi": ""
    },
    {
      "text": "Graduate Admission 2. Retrieved September 2, 2020 from https://kaggle.com/mohansacharya/graduate-admissions",
      "doi": ""
    },
    {
      "text": "Qualtrics. Qualtrics. Retrieved August 5, 2020 from https://www.qualtrics.com/core-xm/survey-software/",
      "doi": ""
    }
  ]
}