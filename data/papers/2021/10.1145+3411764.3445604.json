{
  "doi": "10.1145/3411764.3445604",
  "title": "Towards Fairness in Practice: A Practitioner-Oriented Rubric for Evaluating Fair ML Toolkits",
  "published": "2021-05-07",
  "proctitle": "CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems",
  "pages": "1-13",
  "year": 2021,
  "badges": [],
  "abstract": "In order to support fairness-forward thinking by machine learning (ML) practitioners, fairness researchers have created toolkits that aim to transform state-of-the-art research contributions into easily-accessible APIs. Despite these efforts, recent research indicates a disconnect between the needs of practitioners and the tools offered by fairness research. By engaging 20 ML practitioners in a simulated scenario in which they utilize fairness toolkits to make critical decisions, this work aims to utilize practitioner feedback to inform recommendations for the design and creation of fair ML toolkits. Through the use of survey and interview data, our results indicate that though fair ML toolkits are incredibly impactful on users\u2019 decision-making, there is much to be desired in the design and demonstration of fairness results. To support the future development and evaluation of toolkits, this work offers a rubric that can be used to identify critical components of Fair ML toolkits.",
  "tags": [
    "machine learning fairness",
    "user-centric evaluation",
    "ML",
    "AI",
    "algorithmic bias",
    "ethics",
    "fairness"
  ],
  "authors": [
    {
      "name": "Brianna Richardson",
      "institution": "Spotify, United States",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659685889",
      "orcid": "missing"
    },
    {
      "name": "Jean Garcia-Gathright",
      "institution": "Spotify, United States",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99658713032",
      "orcid": "missing"
    },
    {
      "name": "Samuel F. Way",
      "institution": "Spotify, United States",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659702177",
      "orcid": "missing"
    },
    {
      "name": "Jennifer Thom",
      "institution": "Spotify, United States",
      "img": "/do/10.1145/contrib-81100334243/rel-imgonly/jenn_badge__2_.jpeg",
      "acmid": "81100334243",
      "orcid": "missing"
    },
    {
      "name": "Henriette Cramer",
      "institution": "Spotify, United States",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81328487965",
      "orcid": "missing"
    }
  ],
  "references": [
    {
      "text": "2020. TensorFlow Extended (TFX) | ML Production Pipelines. https://www.tensorflow.org/tfx",
      "doi": ""
    },
    {
      "text": "2020. Tensorflow\u2019s Fairness Evaluation and Visualization Toolkit. https://github.com/tensorflow/fairness-indicators",
      "doi": ""
    },
    {
      "text": "ACM. 2020. ACM FAccT. https://facctconference.org/",
      "doi": ""
    },
    {
      "text": "Aniya Aggarwal, Seema Nagar, and Diptikalyan Saha. 2019. Black Box Fairness Testing of Machine Learning Models. 11 (2019). https://doi.org/10.1145/3338906.3338937",
      "doi": ""
    },
    {
      "text": "Rikke\u00a0Sand Andersen and Mette\u00a0Bech Ris\u00f8r. 2014. The importance of contextualization. Anthropological reflections on descriptive analysis, its limitations and implications. Anthropology and Medicine 21, 3 (sep 2014), 345\u2013356. https://doi.org/10.1080/13648470.2013.876355",
      "doi": ""
    },
    {
      "text": "Vijay Arya, Rachel K\u00a0E Bellamy, Yu Chen, Amit Dhurandhar, Michael Hind, Samuel\u00a0C Hoffman, Stephanie Houde, Q\u00a0Vera Liao, Ronny Luss, Aleksandra Mojsilovi\u0107, Sami Mourad, Pablo Pedemonte, Ramya Raghavendra, John Richards, Prasanna Sattigeri, Karthikeyan Shanmugam, Moninder Singh, Kush\u00a0R Varshney, Dennis Wei, and Yunfeng Zhang. [n.d.]. One Explanation Does Not Fit All: A Toolkit and Taxonomy of AI Explainability Techniques. Technical Report. arxiv:1909.03012v2http://aix360.",
      "doi": ""
    },
    {
      "text": "Tobias Baer. 2019. Understand, Manage, and Prevent Algorithmic Bias: A Guide for Business Users and Data Scientists. Apress.",
      "doi": ""
    },
    {
      "text": "Solon Barocas, Moritz Hardt, and Arvind Narayanan. 2019. Fairness and machine learning. fairmlbook.org. https://fairmlbook.org/index.html",
      "doi": ""
    },
    {
      "text": "Rachel K.\u00a0E. Bellamy, Kuntal Dey, Michael Hind, Samuel\u00a0C. Hoffman, Stephanie Houde, Kalapriya Kannan, Pranay Lohia, Jacquelyn Martino, Sameep Mehta, Aleksandra Mojsilovic, Seema Nagar, Karthikeyan\u00a0Natesan Ramamurthy, John Richards, Diptikalyan Saha, Prasanna Sattigeri, Moninder Singh, Kush\u00a0R. Varshney, and Yunfeng Zhang. 2018. AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias. Advances in Neural Information Processing Systems 2017-Decem, Nips (oct 2018), 5681\u20135690. arxiv:1810.01943http://arxiv.org/abs/1810.01943",
      "doi": ""
    },
    {
      "text": "Jay Budzik and Kristian Hammond. 1999. Watson: Anticipating and Contextualizing Information Needs. In Proceedings of the Sixty-second Annual Meeting of the American Society for Information Science.",
      "doi": ""
    },
    {
      "text": "Flavio Chierichetti, Ravi Kumar, Silvio Lattanzi, and Sergei Vassilvitskii. 2017. Fair Clustering Through Fairlets. In Proceedings of the 31st Conference on Neural Information Processing Systems (NIPS). Long Beach, CA, 5029\u20135037.",
      "doi": "10.5555/3295222.3295256"
    },
    {
      "text": "Civil Comments. 2019. Jigsaw Unintended Bias in Toxicity Classification. https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification",
      "doi": ""
    },
    {
      "text": "Sam Corbett-Davies and Sharad Goel. 2018. The Measure and Mismeasure of Fairness: A Critical Review of Fair Machine Learning. (jul 2018). arxiv:1808.00023http://arxiv.org/abs/1808.00023",
      "doi": ""
    },
    {
      "text": "Henriette Cramer, Sravana Reddy, Romain\u00a0Takeo Bouyer, Jean Garcia-Gathright, and Aaron Springer. 2019. Translation, tracks & Data: An algorithmic bias effort in practice. In Conference on Human Factors in Computing Systems - Proceedings. Association for Computing Machinery. https://doi.org/10.1145/3290607.3299057",
      "doi": "10.1145/3290607.3299057"
    },
    {
      "text": "Kate Crawford. 2017. The Trouble with Bias. https://www.youtube.com/watch?v=fMym_BKWQzk",
      "doi": ""
    },
    {
      "text": "Kimberle Crenshaw. 1989. Demarginalizing the Intersection of Race and Sex: A Black Feminist Critique of Antidiscrimination Doctrine, Feminist Theory and Antiracist Politics. University of Chicago Legal Forum 1989, 1 (1989), 139\u2013167. http://chicagounbound.uchicago.edu/uclfhttp://chicagounbound.uchicago.edu/uclf/vol1989/iss1/8",
      "doi": ""
    },
    {
      "text": "Matt Day. 2016. How LinkedIn\u2019s search engine may reflect a gender bias. https://www.seattletimes.com/business/microsoft/how-linkedins-search-engine-may-reflect-a-bias/",
      "doi": ""
    },
    {
      "text": "Jonathan Dodge, Q.\u00a0Vera Liao, Yunfeng Zhang, Rachel K.\u00a0E. Bellamy, and Casey Dugan. 2019. Explaining Models: An Empirical Study of How Explanations Impact Fairness Judgment. International Conference on Intelligent User Interfaces, Proceedings IUI Part F147615 (jan 2019), 275\u2013285. https://doi.org/10.1145/3301275.3302310 arxiv:1901.07694",
      "doi": "10.1145/3301275.3302310"
    },
    {
      "text": "Dheeru Dua and Casey Graff. 2017. UCI Machine Learning Repository. http://archive.ics.uci.edu/ml",
      "doi": ""
    },
    {
      "text": "Sorelle\u00a0A. Friedler, Carlos Scheidegger, Suresh Venkatasubramanian, Sonam Choudhary, Evan\u00a0P. Hamilton, and Derek Roth. 2018. A comparative study of fairness-enhancing interventions in machine learning. FAT* 2019 - Proceedings of the 2019 Conference on Fairness, Accountability, and Transparency (feb 2018), 329\u2013338. arxiv:1802.04422http://arxiv.org/abs/1802.04422",
      "doi": ""
    },
    {
      "text": "Salvador Garc\u00eda, Sergio Ram\u00edrez-Gallego, Juli\u00e1n Luengo, Jos\u00e9\u00a0Manuel Ben\u00edtez, and Francisco Herrera. 2016. Big data preprocessing: methods and prospects. Big Data Analytics 1, 1 (2016), 9.",
      "doi": ""
    },
    {
      "text": "Jean Garcia-Gathright, Aaron Springer, and Henriette Cramer. 2018. Assessing and Addressing Algorithmic Bias - But Before We Get There. In Proceedings ofthe AAAI2018 Spring Symposium: Designing the User Experience ofArtificial Intelligence. arxiv:1809.03332http://arxiv.org/abs/1809.03332",
      "doi": ""
    },
    {
      "text": "Google. 2020. Google Colaboratory.",
      "doi": ""
    },
    {
      "text": "Google. 2020. What-If Tool. https://pair-code.github.io/what-if-tool/get-started/",
      "doi": ""
    },
    {
      "text": "Moritz Hardt, Eric Price, and Nati Srebro. 2016. Equality of opportunity in supervised learning. In Advances in neural information processing systems. 3315\u20133323.",
      "doi": ""
    },
    {
      "text": "Anna\u00a0Lauren Hoffmann. 2019. Where fairness fails: data, algorithms, and the limits of antidiscrimination discourse. Information, Communication & Society 22, 7 (2019), 900\u2013915. https://doi.org/10.1080/1369118X.2019.1573912",
      "doi": ""
    },
    {
      "text": "Kenneth Holstein, Jennifer\u00a0Wortman Vaughan, Hal Daum\u00e9, Miro Dud\u00edk, and Hanna Wallach. 2018. Improving fairness in machine learning systems: What do industry practitioners need?Conference on Human Factors in Computing Systems - Proceedings (dec 2018). https://doi.org/10.1145/3290605.3300830 arxiv:1812.05239",
      "doi": ""
    },
    {
      "text": "Anna Jobin, Marcello Ienca, and Effy Vayena. 2019. The global landscape of AI ethics guidelines. Nature Machine Intelligence 1, 9 (sep 2019), 389\u2013399. https://doi.org/10.1038/s42256-019-0088-2",
      "doi": ""
    },
    {
      "text": "M.\u00a0I. Jordan and T.\u00a0M. Mitchell. 2015. Machine learning: Trends, perspectives, and prospects. , 255\u2013260\u00a0pages. https://doi.org/10.1126/science.aaa8415",
      "doi": ""
    },
    {
      "text": "Faisal Kamiran and Toon Calders. 2012. Data preprocessing techniques for classification without discrimination. Knowledge and Information Systems 33, 1 (oct 2012), 1\u201333. https://doi.org/10.1007/s10115-011-0463-8",
      "doi": "10.1007/s10115-011-0463-8"
    },
    {
      "text": "Harmanpreet Kaur, Harsha Nori, Samuel Jenkins, Rich Caruana, Hanna Wallach, and Jennifer Wortman Vaughan. 2020. Interpreting Interpretability: Understanding Data Scientists\u2019 Use of Interpretability Tools for Machine Learning. In CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3313831.3376219",
      "doi": ""
    },
    {
      "text": "Ronny Kohavi and Barry Becker. 1996. Adult Data Set. http://archive.ics.uci.edu/ml/datasets/Adult",
      "doi": ""
    },
    {
      "text": "Himabindu Lakkaraju and Osbert Bastani. 2019. \u201dHow do I fool you?\u201d: Manipulating User Trust via Misleading Black Box Explanations. AIES 2020 - Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society (nov 2019), 79\u201385. arxiv:1911.06473http://arxiv.org/abs/1911.06473",
      "doi": ""
    },
    {
      "text": "Po-Ming Law, Sana Malik, Fan Du, and Moumita Sinha. [n.d.]. The Impact of Presentation Style on Human-In-The-Loop Detection of Algorithmic Bias. Technical Report. arxiv:2004.12388v3https://youtu.be/8ZqCKxsbMHg",
      "doi": ""
    },
    {
      "text": "Po-Ming Law, Sana Malik, Fan Du, and Moumita Sinha. 2020. Designing Tools for Semi-Automated Detection of Machine Learning Biases: An Interview Study. In Proceedings of the CHI 2020 Workshop on Detection and Design for Cognitive Biases in People and Computing Systems.",
      "doi": ""
    },
    {
      "text": "Pranay\u00a0K. Lohia, Karthikeyan\u00a0Natesan Ramamurthy, Manish Bhide, Diptikalyan Saha, Kush\u00a0R. Varshney, and Ruchir Puri. 2018. Bias Mitigation Post-processing for Individual and Group Fairness. ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings 2019-May (dec 2018), 2847\u20132851. arxiv:1812.06135http://arxiv.org/abs/1812.06135",
      "doi": ""
    },
    {
      "text": "Michael\u00a0A Madaio, Luke Stark, Jennifer Wortman Vaughan, and Hanna Wallach. 2020. Co-Designing Checklists to Understand Organizational Challenges and Opportunities around Fairness in AI. In CHI Conference on Human Factors in Computing Systems. ACM, Honolulu. https://doi.org/10.1145/3313831.3376445",
      "doi": "10.1145/3313831.3376445"
    },
    {
      "text": "Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. 2019. A Survey on Bias and Fairness in Machine Learning. (2019). arxiv:1908.09635v2",
      "doi": ""
    },
    {
      "text": "Microsoft. 2020. Fairlearn. https://fairlearn.github.io/",
      "doi": ""
    },
    {
      "text": "Arvind Narayanan. 2018. 21 fairness definitions and their politics. https://fairmlbook.org/tutorial2.html,https://www.youtube.com/embed/jIXIuYdnyyk",
      "doi": ""
    },
    {
      "text": "David\u00a0T. Newman, Nathanael\u00a0J. Fast, and Derek\u00a0J. Harmon. 2020. When eliminating bias isn\u2019t fair: Algorithmic reductionism and procedural justice in human resource decisions. Organizational Behavior and Human Decision Processes 160 (sep 2020), 149\u2013167. https://doi.org/10.1016/j.obhdp.2020.03.008",
      "doi": ""
    },
    {
      "text": "Safiya\u00a0Umoja Noble. 2018. Algorithms of Oppression: How Search Engines Reinforce Racism (first ed.). NYU Press. https://doi.org/10.2307/j.ctt1pwt9w5",
      "doi": ""
    },
    {
      "text": "Alexandra Olteanu, Carlos Castillo, Fernando Diaz, and Emre K\u0131c\u0131man. 2019. Social Data: Biases, Methodological Pitfalls, and Ethical Boundaries. Frontiers in Big Data 2, 13 (jul 2019), 13. https://doi.org/10.3389/fdata.2019.00013",
      "doi": ""
    },
    {
      "text": "Qualtrics. 2020. Qualtrics. https://www.qualtrics.com",
      "doi": ""
    },
    {
      "text": "Tahleen Rahman, Bartlomiej Surma, Michael Backes, and Yang Zhang. 2019. FairWalk: Towards fair graph embedding. In IJCAI International Joint Conference on Artificial Intelligence, Vol.\u00a02019-August. International Joint Conferences on Artificial Intelligence, 3289\u20133295. https://doi.org/10.24963/ijcai.2019/456",
      "doi": ""
    },
    {
      "text": "Marco\u00a0Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. [n.d.]. \u201dWhy Should I Trust You?\u201d Explaining the Predictions of Any Classifier. Technical Report. arxiv:1602.04938v1",
      "doi": ""
    },
    {
      "text": "Pedro Saleiro, Benedict Kuester, Loren Hinkson, Jesse London, Abby Stevens, Ari Anisfeld, Kit\u00a0T. Rodolfa, and Rayid Ghani. 2018. Aequitas: A Bias and Fairness Audit Toolkit. (nov 2018). arxiv:1811.05577http://arxiv.org/abs/1811.05577",
      "doi": ""
    },
    {
      "text": "Marcus Specht, Andreas Lorenz, and Andreas Zimmermann. 2006. An architecture for contextualized learning experiences. In Proceedings - Sixth International Conference on Advanced Learning Technologies, ICALT 2006, Vol.\u00a02006. 169\u2013173. https://doi.org/10.1109/icalt.2006.1652397",
      "doi": ""
    },
    {
      "text": "Sriram Vasudevan, Cyrus DiCiccio, and Kinjal Basu. 2020. Addressing bias in large-scale AI applications: The LinkedIn Fairness Toolkit. https://engineering.linkedin.com/blog/2020/lift-addressing-bias-in-large-scale-ai-applications",
      "doi": ""
    },
    {
      "text": "Michael Veale and Reuben Binns. 2017. Fairer machine learning in the real world: Mitigating discrimination without collecting sensitive data. Big Data & Society 4, 2 (2017). https://doi.org/10.1177/2053951717743530",
      "doi": ""
    },
    {
      "text": "Michael Veale, Max Van Kleek, and Reuben Binns. 2018. Fairness and Accountability Design Needs for Algorithmic Support in High-Stakes Public Sector Decision-Making. Conference on Human Factors in Computing Systems - Proceedings 2018-April (feb 2018). https://doi.org/10.1145/3173574.3174014 arxiv:1802.01029",
      "doi": ""
    },
    {
      "text": "Sahil Verma and Julia Rubin. 2018. Fairness Definitions Explained. IEEE/ACM International Workshop on Software Fairness 18 (2018). https://doi.org/10.1145/3194770.3194776",
      "doi": ""
    },
    {
      "text": "Benjamin Wilson, Judy Hoffman, and Jamie Morgenstern. 2019. Predictive Inequity in Object Detection. (feb 2019). arxiv:1902.11097http://arxiv.org/abs/1902.11097",
      "doi": ""
    },
    {
      "text": "Meike Zehlike, Francesco Bonchi, Carlos Castillo, Sara Hajian, Mohamed Megahed, and Ricardo Baeza-Yates. 2017. FA*IR: A fair top-k ranking algorithm. In International Conference on Information and Knowledge Management, Proceedings, Vol.\u00a0Part F131841. Association for Computing Machinery, New York, NY, USA, 1569\u20131578. https://doi.org/10.1145/3132847.3132938",
      "doi": "10.1145/3132847.3132938"
    }
  ]
}