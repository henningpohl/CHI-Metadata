{
  "doi": "10.1145/3411764.3445171",
  "title": "SEMOUR: A Scripted Emotional Speech Repository for Urdu",
  "published": "2021-05-07",
  "proctitle": "CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems",
  "pages": "1-12",
  "year": 2021,
  "badges": [],
  "abstract": "Designing reliable Speech Emotion Recognition systems is a complex task that inevitably requires sufficient data for training purposes. Such extensive datasets are currently available in only a few languages, including English, German, and Italian. In this paper, we present SEMOUR, the first scripted database of emotion-tagged speech in the Urdu language, to design an Urdu Speech Recognition System. Our gender-balanced dataset contains 15,040 unique instances recorded by eight professional actors eliciting a syntactically complex script. The dataset is phonetically balanced, and reliably exhibits a varied set of emotions as marked by the high agreement scores among human raters in experiments. We also provide various baseline speech emotion prediction scores on the database, which could be used for various applications like personalized robot assistants, diagnosis of psychological disorders, and getting feedback from a low-tech-enabled population, etc. On a random test sample, our model correctly predicts an emotion with a state-of-the-art 92% accuracy.",
  "authors": [
    {
      "name": "Nimra Zaheer",
      "institution": "Computer Science Information Technology University, Pakistan",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659702619",
      "orcid": "missing"
    },
    {
      "name": "Obaid Ullah Ahmad",
      "institution": "Computer Science Information Technology University, Pakistan",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659702077",
      "orcid": "missing"
    },
    {
      "name": "Ammar Ahmed",
      "institution": "Computer Science Information Technology University, Pakistan",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659701942",
      "orcid": "missing"
    },
    {
      "name": "Muhammad Shehryar Khan",
      "institution": "Computer Science Information Technology University, Pakistan",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659700935",
      "orcid": "missing"
    },
    {
      "name": "Mudassir Shabbir",
      "institution": "Computer Science Information Technology University, Pakistan",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81387608403",
      "orcid": "missing"
    }
  ],
  "references": [
    {
      "text": "Hazrat Ali, Nasir Ahmad, Khawaja\u00a0M Yahya, and Omar Farooq. 2012. A medium vocabulary Urdu isolated words balanced corpus for automatic speech recognition. In 2012 international conference on electronics computer technology (ICECT 2012). IEEE, Kanyakumari, India, 473\u2013476.",
      "doi": ""
    },
    {
      "text": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In 3rd International Conference on Learning Representations, ICLR 2015. Computational and Biological Learning Society, San Diego, CA, USA, 15\u00a0pages.",
      "doi": ""
    },
    {
      "text": "Anton Batliner, Kerstin Fischer, Richard Huber, J\u00f6rg Spilker, and Elmar N\u00f6th. 2000. Desperately seeking emotions or: Actors, wizards, and human beings. In ISCA tutorial and research workshop (ITRW) on speech and emotion. International Speech Communication Association, Newcastle, Northern Ireland, UK, 6\u00a0pages.",
      "doi": ""
    },
    {
      "text": "Anton Batliner, Stefan Steidl, and Elmar N\u00f6th. 2008. Releasing a thoroughly annotated and processed spontaneous emotional database: the FAU Aibo Emotion Corpus. In Proc. of a Satellite Workshop of LREC. European Language Resources Association, Marrakesh, Morocco, 28.",
      "doi": ""
    },
    {
      "text": "Felix Burkhardt, Astrid Paeschke, Miriam Rolfes, Walter\u00a0F Sendlmeier, and Benjamin Weiss. 2005. A database of German emotional speech. In Ninth European Conference on Speech Communication and Technology. International Speech Communication Association, Lisbon, Portugal, 1517\u20131520.",
      "doi": ""
    },
    {
      "text": "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, Jeannette\u00a0N Chang, Sungbok Lee, and Shrikanth\u00a0S Narayanan. 2008. IEMOCAP: Interactive emotional dyadic motion capture database. Language resources and evaluation 42, 4 (2008), 335.",
      "doi": ""
    },
    {
      "text": "Carlos Busso, Srinivas Parthasarathy, Alec Burmania, Mohammed AbdelWahab, Najmeh Sadoughi, and Emily\u00a0Mower Provost. 2016. MSP-IMPROV: An acted corpus of dyadic interactions to study emotion perception. IEEE Transactions on Affective Computing 8, 1 (2016), 67\u201380.",
      "doi": "10.1109/TAFFC.2016.2515617"
    },
    {
      "text": "Jos\u00e9\u00a0Carlos Castillo, Antonio Fern\u00e1ndez-Caballero, \u00c1lvaro Castro-Gonz\u00e1lez, Miguel\u00a0A. Salichs, and Mar\u00eda\u00a0T. L\u00f3pez. 2014. A Framework for Recognizing and Regulating Emotions in the Elderly. In Ambient Assisted Living and Daily Activities, Leandro Pecchia, Liming\u00a0Luke Chen, Chris Nugent, and Jos\u00e9 Bravo (Eds.). Springer International Publishing, Cham, 320\u2013327.",
      "doi": ""
    },
    {
      "text": "Richard\u00a0T Cauldwell. 2000. Where did the anger go? The role of context in interpreting emotion in speech. In ISCA Tutorial and Research Workshop (ITRW) on Speech and Emotion. International Speech Communication Association, Newcastle, Northern Ireland, UK, 5\u00a0pages.",
      "doi": ""
    },
    {
      "text": "Aggelina Chatziagapi, Georgios Paraskevopoulos, Dimitris Sgouropoulos, Georgios Pantazopoulos, Malvina Nikandrou, Theodoros Giannakopoulos, Athanasios Katsamanis, Alexandros Potamianos, and Shrikanth Narayanan. 2019. Data Augmentation Using GANs for Speech Emotion Recognition.. In INTERSPEECH. International Speech Communication Association, Graz, Austria, 171\u2013175.",
      "doi": ""
    },
    {
      "text": "Mingyi Chen, Xuanji He, Jing Yang, and Han Zhang. 2018. 3-D convolutional recurrent neural networks with attention model for speech emotion recognition. IEEE Signal Processing Letters 25, 10 (2018), 1440\u20131444.",
      "doi": ""
    },
    {
      "text": "Giovanni Costantini, Iacopo Iaderola, Andrea Paoloni, and Massimiliano Todisco. 2014. EMOVO corpus: an Italian emotional speech database. In International Conference on Language Resources and Evaluation (LREC 2014). European Language Resources Association (ELRA), European Language Resources Association, Reykjavik, Iceland, 3501\u20133504.",
      "doi": ""
    },
    {
      "text": "Ellen Douglas-Cowie, Nick Campbell, Roddy Cowie, and Peter Roach. 2003. Emotional speech: Towards a new generation of databases. Speech communication 40, 1-2 (2003), 33\u201360.",
      "doi": ""
    },
    {
      "text": "Ellen Douglas-Cowie, Laurence Devillers, Jean-Claude Martin, Roddy Cowie, Suzie Savvidou, Sarkis Abrilian, and Cate Cox. 2005. Multimodal databases of everyday emotion: Facing up to complexity. In Ninth European conference on speech communication and technology. International Speech Communication Association, Lisbon, Portugal, 4.",
      "doi": ""
    },
    {
      "text": "David\u00a0M Eberhard, Gary\u00a0F Simons, and Charles\u00a0D Fennig. 2020. Ethnologue: Languages of the world. 23rd edn. Dallas.",
      "doi": ""
    },
    {
      "text": "Florian Eyben, Klaus\u00a0R Scherer, Bj\u00f6rn\u00a0W Schuller, Johan Sundberg, Elisabeth Andr\u00e9, Carlos Busso, Laurence\u00a0Y Devillers, Julien Epps, Petri Laukka, Shrikanth\u00a0S Narayanan, 2015. The Geneva minimalistic acoustic parameter set (GeMAPS) for voice research and affective computing. IEEE transactions on affective computing 7, 2 (2015), 190\u2013202.",
      "doi": ""
    },
    {
      "text": "Haytham\u00a0M Fayek, Margaret Lech, and Lawrence Cavedon. 2017. Evaluating deep learning architectures for Speech Emotion Recognition. Neural Networks 92(2017), 60\u201368.",
      "doi": ""
    },
    {
      "text": "S.\u00a0M. Ghulam and T.\u00a0R. Soomro. 2018. Twitter and Urdu. In 2018 International Conference on Computing, Mathematics and Engineering Technologies (iCoMET). IEEE, Sukkur, Pakistan, 1\u20136. https://doi.org/10.1109/ICOMET.2018.8346370",
      "doi": ""
    },
    {
      "text": "Michael Grimm, Kristian Kroschel, and Shrikanth Narayanan. 2008. The Vera am Mittag German audio-visual emotional speech database. In 2008 IEEE international conference on multimedia and expo. IEEE, Hannover, Germany, 865\u2013868.",
      "doi": ""
    },
    {
      "text": "Wenjing Han, Tao Jiang, Yan Li, Bj\u00f6rn Schuller, and Huabin Ruan. 2020. Ordinal Learning for Emotion Recognition in Customer Service Calls. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, Barcelona, Spain, 6494\u20136498.",
      "doi": ""
    },
    {
      "text": "Agha Ali\u00a0Raza Haris Bin\u00a0Zia and Awais Athar. 2018. PronouncUR: An Urdu Pronunciation Lexicon Generator. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)(Miyazaki, Japan, 7-12). European Language Resources Association (ELRA), Paris, France, 5\u00a0pages.",
      "doi": ""
    },
    {
      "text": "Madiha Ijaz and Sarmad Hussain. 2007. Corpus based Urdu lexicon development. In the Proceedings of Conference on Language Technology (CLT07), University of Peshawar, Pakistan, Vol.\u00a073. Academia, Pakistan, 12.",
      "doi": ""
    },
    {
      "text": "P Jackson and S Haq. 2014. Surrey audio-visual expressed emotion (savee) database.",
      "doi": ""
    },
    {
      "text": "Rebecca J\u00fcrgens, Annika Grass, Matthis Drolet, and Julia Fischer. 2015. Effect of acting experience on emotion expression and recognition in voice: Non-actors provide better stimuli than expected. Journal of nonverbal behavior 39, 3 (2015), 195\u2013214.",
      "doi": ""
    },
    {
      "text": "Hasan Kabir and Abdul\u00a0Mannan Saleem. 2002. Speech assessment methods phonetic alphabet (SAMPA): Analysis of Urdu. , 6\u00a0pages.",
      "doi": ""
    },
    {
      "text": "Theodoros Kostoulas, Iosif Mporas, Todor Ganchev, and Nikos Fakotakis. 2008. The effect of emotional speech on a smart-home application. In International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems. Springer, Springer, Berlin, Heidelberg, Wroclaw, Poland, 305\u2013310.",
      "doi": "10.1007/978-3-540-69052-8_32"
    },
    {
      "text": "Siddique Latif, Adnan Qayyum, Muhammad Usman, and Junaid Qadir. 2018. Cross lingual speech emotion recognition: Urdu vs. western languages. In 2018 International Conference on Frontiers of Information Technology (FIT). IEEE, Islamabad, Pakistan, Pakistan, 88\u201393. https://doi.org/10.1109/FIT.2018.00023",
      "doi": ""
    },
    {
      "text": "Aijun Li, Fang Zheng, William Byrne, Pascale Fung, Terri Kamm, Yi Liu, Zhanjiang Song, Umar Ruhi, Veera Venkataramani, and Xiaoxia Chen. 2000. CASS: A phonetically transcribed corpus of Mandarin spontaneous speech. In Sixth International Conference on Spoken Language Processing. International Speech Communication Association, Beijing, China, 485\u2013488.",
      "doi": ""
    },
    {
      "text": "Jeng-Lin Li and Chi-Chun Lee. 2019. Attentive to Individual: A Multimodal Emotion Recognition Network with Personalized Attention Profile.. In INTERSPEECH. International Speech Communication Association, Graz, Austria, 211\u2013215.",
      "doi": ""
    },
    {
      "text": "Ya Li, Jianhua Tao, Linlin Chao, Wei Bao, and Yazhu Liu. 2017. CHEAVD: a Chinese natural emotional audio\u2013visual database. Journal of Ambient Intelligence and Humanized Computing 8, 6 (2017), 913\u2013924.",
      "doi": ""
    },
    {
      "text": "Steven\u00a0R Livingstone and Frank\u00a0A Russo. 2018. The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English. PloS one 13, 5 (2018), e0196391.",
      "doi": ""
    },
    {
      "text": "Brian McFee, Colin Raffel, Dawen Liang, Daniel\u00a0PW Ellis, Matt McVicar, Eric Battenberg, and Oriol Nieto. 2015. librosa: Audio and music signal analysis in python. In Proceedings of the 14th python in science conference, Vol.\u00a08. Academia, Austin, Texas, 18\u201325.",
      "doi": ""
    },
    {
      "text": "Mohamed Meddeb, Hichem Karray, and Adel\u00a0M Alimi. 2017. Building and analysing emotion corpus of the Arabic speech. In 2017 1st International Workshop on Arabic Script Analysis and Recognition (ASAR). IEEE, Nancy, France, 134\u2013139. https://doi.org/10.1109/ASAR.2017.8067775",
      "doi": ""
    },
    {
      "text": "Shinya Mori, Tsuyoshi Moriyama, and Shinji Ozawa. 2006. Emotional speech synthesis using subspace constraints in prosody. In 2006 IEEE International Conference on Multimedia and Expo. IEEE, Toronto, Ont., Canada, 1093\u20131096. https://doi.org/10.1109/ICME.2006.262725",
      "doi": ""
    },
    {
      "text": "Omid\u00a0Mohamad Nezami, Paria\u00a0Jamshid Lou, and Mansoureh Karami. 2019. ShEMO: a large-scale validated database for Persian speech emotion detection. Language Resources and Evaluation 53, 1 (2019), 1\u201316.",
      "doi": "10.1007/s10579-018-9427-x"
    },
    {
      "text": "Caglar Oflazoglu and Serdar Yildirim. 2013. Recognizing emotion from Turkish speech using acoustic features. EURASIP Journal on Audio, Speech, and Music Processing 2013, 1(2013), 26.",
      "doi": ""
    },
    {
      "text": "Jack Parry, Dimitri Palaz, Georgia Clarke, Pauline Lecomte, Rebecca Mead, Michael Berger, and Gregor Hofer. 2019. Analysis of Deep Learning Architectures for Cross-Corpus Speech Emotion Recognition.. In INTERSPEECH. International Speech Communication Association, Graz, Austria, 1656\u20131660.",
      "doi": ""
    },
    {
      "text": "M. Qasim, S. Nawaz, S. Hussain, and T. Habib. 2016. Urdu speech recognition system for district names of Pakistan: Development, challenges and solutions. In 2016 Conference of The Oriental Chapter of International Committee for Coordination and Standardization of Speech Databases and Assessment Techniques (O-COCOSDA). IEEE, Bali, Indonesia, 28\u201332. https://doi.org/10.1109/ICSDA.2016.7918979",
      "doi": ""
    },
    {
      "text": "Agha\u00a0Ali Raza, Sarmad Hussain, Huda Sarfraz, Inam Ullah, and Zahid Sarfraz. 2009. Design and development of phonetically rich Urdu speech corpus. In 2009 oriental COCOSDA international conference on speech database and assessments. IEEE, Urumqi, China, 38\u201343.",
      "doi": ""
    },
    {
      "text": "Fabien Ringeval, Andreas Sonderegger, Juergen Sauer, and Denis Lalanne. 2013. Introducing the RECOLA multimodal corpus of remote collaborative and affective interactions. In 2013 10th IEEE international conference and workshops on automatic face and gesture recognition (FG). IEEE, Shanghai, China, 1\u20138. https://doi.org/10.1109/FG.2013.6553805",
      "doi": ""
    },
    {
      "text": "James\u00a0A Russell and Albert Mehrabian. 1977. Evidence for a three-factor theory of emotions. Journal of Research in Personality 11, 3 (1977), 273 \u2013 294. https://doi.org/10.1016/0092-6566(77)90037-X",
      "doi": ""
    },
    {
      "text": "Jacob Sager, Ravi Shankar, Jacob Reinhold, and Archana Venkataraman. 2019. VESUS: A Crowd-Annotated Database to Study Emotion Production and Perception in Spoken English.. In INTERSPEECH. International Speech Communication Association, Graz, Austria, 316\u2013320.",
      "doi": ""
    },
    {
      "text": "Md Sahidullah and Goutam Saha. 2012. Design, analysis and experimental evaluation of block based transformation in MFCC computation for speaker recognition. Speech communication 54, 4 (2012), 543\u2013565.",
      "doi": ""
    },
    {
      "text": "Klaus\u00a0R Scherer. 2003. Vocal communication of emotion: A review of research paradigms. Speech communication 40, 1-2 (2003), 227\u2013256.",
      "doi": ""
    },
    {
      "text": "Bj\u00f6rn Schuller, Stefan Steidl, Anton Batliner, Julia Hirschberg, Judee\u00a0K Burgoon, Alice Baird, Aaron Elkins, Yue Zhang, Eduardo Coutinho, Keelan Evanini, 2016. The interspeech 2016 computational paralinguistics challenge: Deception, sincerity & native language. In 17TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2016), VOLS 1-5. International Speech Communication Association, San Francisco, CA, USA, 2001\u20132005.",
      "doi": ""
    },
    {
      "text": "Kristen\u00a0M. Scott, Simone Ashby, and Julian Hanna. 2020. \u201dHuman, All Too Human\u201d: NOAA Weather Radio and the Emotional Impact of Synthetic Voices. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI \u201920). Association for Computing Machinery, New York, NY, USA, 1\u20139. https://doi.org/10.1145/3313831.3376338",
      "doi": "10.1145/3313831.3376338"
    },
    {
      "text": "Jilt Sebastian and Piero Pierucci. 2019. Fusion Techniques for Utterance-Level Emotion Recognition Combining Speech and Transcripts.. In INTERSPEECH. International Speech Communication Association, Graz, Austria, 51\u201355.",
      "doi": ""
    },
    {
      "text": "Stanley\u00a0Smith Stevens, John Volkmann, and Edwin\u00a0B Newman. 1937. A scale for the measurement of the psychological magnitude pitch. The Journal of the Acoustical Society of America 8, 3 (1937), 185\u2013190.",
      "doi": ""
    },
    {
      "text": "Monorama Swain, Aurobinda Routray, and Prithviraj Kabisatpathy. 2018. Databases, features and classifiers for speech emotion recognition: a review. International Journal of Speech Technology 21, 1 (2018), 93\u2013120.",
      "doi": "10.1007/s10772-018-9491-z"
    },
    {
      "text": "Aditya Vashistha, Abhinav Garg, Richard Anderson, and Agha\u00a0Ali Raza. 2019. Threats, Abuses, Flirting, and Blackmail: Gender Inequity in Social Media Voice Forums. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (Glasgow, Scotland Uk) (CHI \u201919). Association for Computing Machinery, New York, NY, USA, 1\u201313. https://doi.org/10.1145/3290605.3300302",
      "doi": "10.1145/3290605.3300302"
    },
    {
      "text": "Xinzhou Xu, Jun Deng, Nicholas Cummins, Zixing Zhang, Li Zhao, and Bj\u00f6rn\u00a0W Schuller. 2019. Autonomous Emotion Learning in Speech: A View of Zero-Shot Speech Emotion Recognition.. In INTERSPEECH. International Speech Communication Association, Graz, Austria, 949\u2013953.",
      "doi": ""
    },
    {
      "text": "Sara Zhalehpour, Onur Onder, Zahid Akhtar, and Cigdem\u00a0Eroglu Erdem. 2016. BAUM-1: A spontaneous audio-visual face database of affective and mental states. IEEE Transactions on Affective Computing 8, 3 (2016), 300\u2013313.",
      "doi": "10.1109/TAFFC.2016.2553038"
    },
    {
      "text": "Jianhua Tao Fangzhou Liu\u00a0Meng Zhang and Huibin Jia. 2008. Design of speech corpus for mandarin text to speech. In The Blizzard Challenge 2008 workshop. International Speech Communication Association, Brisbane, Australia, 4.",
      "doi": ""
    },
    {
      "text": "Jianfeng Zhao, Xia Mao, and Lijiang Chen. 2019. Speech emotion recognition using deep 1D & 2D CNN LSTM networks. Biomedical Signal Processing and Control 47 (2019), 312\u2013323.",
      "doi": "10.1016/J.BSPC.2018.08.035"
    }
  ]
}