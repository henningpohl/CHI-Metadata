{
  "doi": "10.1145/3411764.3445315",
  "title": "Manipulating and Measuring Model Interpretability",
  "published": "2021-05-07",
  "proctitle": "CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems",
  "pages": "1-52",
  "year": 2021,
  "badges": [],
  "abstract": "With machine learning models being increasingly used to aid decision making even in high-stakes domains, there has been a growing interest in developing interpretable models. Although many supposedly interpretable models have been proposed, there have been relatively few experimental studies investigating whether these models achieve their intended effects, such as making people more closely follow a model\u2019s predictions when it is beneficial for them to do so or enabling them to detect when a model has made a mistake. We present a sequence of pre-registered experiments (N = 3, 800) in which we showed participants functionally identical models that varied only in two factors commonly thought to make machine learning models more or less interpretable: the number of features and the transparency of the model (i.e., whether the model internals are clear or black box). Predictably, participants who saw a clear model with few features could better simulate the model\u2019s predictions. However, we did not find that participants more closely followed its predictions. Furthermore, showing participants a clear model meant that they were less able to detect and correct for the model\u2019s sizable mistakes, seemingly due to information overload. These counterintuitive findings emphasize the importance of testing over intuition when developing interpretable models.",
  "authors": [
    {
      "name": "Forough Poursabzi-Sangdeh",
      "institution": "Microsoft Research, United States",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659323298",
      "orcid": "missing"
    },
    {
      "name": "Daniel G Goldstein",
      "institution": "Microsoft Research, United States",
      "img": "/do/10.1145/contrib-81485649647/rel-imgonly/dangoldstein_badge1.jpg",
      "acmid": "81485649647",
      "orcid": "missing"
    },
    {
      "name": "Jake M Hofman",
      "institution": "Microsoft Research, United States",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81436601976",
      "orcid": "missing"
    },
    {
      "name": "Jennifer Wortman Wortman Vaughan",
      "institution": "Microsoft Research, United States",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81758671157",
      "orcid": "missing"
    },
    {
      "name": "Hanna Wallach",
      "institution": "Microsoft Research, United States",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81315492225",
      "orcid": "missing"
    }
  ],
  "references": [
    {
      "text": "Ashraf Abdul, Christian von\u00a0der Weth, Mohan Kankanhalli, and Brian\u00a0Y Lim. 2020. COGAM: Measuring and Moderating Cognitive Load in Machine Learning Model Explanations. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. 1\u201314.",
      "doi": "10.1145/3313831.3376615"
    },
    {
      "text": "Russell\u00a0L Ackoff. 1967. Management misinformation systems. Management Science 14, 4 (1967), B\u2013141\u2013B\u2013274. https://doi.org/10.1287/mnsc.14.4.B147",
      "doi": ""
    },
    {
      "text": "Oscar Alvarado and Annika Waern. 2018. Towards Algorithmic Experience: Initial Efforts for Social Media Contexts. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems. ACM, 286. http://doi.acm.org/10.1145/3173574.3173860",
      "doi": "10.1145/3173574.3173860"
    },
    {
      "text": "Giuseppe Amatulli, Maria\u00a0Jo\u00e3o Rodrigues, Marco Trombetti, and Raffaella Lovreglio. 2006. Assessing long-term fire risk at local scale by means of decision tree technique. Journal of Geophysical Research: Biogeosciences 111, G4(2006).",
      "doi": ""
    },
    {
      "text": "Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2016. Machine bias: There\u2019s software used across the country to predict future criminals. And it\u2019s biased against blacks. ProPublica (2016).",
      "doi": ""
    },
    {
      "text": "Thomas \u00c5stebro and Samir Elhedhli. 2006. The effectiveness of simple decision heuristics: Forecasting commercial success for early-stage ventures. Management Science 52, 3 (2006), 395\u2013409. https://doi.org/10.1287/mnsc.1050.0468",
      "doi": "10.1287/mnsc.1050.0468"
    },
    {
      "text": "Douglas Bates, Martin M\u00e4chler, Ben Bolker, and Steve Walker. 2015. Fitting Linear Mixed-Effects Models Using lme4. Journal of Statistical Software 67, 1 (2015), 1\u201348.",
      "doi": ""
    },
    {
      "text": "Max\u00a0H Bazerman. 1985. Norms of distributive justice in interest arbitration. ILR Review 38, 4 (1985), 558\u2013570.",
      "doi": ""
    },
    {
      "text": "Victoria Bellotti and Keith Edwards. 2001. Intelligibility and accountability: Human considerations in context-aware systems. Human\u2013Computer Interaction 16, 2\u20134 (2001), 193\u2013212.",
      "doi": "10.1207/S15327051HCI16234_05"
    },
    {
      "text": "James Bennett, Stan Lanning, 2007. The netflix prize. In Proceedings of KDD Cup and Workshop.",
      "doi": ""
    },
    {
      "text": "Reuben Binns, Max Van\u00a0Kleek, Michael Veale, Ulrik Lyngs, Jun Zhao, and Nigel Shadbolt. 2018. \u2018It\u2019s Reducing a Human Being to a Percentage\u2019: Perceptions of Justice in Algorithmic Decisions. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems. ACM, 377. http://doi.acm.org/10.1145/3173574.3173951",
      "doi": "10.1145/3173574.3173951"
    },
    {
      "text": "L Breiman, JH Friedman, R Olshen, and CJ Stone. 1984. Classification and Regression Trees. (1984).",
      "doi": ""
    },
    {
      "text": "Taina Bucher. 2017. The algorithmic imaginary: exploring the ordinary affects of Facebook algorithms. Information, Communication & Society 20, 1 (2017), 30\u201344.",
      "doi": ""
    },
    {
      "text": "Michael Buhrmester, Tracy Kwang, and Samuel\u00a0D Gosling. 2011. Amazon\u2019s Mechanical Turk: A new source of inexpensive, yet high-quality, data?Perspectives on psychological science 6, 1 (2011), 3\u20135.",
      "doi": ""
    },
    {
      "text": "Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie Elhadad. 2015. Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission. In Proceedings of the 21st ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD).",
      "doi": "10.1145/2783258.2788613"
    },
    {
      "text": "Krista Casler, Lydia Bickel, and Elizabeth Hackett. 2013. Separate but equal? A comparison of participants and data gathered via Amazon\u2019s MTurk, social media, and face-to-face behavioral testing. Computers in Human Behavior 29, 6 (2013), 2156\u20132160.",
      "doi": "10.1016/j.chb.2013.05.009"
    },
    {
      "text": "Hao-Fei Cheng, Ruotong Wang, Zheng Zhang, Fiona O\u2019Connell, Terrance Gray, F.\u00a0Maxwell Harper, and Haiyi Zhu. 2019. Explaining Decision-Making Algorithms through UI: Strategies to Help Non-Expert Stakeholders. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems.",
      "doi": "10.1145/3290605.3300789"
    },
    {
      "text": "Alexandra Chouldechova. 2017. Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. Big data 5, 2 (2017), 153\u2013163.",
      "doi": ""
    },
    {
      "text": "Marvin\u00a0M Chun. 2000. Contextual cueing of visual attention. Trends in cognitive sciences 4, 5 (2000), 170\u2013178.",
      "doi": ""
    },
    {
      "text": "Eric Colson. 2013. Using human and machine processing in recommendation systems. In First AAAI Conference on Human Computation and Crowdsourcing.",
      "doi": ""
    },
    {
      "text": "Alexander Coppock. 2019. Generalizing from survey experiments conducted on Mechanical Turk: A replication approach. Political Science Research and Methods 7, 3 (2019), 613\u2013628.",
      "doi": ""
    },
    {
      "text": "Jeffrey Dastin. 2018. Amazon scraps secret AI recruiting tool that showed bias against women. (2018).",
      "doi": ""
    },
    {
      "text": "Robyn\u00a0M Dawes. 1979. The robust beauty of improper linear models in decision making.American psychologist 34, 7 (1979), 571.",
      "doi": ""
    },
    {
      "text": "Robyn\u00a0M Dawes, David Faust, and Paul\u00a0E Meehl. 1989. Clinical versus actuarial judgment. Science 243, 4899 (1989), 1668\u20131674.",
      "doi": ""
    },
    {
      "text": "Berkeley\u00a0J. Dietvorst, Joseph\u00a0P. Simmons, and Cade Massey. 2015. Algorithm aversion: People erroneously avoid algorithms after seeing them err. Journal of Experimental Psychology: General 144, 1 (2015), 114\u2013126.",
      "doi": ""
    },
    {
      "text": "Berkeley\u00a0J Dietvorst, Joseph\u00a0P Simmons, and Cade Massey. 2018. Overcoming algorithm aversion: People will use imperfect algorithms if they can (even slightly) modify them. Management Science 64, 3 (2018), 1155\u20131170.",
      "doi": "10.1287/mnsc.2016.2643"
    },
    {
      "text": "Jaap\u00a0J Dijkstra. 1999. User agreement with incorrect expert system advice. Behaviour & Information Technology 18, 6 (1999), 399\u2013411.",
      "doi": ""
    },
    {
      "text": "Jaap\u00a0J Dijkstra, Wim\u00a0BG Liebrand, and Ellen Timminga. 1998. Persuasiveness of expert systems. Behaviour & Information Technology 17, 3 (1998), 155\u2013163.",
      "doi": ""
    },
    {
      "text": "Jonathan Dodge, Q\u00a0Vera Liao, Yunfeng Zhang, Rachel\u00a0KE Bellamy, and Casey Dugan. 2019. Explaining models: an empirical study of how explanations impact fairness judgment. In Proceedings of the 24th International Conference on Intelligent User Interfaces. 275\u2013285.",
      "doi": "10.1145/3301275.3302310"
    },
    {
      "text": "Finale Doshi-Velez and Been Kim. 2017. Towards a rigorous science of interpretable machine learning. arXiv preprint arXiv:1702.08608(2017).",
      "doi": ""
    },
    {
      "text": "Mary\u00a0T Dzindolet, Linda\u00a0G Pierce, Hall\u00a0P Beck, and Lloyd\u00a0A Dawe. 2002. The perceived utility of human and automated aids in a visual detection task. Human Factors 44, 1 (2002), 79\u201394.",
      "doi": ""
    },
    {
      "text": "Dedre Gentner and Albert\u00a0L. Stevens. 1983. Mental Models. Lawrence Erlbaum Associates.",
      "doi": ""
    },
    {
      "text": "Vivian Giang. 2018. The Potential Hidden Bias In Automated Hiring Systems. (2018). Accessed at https://www.fastcompany.com/40566971/the-potential-hidden-bias-in-automated-hiring-systems/.",
      "doi": ""
    },
    {
      "text": "Gerd Gigerenzer and Daniel\u00a0G Goldstein. 1996. Reasoning the fast and frugal way: models of bounded rationality.Psychological review 103, 4 (1996), 650.",
      "doi": ""
    },
    {
      "text": "Gerd Gigerenzer and Peter\u00a0M Todd. 1999. Simple heuristics that make us smart. Oxford University Press, USA.",
      "doi": ""
    },
    {
      "text": "Francesca Gino and Don\u00a0A. Moore. 2007. Effects of task difficulty on use of advice. Journal of Behavioral Decision Making 20, 1 (2007), 21\u201335.",
      "doi": ""
    },
    {
      "text": "Alyssa Glass, Deborah\u00a0L McGuinness, and Michael Wolverton. 2008. Toward establishing trust in adaptive agents. In Proceedings of the 13th International Conference on Intelligent User Interfaces (IUI).",
      "doi": "10.1145/1378773.1378804"
    },
    {
      "text": "Daniel\u00a0G Goldstein and Gerd Gigerenzer. 2009. Fast and frugal forecasting. International journal of forecasting 25, 4 (2009), 760\u2013772.",
      "doi": ""
    },
    {
      "text": "Nitesh Goyal and Susan\u00a0R Fussell. 2016. Effects of sensemaking translucence on distributed collaborative analysis. In Proceedings of the 19th ACM Conference on Computer-Supported Cooperative Work & Social Computing. 288\u2013302.",
      "doi": "10.1145/2818048.2820071"
    },
    {
      "text": "Nina Grgi\u0107-Hla\u010da, Christoph Engel, and Krishna\u00a0P Gummadi. 2019. Human Decision Making with Machine Assistance: An Experiment on Bailing and Jailing. Proceedings of the ACM on Human-Computer Interaction 3, CSCW(2019), 1\u201325.",
      "doi": ""
    },
    {
      "text": "William\u00a0M Grove, David\u00a0H Zald, Boyd\u00a0S Lebow, Beth\u00a0E Snitz, and Chad Nelson. 2000. Clinical versus mechanical prediction: a meta-analysis.Psychological assessment 12, 1 (2000), 19.",
      "doi": ""
    },
    {
      "text": "Todd\u00a0M Gureckis, Jay Martin, John McDonnell, Alexander\u00a0S Rich, Doug Markant, Anna Coenen, David Halpern, Jessica\u00a0B Hamrick, and Patricia Chan. 2016. psiTurk: An open-source framework for conducting replicable behavioral experiments online. Behavior Research Methods 48, 3 (2016), 829\u2013842.",
      "doi": ""
    },
    {
      "text": "David\u00a0J Hand and William\u00a0E Henley. 1997. Statistical classification methods in consumer credit scoring: a review. Journal of the Royal Statistical Society: Series A (Statistics in Society) 160, 3(1997), 523\u2013541.",
      "doi": ""
    },
    {
      "text": "Fred Hohman, Andrew Head, Rich Caruana, Robert DeLine, and Steven\u00a0M Drucker. 2019. Gamut: A design probe to understand how data scientists understand machine learning models. In Proceedings of the 2019 CHI conference on human factors in computing systems. 1\u201313.",
      "doi": "10.1145/3290605.3300809"
    },
    {
      "text": "Johan Huysmans, Karel Dejaeger, Christophe Mues, Jan Vanthienen, and Bart Baesens. 2011. An empirical evaluation of the comprehensibility of decision table, tree and rule based predictive models. Decision Support Systems 51, 1 (2011), 141\u2013154.",
      "doi": "10.1016/j.dss.2010.12.003"
    },
    {
      "text": "Jacob Jacoby. 1984. Perspectives on information overload. Journal of consumer research 10, 4 (1984), 432\u2013435.",
      "doi": ""
    },
    {
      "text": "Philip Johnson-Laird. 1983. Mental Models: Towards a Cognitive Science of Language, Inference, and Consciousness. Cambridge University Press.",
      "doi": "10.5555/7909"
    },
    {
      "text": "Jongbin Jung, Connor Concannon, Ravi Shroff, Sharad Goel, and Daniel\u00a0G Goldstein. 2020. Simple rules to guide expert classifications. Journal of the Royal Statistical Society: Series A (Statistics in Society) (2020).",
      "doi": ""
    },
    {
      "text": "Harmanpreet Kaur, Harsha Nori, Samuel Jenkins, Rich Caruana, Hanna Wallach, and Jennifer\u00a0Wortman Vaughan. 2020. Interpreting Interpretability: Understanding Data Scientists\u2019 Use of Interpretability Tools for Machine Learning. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems.",
      "doi": "10.1145/3313831.3376219"
    },
    {
      "text": "Kevin\u00a0Lane Keller and Richard Staelin. 1987. Effects of quality and quantity of information on decision effectiveness. Journal of consumer research 14, 2 (1987), 200\u2013213.",
      "doi": ""
    },
    {
      "text": "Hyunjoong Kim, Wei-Yin Loh, Yu-Shan Shih, and Probal Chaudhuri. 2007. Visualizable and interpretable regression models with good prediction power. IIE Transactions 39, 6 (2007), 565\u2013579.",
      "doi": ""
    },
    {
      "text": "Yea-Seul Kim, Katharina Reinecke, and Jessica Hullman. 2017. Explaining the gap: Visualizing one\u2019s predictions improves recall and comprehension of data. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems. 1375\u20131386.",
      "doi": "10.1145/3025453.3025592"
    },
    {
      "text": "Jon Kleinberg, Himabindu Lakkaraju, Jure Leskovec, Jens Ludwig, and Sendhil Mullainathan. 2018. Human decisions and machine predictions. The quarterly journal of economics 133, 1 (2018), 237\u2013293.",
      "doi": ""
    },
    {
      "text": "Jon Kleinberg and Sendhil Mullainathan. 2019. Simplicity creates inequity: implications for fairness, stereotypes, and interpretability. In Proceedings of the 2019 ACM Conference on Economics and Computation. 807\u2013808.",
      "doi": "10.1145/3328526.3329621"
    },
    {
      "text": "Rafal Kocielnik, Saleema Amershi, and Paul\u00a0N Bennett. 2019. Will you accept an imperfect ai? exploring designs for adjusting end-user expectations of ai systems. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. 1\u201314.",
      "doi": "10.1145/3290605.3300641"
    },
    {
      "text": "Pang\u00a0Wei Koh and Percy Liang. 2017. Understanding Black-box Predictions via Influence Functions. In Proceedings of the 34th International Conference on Machine Learning (ICML).",
      "doi": "10.5555/3305381.3305576"
    },
    {
      "text": "Igor Kononenko. 2001. Machine learning for medical diagnosis: history, state of the art and perspective. Artificial Intelligence in medicine 23, 1 (2001), 89\u2013109.",
      "doi": "10.1016/S0933-3657%2801%2900077-X"
    },
    {
      "text": "Josua Krause, Aritra Dasgupta, Jordan Swartz, Yindalon Aphinyanaphongs, and Enrico Bertini. 2017. A Workflow for Visual Diagnostics of Binary Classifiers using Instance-Level Explanations. In Proceedings of IEEE Conference and Visual Analytics Science and Technology.",
      "doi": ""
    },
    {
      "text": "Todd Kulesza, Simone Stumpf, Margaret Burnett, Sherry Yang, Irwin Kwan, and Weng-Keen Wong. 2013. Too much, too little, or just right? Ways explanations impact end users\u2019 mental models. In Proceedings of the IEEE Symposium on Visual Languages and Human-Centric Computing. 3\u201310.",
      "doi": ""
    },
    {
      "text": "Isaac Lage, Emily Chen, Jeffrey He, Menaka Narayanan, Been Kim, Samuel\u00a0J Gershman, and Finale Doshi-Velez. 2019. Human evaluation of models built for interpretability. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, Vol.\u00a07. 59\u201367.",
      "doi": ""
    },
    {
      "text": "Isaac Lage, Andrew\u00a0Slavin Ross, Been Kim, Samuel\u00a0J. Gershman, and Finale Doshi-Velez. 2018. Human-in-the-Loop Interpretability Prior. In Advances in Neural Information Processing Systems.",
      "doi": ""
    },
    {
      "text": "Himabindu Lakkaraju, Stephen\u00a0H. Bach, and Jure Leskovec. 2016. Interpretable Decision Sets: A Joint Framework for Description and Prediction. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 1675\u20131684.",
      "doi": "10.1145/2939672.2939874"
    },
    {
      "text": "Himabindu Lakkaraju, Ece Kamar, Rich Caruana, and Jure Leskovec. 2017. Interpretable & Explorable Approximations of Black Box Models. In FATML Workshop.",
      "doi": ""
    },
    {
      "text": "Himabindu Lakkaraju, Ece Kamar, Rich Caruana, and Jure Leskovec. 2019. Faithful and customizable explanations of black box models. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society. 131\u2013138.",
      "doi": "10.1145/3306618.3314229"
    },
    {
      "text": "Q\u00a0Vera Liao, Daniel Gruen, and Sarah Miller. 2020. Questioning the AI: Informing Design Practices for Explainable AI User Experiences. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. 1\u201315.",
      "doi": "10.1145/3313831.3376590"
    },
    {
      "text": "Cynthia\u00a0CS Liem, Markus Langer, Andrew Demetriou, Annemarie\u00a0MF Hiemstra, Achmadnoer\u00a0Sukma Wicaksana, Marise\u00a0Ph Born, and Cornelius\u00a0J K\u00f6nig. 2018. Psychology Meets Machine Learning: Interdisciplinary Perspectives on Algorithmic Job Candidate Screening. In Explainable and Interpretable Models in Computer Vision and Machine Learning. Springer, 197\u2013253.",
      "doi": ""
    },
    {
      "text": "Brian\u00a0Y Lim, Anind\u00a0K Dey, and Daniel Avrahami. 2009. Why and why not explanations improve the intelligibility of context-aware intelligent systems. In Proceedings of the 2009 CHI Conference on Human Factors in Computing Systems. ACM, 2119\u20132128. http://doi.acm.org/10.1145/1518701.1519023",
      "doi": ""
    },
    {
      "text": "Zachary\u00a0C Lipton. 2016. The mythos of model interpretability. arXiv preprint arXiv:1606.03490(2016).",
      "doi": ""
    },
    {
      "text": "Jia Liu and Olivier Toubia. 2018. A semantic approach for estimating consumer content preferences from online search queries. Marketing Science 37, 6 (2018), 855\u20131052. https://doi.org/10.1287/mksc.2018.1112",
      "doi": ""
    },
    {
      "text": "Jennifer\u00a0M. Logg. 2017. Theory of Machine: When Do People Rely on Algorithms? (2017). Harvard Business School NOM Unit Working Paper No. 17-086.",
      "doi": ""
    },
    {
      "text": "Jennifer\u00a0M Logg, Julia\u00a0A Minson, and Don\u00a0A Moore. 2019. Algorithm appreciation: People prefer algorithmic to human judgment. Organizational Behavior and Human Decision Processes 151 (2019), 90\u2013103.",
      "doi": ""
    },
    {
      "text": "Yin Lou, Rich Caruana, and Johannes Gehrke. 2012. Intelligible Models for Classification and Regression. In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)(Beijing, China).",
      "doi": "10.1145/2339530.2339556"
    },
    {
      "text": "Yin Lou, Rich Caruana, Johannes Gehrke, and Giles Hooker. 2013. Accurate Intelligible Models with Pairwise Interactions. In Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD) (Chicago, IL, USA).",
      "doi": "10.1145/2487575.2487579"
    },
    {
      "text": "Scott Lundberg and Su-In Lee. 2017. A unified approach to interpreting model predictions. In Advances in Neural Information Processing Systems 30 (NIPS).",
      "doi": ""
    },
    {
      "text": "Winter Mason and Siddharth Suri. 2012. Conducting behavioral research on Amazon\u2019s Mechanical Turk. Behavior research methods 44, 1 (2012), 1\u201323.",
      "doi": ""
    },
    {
      "text": "Paul\u00a0E Meehl. 1990. Why summaries of research on psychological theories are often uninterpretable. Psychological reports 66, 1 (1990), 195\u2013244.",
      "doi": ""
    },
    {
      "text": "Rishabh Mehrotra, James McInerney, Hugues Bouchard, Mounia Lalmas, and Fernando Diaz. 2018. Towards a Fair Marketplace: Counterfactual Evaluation of the trade-off between Relevance, Fairness & Satisfaction in Recommendation Systems. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management. ACM, 2243\u20132251. http://doi.acm.org/10.1145/3269206.3272027",
      "doi": "10.1145/3269206.3272027"
    },
    {
      "text": "Tim Miller. 2019. Explanation in artificial intelligence: Insights from the social sciences. Artificial Intelligence 267 (2019), 1\u201338.",
      "doi": ""
    },
    {
      "text": "Tim Miller, Piers Howe, and Liz Sonenberg. 2017. Explainable AI: Beware of inmates running the asylum. arXiv preprint arXiv:1712.00547(2017).",
      "doi": ""
    },
    {
      "text": "Safiya\u00a0Umoja Noble. 2018. Algorithms of Oppression: How search engines reinforce racism. NYU Press.",
      "doi": ""
    },
    {
      "text": "Harsha Nori, Samuel Jenkins, Paul Koch, and Rich Caruana. 2019. InterpretML: A Unified Framework for Machine Learning Interpretability. arXiv preprint arXiv:1909.09223(2019).",
      "doi": ""
    },
    {
      "text": "Don\u00a0A. Norman. 1987. Some Observations on Mental Models. In Human-Computer Interaction: A Multidisciplinary Approach, R.\u00a0M. Baecker and W.\u00a0A.\u00a0S. Buxton (Eds.). Morgan Kaufmann Publishers Inc., 241\u2013244.",
      "doi": ""
    },
    {
      "text": "Dilek \u00d6nkal, Paul Goodwin, Mary Thomson, and Sinan G\u00f6n\u00fcl. 2009. The relative influence of advice from human experts and statistical methods on forecast adjustments. Journal of Behavioral Decision Making 22 (2009), 390\u2013409.",
      "doi": ""
    },
    {
      "text": "Gabriele Paolacci and Jesse Chandler. 2014. Inside the Turk: Understanding Mechanical Turk as a participant pool. Current Directions in Psychological Science 23, 3 (2014), 184\u2013188.",
      "doi": ""
    },
    {
      "text": "Peter Pirolli and Daniel\u00a0M Russell. 2011. Introduction to this special issue on sensemaking.",
      "doi": ""
    },
    {
      "text": "Marianne Promberger and Jonathan Baron. 2006. Do patients trust computers?Journal of Behavioral Decision Making 19, 5 (2006), 455\u2013468.",
      "doi": ""
    },
    {
      "text": "Emilee Rader, Kelley Cotter, and Janghee Cho. 2018. Explanations as Mechanisms for Supporting Algorithmic Transparency. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems. ACM, 103. http://doi.acm.org/10.1145/3173574.3173677",
      "doi": "10.1145/3173574.3173677"
    },
    {
      "text": "Emilee Rader and Rebecca Gray. 2015. Understanding user beliefs about algorithmic curation in the Facebook news feed. In Proceedings of the 2015 CHI Conference on Human Factors in Computing Systems. ACM, 173\u2013182. http://doi.acm.org/10.1145/2702123.2702174",
      "doi": "10.1145/2702123.2702174"
    },
    {
      "text": "Marco\u00a0Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. Why should I trust you?: Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD).",
      "doi": "10.1145/2939672.2939778"
    },
    {
      "text": "Christian Rudder. 2014. Dataclysm: Love, Sex, Race, and Identity\u2013What Our Online Lives Tell Us about Our Offline Selves. Crown.",
      "doi": ""
    },
    {
      "text": "Cynthia Rudin. 2019. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence 1, 5 (2019), 206\u2013215.",
      "doi": ""
    },
    {
      "text": "Cynthia Rudin and Berk Ustun. 2018. Optimized Scoring Systems: Toward Trust in Machine Learning for Healthcare and Criminal Justice. Applied Analytics 48, 5 (2018), 449\u2013466. https://doi.org/10.1287/inte.2018.0957",
      "doi": ""
    },
    {
      "text": "Chris Russell. 2019. Efficient search for diverse coherent explanations. In Proceedings of the Conference on Fairness, Accountability, and Transparency. 20\u201328.",
      "doi": "10.1145/3287560.3287569"
    },
    {
      "text": "Daniel\u00a0M Russell, Mark\u00a0J Stefik, Peter Pirolli, and Stuart\u00a0K Card. 1993. The cost structure of sensemaking. In Proceedings of the INTERACT\u201993 and CHI\u201993 conference on Human factors in computing systems. 269\u2013276.",
      "doi": "10.1145/169059.169209"
    },
    {
      "text": "Paul\u00a0JH Schoemaker and C\u00a0Carter Waid. 1982. An experimental comparison of different approaches to determining weights in additive utility models. Management Science 28, 2 (1982), 182\u2013196. https://doi.org/10.1287/mnsc.28.2.182",
      "doi": "10.1287/mnsc.28.2.182"
    },
    {
      "text": "Kathryn Sharpe\u00a0Wessling, Joel Huber, and Oded Netzer. 2017. MTurk character misrepresentation: Assessment and solutions. Journal of Consumer Research 44, 1 (2017), 211\u2013230.",
      "doi": ""
    },
    {
      "text": "Simone Stumpf, Vidya Rajaram, Lida Li, Weng-Keen Wong, Margaret Burnett, Thomas Dietterich, Erin Sullivan, and Jonathan Herlocker. 2009. Interacting meaningfully with machine learning systems: Three experiments. International Journal of Human-Computer Studies 67, 8 (2009), 639\u2013662.",
      "doi": "10.1016/j.ijhcs.2009.03.004"
    },
    {
      "text": "Sarah Tan, Rich Caruana, Giles Hooker, and Yin Lou. 2018. Distill-and-compare: Auditing black-box models using transparent model distillation. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society. 303\u2013310.",
      "doi": "10.1145/3278721.3278725"
    },
    {
      "text": "Nava Tintarev and Judith Masthoff. 2015. Explaining recommendations: Design and evaluation. In Recommender systems handbook. Springer, 353\u2013382.",
      "doi": ""
    },
    {
      "text": "Richard Tomsett, David Braines, Dan Harborne, Alun Preece, and Supriyo Chakraborty. 2018. Interpretable to Whom? A Role-Based Model for Analyzing Interpretable Machine Learning Systems. In 2018 Workshop on Human Interpretability in Machine Learning.",
      "doi": ""
    },
    {
      "text": "Amos Tversky and Daniel Kahneman. 1974. Judgment under uncertainty: Heuristics and biases. science 185, 4157 (1974), 1124\u20131131.",
      "doi": ""
    },
    {
      "text": "Berk Ustun and Cynthia Rudin. 2016. Supersparse Linear Integer Models for Optimized Medical Scoring Systems. Machine Learning Journal 102, 3 (2016), 349\u2013391.",
      "doi": "10.1007/s10994-015-5528-6"
    },
    {
      "text": "Berk Ustun, Alexander Spangher, and Yang Liu. 2019. Actionable recourse in linear classification. In Proceedings of the Conference on Fairness, Accountability, and Transparency. 10\u201319.",
      "doi": "10.1145/3287560.3287566"
    },
    {
      "text": "Kristen Vaccaro, Dylan Huang, Motahhare Eslami, Christian Sandvig, Kevin Hamilton, and Karrie Karahalios. 2018. The illusion of control: Placebo effects of control settings. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems. 1\u201313.",
      "doi": "10.1145/3173574.3173590"
    },
    {
      "text": "Vanya\u00a0MCA Van\u00a0Belle, Ben Van\u00a0Calster, Dirk Timmerman, Tom Bourne, Cecilia Bottomley, Lil Valentin, Patrick Neven, Sabine Van\u00a0Huffel, Johan\u00a0AK Suykens, and Stephen Boyd. 2012. A mathematical model for interpretable clinical decision support with applications in gynecology. PloS one 7, 3 (2012).",
      "doi": ""
    },
    {
      "text": "Michael Veale, Max Van\u00a0Kleek, and Reuben Binns. 2018. Fairness and accountability design needs for algorithmic support in high-stakes public sector decision-making. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems. 1\u201314.",
      "doi": "10.1145/3173574.3174014"
    },
    {
      "text": "Kim\u00a0J Vicente and Gerard\u00a0L Torenvliet. 2000. The Earth is spherical(p < 0. 05): alternative methods of statistical inference. Theoretical Issues in Ergonomics Science 1, 3 (2000), 248\u2013271.",
      "doi": ""
    },
    {
      "text": "Sandra Wachter, Brent Mittelstadt, and Chris Russell. 2017. Counterfactual explanations without opening the black box: Automated decisions and the GDPR. Harv. JL & Tech. 31(2017), 841.",
      "doi": ""
    },
    {
      "text": "Sara Wachter-Boettcher. 2017. Why You Can\u2019t Trust AI to Make Unbiased Hiring Decisions. (2017). Accessed at http://time.com/4993431/ai-recruiting-tools-do-not-/.",
      "doi": ""
    },
    {
      "text": "Danding Wang, Qian Yang, Ashraf Abdul, and Brian\u00a0Y Lim. 2019. Designing theory-driven user-centric explainable AI. In Proceedings of the 2019 CHI conference on human factors in computing systems. 1\u201315.",
      "doi": "10.1145/3290605.3300831"
    },
    {
      "text": "Fulton Wang and Cynthia Rudin. 2015. Falling rule lists. In Artificial Intelligence and Statistics. 1013\u20131022.",
      "doi": ""
    },
    {
      "text": "Martin Wattenberg, Fernanda Vi\u00e9gas, and Moritz Hardt. 2016. Attacking discrimination with smarter machine learning. (2016). Accessed at https://research.google.com/bigpicture/attacking-discrimination-in-ml/.",
      "doi": ""
    },
    {
      "text": "Daniel\u00a0S Weld and Gagan Bansal. 2019. The challenge of crafting intelligible intelligence. Commun. ACM 62, 6 (2019), 70\u201379.",
      "doi": "10.1145/3282486"
    },
    {
      "text": "Ilan Yaniv. 2004. Receiving other people\u2019s advice: Influence and benefit. Organizational Behavior and Human Decision Processes 93 (2004), 1\u201313.",
      "doi": ""
    }
  ]
}