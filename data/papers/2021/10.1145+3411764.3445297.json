{
  "doi": "10.1145/3411764.3445297",
  "title": "StickyPie: A Gaze-Based, Scale-Invariant Marking Menu Optimized for AR/VR",
  "published": "2021-05-07",
  "proctitle": "CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems",
  "pages": "1-16",
  "year": 2021,
  "badges": [],
  "abstract": "This work explores the design of marking menus for gaze-based AR/VR menu selection by expert and novice users. It first identifies and explains the challenges inherent in ocular motor control and current eye tracking hardware, including overshooting, incorrect selections, and false activations. Through three empirical studies, we optimized and validated design parameters to mitigate these errors while reducing completion time, task load, and eye fatigue. Based on the findings from these studies, we derived a set of design guidelines to support gaze-based marking menus in AR/VR. To overcome the overshoot errors found with eye-based expert marking menu behaviour, we developed StickyPie, a marking menu technique that enables scale-independent marking input by estimating saccade landing positions. An evaluation of StickyPie revealed that StickyPie was easier to learn than the traditional technique (i.e., RegularPie) and was 10% more efficient after 3 sessions.",
  "tags": [
    "head-worn display",
    "AR/VR",
    "eye gaze input",
    "marking menu"
  ],
  "authors": [
    {
      "name": "Sunggeun Ahn",
      "institution": "Chatham Labs, Toronto, Ontario, Canada and HCI Lab, School of Computing, KAIST Daejeon, Korea, Republic of",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99658638580",
      "orcid": "missing"
    },
    {
      "name": "Stephanie Santosa",
      "institution": "Chatham Labs, Toronto, Ontario, Canada",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81435594884",
      "orcid": "missing"
    },
    {
      "name": "Mark Parent",
      "institution": "Chatham Labs, Toronto, Ontario, Canada",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659702153",
      "orcid": "missing"
    },
    {
      "name": "Daniel Wigdor",
      "institution": "Chatham Labs, Toronto, Ontario, Canada and University of Toronto Toronto, Ontario, Canada",
      "img": "/do/10.1145/contrib-81100606762/rel-imgonly/img_1274.jpg",
      "acmid": "81100606762",
      "orcid": "0000-0003-2008-7070"
    },
    {
      "name": "Tovi Grossman",
      "institution": "Department of Computer Science, University of Toronto, Canada",
      "img": "/do/10.1145/contrib-81100477897/rel-imgonly/81100477897.jpg",
      "acmid": "81100477897",
      "orcid": "0000-0002-0494-5373"
    },
    {
      "name": "Marcello Giordano",
      "institution": "Chatham Labs, Toronto, Ontario, Canada",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659702351",
      "orcid": "missing"
    }
  ],
  "references": [
    {
      "text": "Caroline Appert and Shumin Zhai. 2009. Using Strokes as Command Shortcuts: Cognitive Benefits and Toolkit Support. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (Boston, MA, USA) (CHI \u201909). Association for Computing Machinery, New York, NY, USA, 2289\u20132298. https://doi.org/10.1145/1518701.1519052",
      "doi": "10.1145/1518701.1519052"
    },
    {
      "text": "Olivier Bau and Wendy\u00a0E. Mackay. 2008. OctoPocus: A Dynamic Guide for Learning Gesture-Based Command Sets. In Proceedings of the 21st Annual ACM Symposium on User Interface Software and Technology(Monterey, CA, USA) (UIST \u201908). Association for Computing Machinery, New York, NY, USA, 37\u201346. https://doi.org/10.1145/1449715.1449724",
      "doi": "10.1145/1449715.1449724"
    },
    {
      "text": "Jonas Blattgerste, Patrick Renner, and Thies Pfeiffer. 2018. Advantages of Eye-Gaze over Head-Gaze-Based Selection in Virtual and Augmented Reality under Varying Field of Views. In Proceedings of the Workshop on Communication by Gaze Interaction (Warsaw, Poland) (COGAIN \u201918). Association for Computing Machinery, New York, NY, USA, Article 1, 9\u00a0pages. https://doi.org/10.1145/3206343.3206349",
      "doi": "10.1145/3206343.3206349"
    },
    {
      "text": "Andy Cockburn, Carl Gutwin, Joey Scarr, and Sylvain Malacria. 2014. Supporting Novice to Expert Transitions in User Interfaces. ACM Comput. Surv. 47, 2, Article 31 (Nov. 2014), 36\u00a0pages. https://doi.org/10.1145/2659796",
      "doi": "10.1145/2659796"
    },
    {
      "text": "HTC Corporation. 2020. VIVE Pro Eye Specs & User Guide. https://developer.vive.com/resources/vive-sense/hardware-guide/vive-pro-eye-specs-user-guide/, last visited Dec. 2020.",
      "doi": ""
    },
    {
      "text": "William Delamare, Teng Han, and Pourang Irani. 2017. Designing a Gaze Gesture Guiding System. In Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services (Vienna, Austria) (MobileHCI \u201917). Association for Computing Machinery, New York, NY, USA, Article 26, 13\u00a0pages. https://doi.org/10.1145/3098279.3098561",
      "doi": "10.1145/3098279.3098561"
    },
    {
      "text": "Heiko Drewes, Mohamed Khamis, and Florian Alt. 2019. DialPlates: Enabling Pursuits-Based User Interfaces with Large Target Numbers. In Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia (Pisa, Italy) (MUM \u201919). Association for Computing Machinery, New York, NY, USA, Article 10, 10\u00a0pages. https://doi.org/10.1145/3365610.3365626",
      "doi": "10.1145/3365610.3365626"
    },
    {
      "text": "Heiko Drewes and Albrecht Schmidt. 2007. Interacting with the computer using gaze gestures. In IFIP Conference on Human-Computer Interaction. Springer, 475\u2013488.",
      "doi": ""
    },
    {
      "text": "Augusto Esteves, Eduardo Velloso, Andreas Bulling, and Hans Gellersen. 2015. Orbits: Gaze Interaction for Smart Watches Using Smooth Pursuit Eye Movements. In Proceedings of the 28th Annual ACM Symposium on User Interface Software and Technology(Charlotte, NC, USA) (UIST \u201915). ACM, New York, NY, USA, 457\u2013466. https://doi.org/10.1145/2807442.2807499",
      "doi": "10.1145/2807442.2807499"
    },
    {
      "text": "inc. FOVE. 2020. FOVE0 Headset Specification. https://fove-inc.com/product/, last visited Dec. 2020.",
      "doi": ""
    },
    {
      "text": "Aaron\u00a0L. Gardony, Robert\u00a0W. Lindeman, and Tad\u00a0T. Bruny\u00e9. 2020. Eye-tracking for human-centered mixed reality: promises and challenges. In Optical Architectures for Displays and Sensing in Augmented, Virtual, and Mixed Reality (AR, VR, MR), Bernard\u00a0C. Kress and Christophe Peroz (Eds.), Vol.\u00a011310. International Society for Optics and Photonics, SPIE, 230 \u2013 247. https://doi.org/10.1117/12.2542699",
      "doi": ""
    },
    {
      "text": "Jay Henderson, Sylvain Malacria, Mathieu Nancel, and Edward Lank. 2020. Investigating the Necessity of Delay in Marking Menu Invocation. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI \u201920). Association for Computing Machinery, New York, NY, USA, 1\u201313. https://doi.org/10.1145/3313831.3376296",
      "doi": "10.1145/3313831.3376296"
    },
    {
      "text": "Wen-jun Hou, Kai-xiang Chen, Hao Li, and Hu Zhou. 2018. User Defined Eye Movement-Based Interaction for Virtual Reality. In Cross-Cultural Design. Methods, Tools, and Users, Pei-Luen\u00a0Patrick Rau (Ed.). Springer International Publishing, Cham, 18\u201330.",
      "doi": ""
    },
    {
      "text": "Anke Huckauf and Mario Urbina. 2007. Gazing with PEYE: New Concepts in Eye Typing. In Proceedings of the 4th Symposium on Applied Perception in Graphics and Visualization (Tubingen, Germany) (APGV \u201907). Association for Computing Machinery, New York, NY, USA, 141. https://doi.org/10.1145/1272582.1272618",
      "doi": "10.1145/1272582.1272618"
    },
    {
      "text": "Anke Huckauf and Mario\u00a0H. Urbina. 2008. Gazing with PEYEs: Towards a Universal Input for Various Applications. In Proceedings of the 2008 Symposium on Eye Tracking Research & Applications (Savannah, Georgia) (ETRA \u201908). Association for Computing Machinery, New York, NY, USA, 51\u201354. https://doi.org/10.1145/1344471.1344483",
      "doi": "10.1145/1344471.1344483"
    },
    {
      "text": "Aulikki Hyrskykari, Howell Istance, and Stephen Vickers. 2012. Gaze Gestures or Dwell-Based Interaction?. In Proceedings of the Symposium on Eye Tracking Research and Applications (Santa Barbara, California) (ETRA \u201912). Association for Computing Machinery, New York, NY, USA, 229\u2013232. https://doi.org/10.1145/2168556.2168602",
      "doi": "10.1145/2168556.2168602"
    },
    {
      "text": "Poika Isokoski. 2000. Text Input Methods for Eye Trackers Using Off-Screen Targets. In Proceedings of the 2000 Symposium on Eye Tracking Research & Applications (Palm Beach Gardens, Florida, USA) (ETRA \u201900). Association for Computing Machinery, New York, NY, USA, 15\u201321. https://doi.org/10.1145/355017.355020",
      "doi": "10.1145/355017.355020"
    },
    {
      "text": "Toshiya Isomoto, Shota Yamanaka, and Buntarou Shizuki. 2020. Gaze-based Command Activation Technique Robust Against Unintentional Activation using Dwell-then-Gesture. In Proceedings of Graphics Interface 2020(University of Toronto) (GI 2020). Canadian Human-Computer Communications Society / Soci\u00e9t\u00e9 canadienne du dialogue humain-machine, 256 \u2013 266. https://doi.org/10.20380/GI2020.26",
      "doi": ""
    },
    {
      "text": "Robert J.\u00a0K. Jacob. 1991. The Use of Eye Movements in Human-computer Interaction Techniques: What You Look at is What You Get. ACM Trans. Inf. Syst. 9, 2 (April 1991), 152\u2013169. https://doi.org/10.1145/123078.128728",
      "doi": "10.1145/123078.128728"
    },
    {
      "text": "Yvonne Kammerer, Katharina Scheiter, and Wolfgang Beinhauer. 2008. Looking My Way through the Menu: The Impact of Menu Design and Multimodal Input on Gaze-Based Menu Selection. In Proceedings of the 2008 Symposium on Eye Tracking Research & Applications (Savannah, Georgia) (ETRA \u201908). Association for Computing Machinery, New York, NY, USA, 213\u2013220. https://doi.org/10.1145/1344471.1344522",
      "doi": "10.1145/1344471.1344522"
    },
    {
      "text": "Mohamed Khamis, Carl Oechsner, Florian Alt, and Andreas Bulling. 2018. VRpursuits: Interaction in Virtual Reality Using Smooth Pursuit Eye Movements. In Proceedings of the 2018 International Conference on Advanced Visual Interfaces(Castiglione della Pescaia, Grosseto, Italy) (AVI \u201918). Association for Computing Machinery, New York, NY, USA, Article 18, 8\u00a0pages. https://doi.org/10.1145/3206505.3206522",
      "doi": "10.1145/3206505.3206522"
    },
    {
      "text": "Gordon Kurtenbach and William Buxton. 1994. User learning and performance with marking menus. In Proceedings of the SIGCHI conference on Human factors in computing systems. 258\u2013264.",
      "doi": "10.1145/191666.191759"
    },
    {
      "text": "Gordon Kurtenbach, Thomas\u00a0P. Moran, and William Buxton. 1994. Contextual animation of gestural commands. In Computer Graphics Forum, Vol.\u00a013. Wiley Online Library, 305\u2013314.",
      "doi": ""
    },
    {
      "text": "Mikko Kyt\u00f6, Barrett Ens, Thammathip Piumsomboon, Gun\u00a0A. Lee, and Mark Billinghurst. 2018. Pinpointing: Precise Head- and Eye-Based Target Selection for Augmented Reality. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (Montreal QC, Canada) (CHI \u201918). ACM, New York, NY, USA, Article 81, 14\u00a0pages. https://doi.org/10.1145/3173574.3173655",
      "doi": "10.1145/3173574.3173655"
    },
    {
      "text": "Pupil Labs. 2020. FOVE0 Headset Specification. https://pupil-labs.com/products/vr-ar/tech-specs/, last visited Dec. 2020.",
      "doi": ""
    },
    {
      "text": "Jeff\u00a0J Macinnes, Shariq Iqbal, John Pearson, and Elizabeth\u00a0N Johnson. 2018. Wearable Eye-tracking for Research: Automated dynamic gaze mapping and accuracy/precision comparisons across devices. bioRxiv (2018), 299925.",
      "doi": ""
    },
    {
      "text": "P\u00e4ivi Majaranta, Ulla-Kaija Ahola, and Oleg \u0160pakov. 2009. Fast Gaze Typing with an Adjustable Dwell Time. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (Boston, MA, USA) (CHI \u201909). Association for Computing Machinery, New York, NY, USA, 357\u2013360. https://doi.org/10.1145/1518701.1518758",
      "doi": "10.1145/1518701.1518758"
    },
    {
      "text": "P\u00e4ivi Majaranta, Jari Laitinen, Jari Kangas, and Poika Isokoski. 2019. Inducing Gaze Gestures by Static Illustrations. In Proceedings of the 11th ACM Symposium on Eye Tracking Research & Applications (Denver, Colorado) (ETRA \u201919). Association for Computing Machinery, New York, NY, USA, Article 75, 5\u00a0pages. https://doi.org/10.1145/3317956.3318151",
      "doi": "10.1145/3317956.3318151"
    },
    {
      "text": "P\u00e4ivi Majaranta, Kari-Jouko R\u00e4ih\u00e4, Aulikki Hyrskykari, and Oleg \u0160pakov. 2019. Eye Movements and Human-Computer Interaction. Springer International Publishing, Cham, 971\u20131015.",
      "doi": ""
    },
    {
      "text": "Miguel\u00a0A. Nacenta, Yemliha Kamber, Yizhou Qiang, and Per\u00a0Ola Kristensson. 2013. Memorability of Pre-Designed and User-Defined Gesture Sets. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (Paris, France) (CHI \u201913). Association for Computing Machinery, New York, NY, USA, 1099\u20131108. https://doi.org/10.1145/2470654.2466142",
      "doi": "10.1145/2470654.2466142"
    },
    {
      "text": "Anneli Olsen and Ricardo Matos. 2012. Identifying parameter values for an I-VT fixation filter suitable for handling data sampled with various sampling frequencies. In proceedings of the symposium on Eye tracking research and applications. 317\u2013320.",
      "doi": "10.1145/2168556.2168625"
    },
    {
      "text": "Marco Porta and Matteo Turina. 2008. Eye-S: A Full-Screen Input Modality for Pure Eye-Based Communication. In Proceedings of the 2008 Symposium on Eye Tracking Research & Applications (Savannah, Georgia) (ETRA \u201908). Association for Computing Machinery, New York, NY, USA, 27\u201334. https://doi.org/10.1145/1344471.1344477",
      "doi": "10.1145/1344471.1344477"
    },
    {
      "text": "Yuan\u00a0Yuan Qian and Robert\u00a0J. Teather. 2017. The Eyes Don\u2019t Have It: An Empirical Comparison of Head-Based and Eye-Based Selection in Virtual Reality. In Proceedings of the 5th Symposium on Spatial User Interaction (Brighton, United Kingdom) (SUI \u201917). Association for Computing Machinery, New York, NY, USA, 91\u201398. https://doi.org/10.1145/3131277.3132182",
      "doi": "10.1145/3131277.3132182"
    },
    {
      "text": "Vijay Rajanna and John\u00a0Paulin Hansen. 2018. Gaze Typing in Virtual Reality: Impact of Keyboard Design, Selection Method, and Motion. In Proceedings of the 2018 ACM Symposium on Eye Tracking Research & Applications (Warsaw, Poland) (ETRA \u201918). ACM, New York, NY, USA, Article 15, 10\u00a0pages. https://doi.org/10.1145/3204493.3204541",
      "doi": "10.1145/3204493.3204541"
    },
    {
      "text": "Quentin Roy, Sylvain Malacria, Yves Guiard, Eric Lecolinet, and James Eagan. 2013. Augmented Letters: Mnemonic Gesture-Based Shortcuts. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (Paris, France) (CHI \u201913). Association for Computing Machinery, New York, NY, USA, 2325\u20132328. https://doi.org/10.1145/2470654.2481321",
      "doi": "10.1145/2470654.2481321"
    },
    {
      "text": "Joey Scarr, Andy Cockburn, Carl Gutwin, and Philip Quinn. 2011. Dips and Ceilings: Understanding and Supporting Transitions to Expertise in User Interfaces. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (Vancouver, BC, Canada) (CHI \u201911). Association for Computing Machinery, New York, NY, USA, 2741\u20132750. https://doi.org/10.1145/1978942.1979348",
      "doi": "10.1145/1978942.1979348"
    },
    {
      "text": "Ludwig Sidenmark and Hans Gellersen. 2019. Eye, Head and Torso Coordination During Gaze Shifts in Virtual Reality. ACM Trans. Comput.-Hum. Interact. 27, 1, Article 4 (Dec. 2019), 40\u00a0pages. https://doi.org/10.1145/3361218",
      "doi": "10.1145/3361218"
    },
    {
      "text": "Ludwig Sidenmark and Hans Gellersen. 2019. Eye&Head: Synergetic Eye and Head Movement for Gaze Pointing and Selection. In Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology (New Orleans, LA, USA) (UIST \u201919). Association for Computing Machinery, New York, NY, USA, 1161\u20131174. https://doi.org/10.1145/3332165.3347921",
      "doi": "10.1145/3332165.3347921"
    },
    {
      "text": "Alexandra Sipatchin, Siegfried Wahl, and Katharina Rifai. 2020. Eye-tracking for low vision with virtual reality (VR): testing status quo usability of the HTC Vive Pro Eye. bioRxiv (2020).",
      "doi": ""
    },
    {
      "text": "Samuel Stuart, Aodhan Hickey, Rodrigo Vitorio, Karen Welman, Stacy Foo, David Keen, and Alan Godfrey. 2019. Eye-tracker algorithms to detect saccades during static and dynamic tasks: a structured review. Physiological Measurement 40, 2 (feb 2019), 02TR01. https://doi.org/10.1088/1361-6579/ab02ab",
      "doi": ""
    },
    {
      "text": "David\u00a0E Thompson, Stefanie Blain-Moraes, and Jane\u00a0E Huggins. 2013. Performance assessment in brain-computer interface-based augmentative and alternative communication. Biomedical engineering online 12, 1 (2013), 43.",
      "doi": ""
    },
    {
      "text": "Geoffrey Tien and M.\u00a0Stella Atkins. 2008. Improving Hands-Free Menu Selection Using Eyegaze Glances and Fixations. In Proceedings of the 2008 Symposium on Eye Tracking Research & Applications (Savannah, Georgia) (ETRA \u201908). Association for Computing Machinery, New York, NY, USA, 47\u201350. https://doi.org/10.1145/1344471.1344482",
      "doi": "10.1145/1344471.1344482"
    },
    {
      "text": "Mario\u00a0H. Urbina, Maike Lorenz, and Anke Huckauf. 2010. Pies with EYEs: The Limits of Hierarchical Pie Menus in Gaze Control. In Proceedings of the 2010 Symposium on Eye-Tracking Research & Applications (Austin, Texas) (ETRA \u201910). Association for Computing Machinery, New York, NY, USA, 93\u201396. https://doi.org/10.1145/1743666.1743689",
      "doi": "10.1145/1743666.1743689"
    },
    {
      "text": "Jacob\u00a0O. Wobbrock, Leah Findlater, Darren Gergle, and James\u00a0J. Higgins. 2011. The Aligned Rank Transform for Nonparametric Factorial Analyses Using Only Anova Procedures. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (Vancouver, BC, Canada) (CHI \u201911). ACM, New York, NY, USA, 143\u2013146. https://doi.org/10.1145/1978942.1978963",
      "doi": "10.1145/1978942.1978963"
    },
    {
      "text": "Jacob\u00a0O. Wobbrock, James Rubinstein, Michael\u00a0W. Sawyer, and Andrew\u00a0T. Duchowski. 2008. Longitudinal Evaluation of Discrete Consecutive Gaze Gestures for Text Entry. In Proceedings of the 2008 Symposium on Eye Tracking Research & Applications (Savannah, Georgia) (ETRA \u201908). Association for Computing Machinery, New York, NY, USA, 11\u201318. https://doi.org/10.1145/1344471.1344475",
      "doi": "10.1145/1344471.1344475"
    },
    {
      "text": "Shengdong Zhao and Ravin Balakrishnan. 2004. Simple vs. Compound Mark Hierarchical Marking Menus. In Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology (Santa Fe, NM, USA) (UIST \u201904). Association for Computing Machinery, New York, NY, USA, 33\u201342. https://doi.org/10.1145/1029632.1029639",
      "doi": "10.1145/1029632.1029639"
    }
  ]
}