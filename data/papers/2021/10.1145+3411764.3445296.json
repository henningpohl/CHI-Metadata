{
  "doi": "10.1145/3411764.3445296",
  "title": "Evaluating the Interpretability of Generative Models by Interactive Reconstruction",
  "published": "2021-05-07",
  "proctitle": "CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems",
  "pages": "1-15",
  "year": 2021,
  "badges": [],
  "abstract": "For machine learning models to be most useful in numerous sociotechnical systems, many have argued that they must be human-interpretable. However, despite increasing interest in interpretability, there remains no firm consensus on how to measure it. This is especially true in representation learning, where interpretability research has focused on \u201cdisentanglement\u201d measures only applicable to synthetic datasets and not grounded in human factors. We introduce a task to quantify the human-interpretability of generative model representations, where users interactively modify representations to reconstruct target instances. On synthetic datasets, we find performance on this task much more reliably differentiates entangled and disentangled models than baseline approaches. On a real dataset, we find it differentiates between representation learning methods widely believed but never shown to produce more or less interpretable models. In both cases, we ran small-scale think-aloud studies and large-scale experiments on Amazon Mechanical Turk to confirm that our qualitative and quantitative results agreed.",
  "authors": [
    {
      "name": "Andrew Ross",
      "institution": "School of Engineering and Applied Sciences Harvard University, United States",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659231588",
      "orcid": "missing"
    },
    {
      "name": "Nina Chen",
      "institution": "Harvard College, United States",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659694305",
      "orcid": "missing"
    },
    {
      "name": "Elisa Zhao Hang",
      "institution": "Harvard College, United States",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659693094",
      "orcid": "missing"
    },
    {
      "name": "Elena L. Glassman",
      "institution": "SEAS Harvard University, United States",
      "img": "/do/10.1145/contrib-82658918057/rel-imgonly/glassman_200x300.jpg",
      "acmid": "82658918057",
      "orcid": "0000-0001-5178-3496"
    },
    {
      "name": "Finale Doshi-Velez",
      "institution": "SEAS Harvard University, United States",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81435611644",
      "orcid": "missing"
    }
  ],
  "references": [
    {
      "text": "Ashraf Abdul, Christian von\u00a0der Weth, Mohan Kankanhalli, and Brian\u00a0Y Lim. 2020. COGAM: Measuring and Moderating Cognitive Load in Machine Learning Model Explanations. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. 1\u201314.",
      "doi": "10.1145/3313831.3376615"
    },
    {
      "text": "Hiva Allahyari and Niklas Lavesson. 2011. User-oriented assessment of classification model understandability. In 11th scandinavian conference on Artificial intelligence. IOS Press.",
      "doi": ""
    },
    {
      "text": "David Alvarez-Melis and Tommi\u00a0S Jaakkola. 2018. Towards robust interpretability with self-explaining neural networks. (2018). arXiv:1806.07538",
      "doi": ""
    },
    {
      "text": "Dustin\u00a0L Arendt, Nasheen Nur, Zhuanyi Huang, Gabriel Fair, and Wenwen Dou. 2020. Parallel embeddings: a visualization technique for contrasting learned representations. In Proceedings of the 25th International Conference on Intelligent User Interfaces. 259\u2013274.",
      "doi": "10.1145/3377325.3377514"
    },
    {
      "text": "Gagan Bansal, Besmira Nushi, Ece Kamar, Walter\u00a0S Lasecki, Daniel\u00a0S Weld, and Eric Horvitz. 2019. Beyond accuracy: The role of mental models in human-AI team performance. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing. 2\u201311.",
      "doi": ""
    },
    {
      "text": "David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. 2017. Network dissection: Quantifying interpretability of deep visual representations. In Proceedings of the IEEE conference on computer vision and pattern recognition. 6541\u20136549.",
      "doi": ""
    },
    {
      "text": "David Bau, Jun-Yan Zhu, Hendrik Strobelt, Bolei Zhou, Joshua\u00a0B Tenenbaum, William\u00a0T Freeman, and Antonio Torralba. 2019. Visualizing and Understanding GANs. (2019).",
      "doi": ""
    },
    {
      "text": "Yoshua Bengio, Aaron Courville, and Pascal Vincent. 2013. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence 35, 8(2013), 1798\u20131828.",
      "doi": "10.1109/TPAMI.2013.50"
    },
    {
      "text": "David\u00a0M Blei, Andrew\u00a0Y Ng, and Michael\u00a0I Jordan. 2003. Latent dirichlet allocation. Journal of machine Learning research 3, Jan (2003), 993\u20131022.",
      "doi": "10.5555/944919.944937"
    },
    {
      "text": "Leo Breiman. 2001. Random forests. Machine learning 45, 1 (2001), 5\u201332.",
      "doi": "10.1023/a%3A1010933404324"
    },
    {
      "text": "Zana Bu\u00e7inca, Phoebe Lin, Krzysztof\u00a0Z Gajos, and Elena\u00a0L Glassman. 2020. Proxy tasks and subjective measures can be misleading in evaluating explainable AI systems. In Proceedings of the 25th International Conference on Intelligent User Interfaces. 454\u2013464.",
      "doi": "10.1145/3377325.3377498"
    },
    {
      "text": "Christopher\u00a0P Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Desjardins, and Alexander Lerchner. 2018. Understanding disentangling in beta-VAE. (2018). arXiv:1804.03599",
      "doi": ""
    },
    {
      "text": "Carrie\u00a0J Cai, Emily Reif, Narayan Hegde, Jason Hipp, Been Kim, Daniel Smilkov, Martin Wattenberg, Fernanda Viegas, Greg\u00a0S Corrado, Martin\u00a0C Stumpe, 2019. Human-centered tools for coping with imperfect algorithms during medical decision-making. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. 1\u201314.",
      "doi": "10.1145/3290605.3300234"
    },
    {
      "text": "Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie Elhadad. 2015. Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission. In Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining. 1721\u20131730.",
      "doi": "10.1145/2783258.2788613"
    },
    {
      "text": "Marco Cavallo and \u00c7a\u011fatay Demiralp. 2018. A visual interaction framework for dimensionality reduction based data exploration. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems. 1\u201313.",
      "doi": ""
    },
    {
      "text": "Paul Chandler and John Sweller. 1992. The split-attention effect as a factor in the design of instruction. British Journal of Educational Psychology 62, 2 (1992), 233\u2013246.",
      "doi": ""
    },
    {
      "text": "Ricky\u00a0TQ Chen, Xuechen Li, Roger\u00a0B Grosse, and David\u00a0K Duvenaud. 2018. Isolating sources of disentanglement in variational autoencoders. In Advances in neural information processing systems. 2610\u20132620.",
      "doi": ""
    },
    {
      "text": "Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. 2016. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In Advances in neural information processing systems. 2172\u20132180.",
      "doi": ""
    },
    {
      "text": "Steffen Czolbe, Oswin Krause, Ingemar Cox, and Christian Igel. 2020. A Loss Function for Generative Neural Networks Based on Watson\u2019s Perceptual Model. Advances in Neural Information Processing Systems 33 (2020).",
      "doi": ""
    },
    {
      "text": "Guillaume Desjardins, Aaron Courville, and Yoshua Bengio. 2012. Disentangling factors of variation via generative entangling. (2012). arXiv:1210.5474",
      "doi": ""
    },
    {
      "text": "Finale Doshi-Velez and Been Kim. 2017. Towards a rigorous science of interpretable machine learning. (2017). arXiv:1702.08608",
      "doi": ""
    },
    {
      "text": "Cian Eastwood and Christopher\u00a0KI Williams. 2018. A framework for the quantitative evaluation of disentangled representations. In International Conference on Learning Representations.",
      "doi": ""
    },
    {
      "text": "Shi Feng and Jordan Boyd-Graber. 2019. What Can AI Do for Me? Evaluating Machine Learning Interpretations in Cooperative Play. In Proceedings of the 24th International Conference on Intelligent User Interfaces (Marina del Ray, California) (IUI \u201919). Association for Computing Machinery, New York, NY, USA, 229\u2013239. https://doi.org/10.1145/3301275.3302265",
      "doi": "10.1145/3301275.3302265"
    },
    {
      "text": "Zoubin Ghahramani and Michael\u00a0I Jordan. 1996. Factorial hidden Markov models. In Advances in Neural Information Processing Systems. 472\u2013478.",
      "doi": ""
    },
    {
      "text": "Leilani\u00a0H Gilpin, David Bau, Ben\u00a0Z Yuan, Ayesha Bajwa, Michael Specter, and Lalana Kagal. 2018. Explaining explanations: An overview of interpretability of machine learning. In 2018 IEEE 5th International Conference on data science and advanced analytics (DSAA). IEEE, 80\u201389.",
      "doi": ""
    },
    {
      "text": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In Advances in neural information processing systems. 2672\u20132680.",
      "doi": ""
    },
    {
      "text": "David Ha and J\u00fcrgen Schmidhuber. 2018. Recurrent World Models Facilitate Policy Evolution. In Advances in Neural Information Processing Systems 31. Curran Associates, Inc., 2451\u20132463. https://papers.nips.cc/paper/7512-recurrent-world-models-facilitate-policy-evolution https://worldmodels.github.io.",
      "doi": ""
    },
    {
      "text": "MM Hansen, T Miron-Shatz, AYS Lau, and C Paton. 2014. Big data in science and healthcare: a review of recent literature and perspectives. Yearbook of medical informatics 23, 01 (2014), 21\u201326.",
      "doi": ""
    },
    {
      "text": "Sandra\u00a0G Hart and Lowell\u00a0E Staveland. 1988. Development of NASA-TLX (Task Load Index): Results of empirical and theoretical research. In Advances in psychology. Vol.\u00a052. Elsevier, 139\u2013183.",
      "doi": ""
    },
    {
      "text": "Irina Higgins, David Amos, David Pfau, Sebastien Racaniere, Loic Matthey, Danilo Rezende, and Alexander Lerchner. 2018. Towards a definition of disentangled representations. (2018). arXiv:1812.02230",
      "doi": ""
    },
    {
      "text": "Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. 2017. beta-vae: Learning basic visual concepts with a constrained variational framework. In International Conference on Learning Representations, Vol.\u00a03.",
      "doi": ""
    },
    {
      "text": "Geoffrey\u00a0E Hinton and Ruslan\u00a0R Salakhutdinov. 2006. Reducing the dimensionality of data with neural networks. science 313, 5786 (2006), 504\u2013507.",
      "doi": ""
    },
    {
      "text": "Johan Huysmans, Karel Dejaeger, Christophe Mues, Jan Vanthienen, and Bart Baesens. 2011. An empirical evaluation of the comprehensibility of decision table, tree and rule based predictive models. Decision Support Systems 51, 1 (2011), 141\u2013154.",
      "doi": "10.1016/j.dss.2010.12.003"
    },
    {
      "text": "Ian\u00a0T Jolliffe. 1986. Principal components in regression analysis. In Principal component analysis. Springer, 129\u2013155.",
      "doi": ""
    },
    {
      "text": "Harmanpreet Kaur, Harsha Nori, Samuel Jenkins, Rich Caruana, Hanna Wallach, and Jennifer Wortman\u00a0Vaughan. 2020. Interpreting Interpretability: Understanding Data Scientists\u2019 Use of Interpretability Tools for Machine Learning. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI \u201920). Association for Computing Machinery, New York, NY, USA, 1\u201314. https://doi.org/10.1145/3313831.3376219",
      "doi": "10.1145/3313831.3376219"
    },
    {
      "text": "Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, 2018. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). In International conference on machine learning. PMLR, 2668\u20132677.",
      "doi": ""
    },
    {
      "text": "Hyunjik Kim and Andriy Mnih. 2018. Disentangling by Factorising. In Proceedings of the 35th International Conference on Machine Learning(Proceedings of Machine Learning Research, Vol.\u00a080), Jennifer Dy and Andreas Krause (Eds.). PMLR, Stockholmsm\u00e4ssan, Stockholm Sweden, 2649\u20132658. http://proceedings.mlr.press/v80/kim18b.html",
      "doi": ""
    },
    {
      "text": "Diederik\u00a0P Kingma and Max Welling. 2013. Auto-encoding variational bayes. (2013). arXiv:1312.6114",
      "doi": ""
    },
    {
      "text": "Josua Krause, Adam Perer, and Kenney Ng. 2016. Interacting with predictions: Visual inspection of black-box machine learning models. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems. 5686\u20135697.",
      "doi": "10.1145/2858036.2858529"
    },
    {
      "text": "Josua Krause, Adam Perer, and Kenney Ng. 2016. Interacting with Predictions: Visual Inspection of Black-Box Machine Learning Models. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (San Jose, California, USA) (CHI \u201916). Association for Computing Machinery, New York, NY, USA, 5686\u20135697. https://doi.org/10.1145/2858036.2858529",
      "doi": "10.1145/2858036.2858529"
    },
    {
      "text": "Todd Kulesza, Simone Stumpf, Margaret Burnett, Sherry Yang, Irwin Kwan, and Weng-Keen Wong. 2013. Too much, too little, or just right? Ways explanations impact end users\u2019 mental models. In 2013 IEEE Symposium on Visual Languages and Human Centric Computing. IEEE, 3\u201310.",
      "doi": ""
    },
    {
      "text": "Isaac Lage, Emily Chen, Jeffrey He, Menaka Narayanan, Been Kim, Sam Gershman, and Finale Doshi-Velez. 2019. An evaluation of the human-interpretability of explanation. (2019). arXiv:1902.00006",
      "doi": ""
    },
    {
      "text": "Isaac Lage, Andrew Ross, Samuel\u00a0J Gershman, Been Kim, and Finale Doshi-Velez. 2018. Human-in-the-loop interpretability prior. In Advances in neural information processing systems. 10159\u201310168.",
      "doi": ""
    },
    {
      "text": "Yann LeCun. 1998. The MNIST database of handwritten digits. http://yann. lecun. com/exdb/mnist/(1998).",
      "doi": ""
    },
    {
      "text": "Michael Levandowsky and David Winter. 1971. Distance between sets. Nature 234, 5323 (1971), 34\u201335.",
      "doi": ""
    },
    {
      "text": "Clayton Lewis. 1982. Using the\u201d thinking-aloud\u201d method in cognitive interface design. IBM TJ Watson Research Center Yorktown Heights, NY.",
      "doi": ""
    },
    {
      "text": "Brian\u00a0Y Lim, Anind\u00a0K Dey, and Daniel Avrahami. 2009. Why and why not explanations improve the intelligibility of context-aware intelligent systems. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 2119\u20132128.",
      "doi": "10.1145/1518701.1519023"
    },
    {
      "text": "Zachary\u00a0C Lipton. 2018. The mythos of model interpretability. Queue 16, 3 (2018), 31\u201357.",
      "doi": "10.1145/3236386.3241340"
    },
    {
      "text": "Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard Sch\u00f6lkopf, and Olivier Bachem. 2019. Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations. In Proceedings of the 36th International Conference on Machine Learning(Proceedings of Machine Learning Research, Vol.\u00a097), Kamalika Chaudhuri and Ruslan Salakhutdinov (Eds.). PMLR, Long Beach, California, USA, 4114\u20134124. http://proceedings.mlr.press/v97/locatello19a.html",
      "doi": ""
    },
    {
      "text": "Laurens van\u00a0der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE. Journal of machine learning research 9, Nov (2008), 2579\u20132605.",
      "doi": ""
    },
    {
      "text": "Loic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner. 2017. dSprites: Disentanglement testing Sprites dataset. https://github.com/deepmind/dsprites-dataset.",
      "doi": ""
    },
    {
      "text": "Tim Miller. 2019. Explanation in artificial intelligence: Insights from the social sciences. Artificial Intelligence 267 (2019), 1\u201338.",
      "doi": ""
    },
    {
      "text": "Don Norman. 2013. The design of everyday things: Revised and expanded edition. Basic books.",
      "doi": ""
    },
    {
      "text": "Chris Olah, Alexander Mordvintsev, and Ludwig Schubert. 2017. Feature visualization. Distill 2, 11 (2017), e7.",
      "doi": ""
    },
    {
      "text": "Chris Olah, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schubert, Katherine Ye, and Alexander Mordvintsev. 2018. The building blocks of interpretability. Distill 3, 3 (2018), e10.",
      "doi": ""
    },
    {
      "text": "Forough Poursabzi-Sangdeh, Daniel\u00a0G Goldstein, Jake\u00a0M Hofman, Jennifer\u00a0Wortman Vaughan, and Hanna Wallach. 2018. Manipulating and measuring model interpretability. (2018). arXiv:1802.07810",
      "doi": ""
    },
    {
      "text": "Karl Ridgeway. 2016. A survey of inductive biases for factorial representation-learning. (2016). arXiv:1612.05299",
      "doi": ""
    },
    {
      "text": "Leonid Rozenblit and Frank Keil. 2002. The misunderstood limits of folk science: An illusion of explanatory depth. Cognitive science 26, 5 (2002), 521\u2013562.",
      "doi": ""
    },
    {
      "text": "Jeff Sauro and Joseph\u00a0S Dumas. 2009. Comparison of three one-question, post-task usability questionnaires. In Proceedings of the SIGCHI conference on human factors in computing systems. 1599\u20131608.",
      "doi": "10.1145/1518701.1518946"
    },
    {
      "text": "Morgan\u00a0Klaus Scheuerman, Katta Spiel, Oliver\u00a0L Haimson, Foad Hamidi, and Stacy\u00a0M Branham. 2019. HCI guidelines for gender equity and inclusivity. https://www.morgan-klaus.com/gender-guidelines.html.",
      "doi": ""
    },
    {
      "text": "Ute Schmid, Christina Zeller, Tarek Besold, Alireza Tamaddoni-Nezhad, and Stephen Muggleton. 2016. How does predicate invention affect human comprehensibility?. In International Conference on Inductive Logic Programming. Springer, 52\u201367.",
      "doi": ""
    },
    {
      "text": "Anna Sepliarskaia, Julia Kiseleva, and Maarten de Rijke. 2019. Evaluating Disentangled Representations. (2019). arXiv:1910.05587",
      "doi": ""
    },
    {
      "text": "Dylan Slack, Sorelle\u00a0A Friedler, Carlos Scheidegger, and Chitradeep\u00a0Dutta Roy. 2019. Assessing the Local Interpretability of Machine Learning Models. (2019). arXiv:1902.03501",
      "doi": ""
    },
    {
      "text": "Daniel Smilkov, Nikhil Thorat, Yannick Assogba, Ann Yuan, Nick Kreeger, Ping Yu, Kangyi Zhang, Shanqing Cai, Eric Nielsen, David Soergel, 2019. Tensorflow. js: Machine learning for the web and beyond. (2019). arXiv:1901.05350",
      "doi": ""
    },
    {
      "text": "Daniel Smilkov, Nikhil Thorat, Charles Nicholson, Emily Reif, Fernanda\u00a0B Vi\u00e9gas, and Martin Wattenberg. 2016. Embedding projector: Interactive visualization and interpretation of embeddings. (2016).",
      "doi": ""
    },
    {
      "text": "John Sweller. 1994. Cognitive load theory, learning difficulty, and instructional design. Learning and instruction 4, 4 (1994), 295\u2013312.",
      "doi": ""
    },
    {
      "text": "Zhou Wang, Alan\u00a0C Bovik, Hamid\u00a0R Sheikh, and Eero\u00a0P Simoncelli. 2004. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing 13, 4 (2004), 600\u2013612.",
      "doi": "10.1109/TIP.2003.819861"
    },
    {
      "text": "James Wexler, Mahima Pushkarna, Tolga Bolukbasi, Martin Wattenberg, Fernanda Vi\u00e9gas, and Jimbo Wilson. 2019. The what-if tool: Interactive probing of machine learning models. IEEE transactions on visualization and computer graphics 26, 1(2019), 56\u201365.",
      "doi": ""
    },
    {
      "text": "Richard Zhang, Phillip Isola, Alexei\u00a0A Efros, Eli Shechtman, and Oliver Wang. 2018. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition. 586\u2013595.",
      "doi": ""
    }
  ]
}