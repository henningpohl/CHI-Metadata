{
  "doi": "10.1145/3411764.3445347",
  "title": "Toward Automatic Audio Description Generation for Accessible Videos",
  "published": "2021-05-07",
  "proctitle": "CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems",
  "pages": "1-12",
  "year": 2021,
  "badges": [],
  "abstract": "Video accessibility is essential for people with visual impairments. Audio descriptions describe what is happening on-screen, e.g., physical actions, facial expressions, and scene changes. Generating high-quality audio descriptions requires a lot of manual description generation\u00a0[50]. To address this accessibility obstacle, we built a system that analyzes the audiovisual contents of a video and generates the audio descriptions. The system consisted of three modules: AD insertion time prediction, AD generation, and AD optimization. We evaluated the quality of our system on five types of videos by conducting qualitative studies with 20 sighted users and 12 users who were blind or visually impaired. Our findings revealed how audio description preferences varied with user types and video types. Based on our study\u2019s analysis, we provided recommendations for the development of future audio description generation technologies.",
  "authors": [
    {
      "name": "Yujia Wang",
      "institution": "Computer Science Beijing Institute of Technology, China",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659482126",
      "orcid": "missing"
    },
    {
      "name": "Wei Liang",
      "institution": "School of Computer Science Beijing Institute of Technology, China",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "84459335757",
      "orcid": "missing"
    },
    {
      "name": "Haikun Huang",
      "institution": "Computer Science Department George Mason University, United States",
      "img": "/do/10.1145/contrib-99659254899/rel-imgonly/99659254899.png",
      "acmid": "99659254899",
      "orcid": "0000-0002-5962-0533"
    },
    {
      "name": "Yongqi Zhang",
      "institution": "Computer Science Department University of Massachusetts Boston, United States",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659572513",
      "orcid": "missing"
    },
    {
      "name": "Dingzeyu Li",
      "institution": "Adobe Research, United States",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99659366015",
      "orcid": "missing"
    },
    {
      "name": "Lap-Fai Yu",
      "institution": "Computer Science George Mason University, United States",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81487650310",
      "orcid": "0000-0002-2656-5654"
    }
  ],
  "references": [
    {
      "text": "2019. Katna: Tool for automating common vide keyframe extraction and Image Autocrop tasks. https://katna.readthedocs.io/.",
      "doi": ""
    },
    {
      "text": "2020. Guidelines for Audio Describers. http://www.acb.org/adp/guidelines.html.",
      "doi": ""
    },
    {
      "text": "Nayyer Aafaq, Ajmal Mian, Wei Liu, Syed\u00a0Zulqarnain Gilani, and Mubarak Shah. 2019. Video description: A survey of methods, datasets, and evaluation metrics. ACM Computing Surveys (CSUR) 52, 6 (2019), 1\u201337.",
      "doi": "10.1145/3355390"
    },
    {
      "text": "Relja Arandjelovic and Andrew Zisserman. 2017. Look, listen and learn. In ICCV. 609\u2013617.",
      "doi": ""
    },
    {
      "text": "Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2016. A simple but tough-to-beat baseline for sentence embeddings. (2016).",
      "doi": ""
    },
    {
      "text": "Yusuf Aytar, Carl Vondrick, and Antonio Torralba. 2016. Soundnet: Learning sound representations from unlabeled video. In Advances in neural information processing systems. 892\u2013900.",
      "doi": ""
    },
    {
      "text": "Yoshua Bengio, R\u00e9jean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic language model. Journal of machine learning research 3, Feb (2003), 1137\u20131155.",
      "doi": "10.5555/944919.944966"
    },
    {
      "text": "Ann Bigelow. 1991. Spatial mapping of familiar locations in blind children. Journal of Visual Impairment & Blindness 85, 3 (1991), 113\u2013117.",
      "doi": ""
    },
    {
      "text": "Sabine Braun. 2011. Creating coherence in audio description. Meta: Journal des traducteurs/Meta: Translators\u2019 Journal 56, 3(2011), 645\u2013662.",
      "doi": ""
    },
    {
      "text": "Richard Brown. 2013. Consciousness inside and out: Phenomenology, neuroscience, and the nature of experience. Springer Science & Business Media.",
      "doi": ""
    },
    {
      "text": "Joao Carreira and Andrew Zisserman. 2017. Quo vadis, action recognition? a new model and the kinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 6299\u20136308.",
      "doi": ""
    },
    {
      "text": "Xuguang Duan, Wenbing Huang, Chuang Gan, Jingdong Wang, Wenwu Zhu, and Junzhou Huang. 2018. Weakly supervised dense event captioning in videos. In Advances in Neural Information Processing Systems. 3059\u20133069.",
      "doi": ""
    },
    {
      "text": "Katie Ellis 2015. Netflix closed captions offer an accessible model for the streaming video industry, but what about audio description?Communication, Politics & Culture 47, 3 (2015), 3.",
      "doi": ""
    },
    {
      "text": "Deborah\u00a0I Fels, John\u00a0Patrick Udo, Jonas\u00a0E Diamond, and Jeremy\u00a0I Diamond. 2006. A comparison of alternative narrative approaches to video description for animated comedy. Journal of Visual Impairment & Blindness 100, 5 (2006), 295\u2013305.",
      "doi": ""
    },
    {
      "text": "Pat Fletcher. 2002. Seeing with sound: A journey into sight. Retrieved September 21(2002), 2015.",
      "doi": ""
    },
    {
      "text": "Louise Fryer. 2016. An introduction to audio description: A practical guide. Routledge.",
      "doi": ""
    },
    {
      "text": "Louise Fryer and Jonathan Freeman. 2013. Cinematic language and the description of film: Keeping AD users in the frame. Perspectives 21, 3 (2013), 412\u2013426.",
      "doi": ""
    },
    {
      "text": "William\u00a0W Gaver. 1993. What in the world do we hear?: An ecological approach to auditory event perception. Ecological psychology 5, 1 (1993), 1\u201329.",
      "doi": ""
    },
    {
      "text": "Nicholas\u00a0A Giudice and Gordon\u00a0E Legge. 2008. Blind navigation and the role of technology. The engineering handbook of smart technology for aging, disability, and independence 8(2008), 479\u2013500.",
      "doi": ""
    },
    {
      "text": "Cole Gleason, Amy Pavel, Himalini Gururaj, Kris\u00a0M Kitani, and Jefrey\u00a0P Bigham. 2020. Making GIFs Accessible. (2020).",
      "doi": ""
    },
    {
      "text": "Cole Gleason, Amy Pavel, Xingyu Liu, Patrick Carrington, Lydia\u00a0B Chilton, and Jeffrey\u00a0P Bigham. 2019. Making Memes Accessible. In The 21st International ACM SIGACCESS Conference on Computers and Accessibility. 367\u2013376.",
      "doi": ""
    },
    {
      "text": "Cole Gleason, Amy Pavel, Emma McCamey, Christina Low, Patrick Carrington, Kris\u00a0M Kitani, and Jeffrey\u00a0P Bigham. 2020. Twitter A11y: A Browser Extension to Make Twitter Images Accessible. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. 1\u201312.",
      "doi": "10.1145/3313831.3376728"
    },
    {
      "text": "Darren Guinness, Edward Cutrell, and Meredith\u00a0Ringel Morris. 2018. Caption crawler: Enabling reusable alternative text descriptions using reverse image search. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems. 1\u201311.",
      "doi": "10.1145/3173574.3174092"
    },
    {
      "text": "Di Hu, Feiping Nie, and Xuelong Li. 2019. Deep multimodal clustering for unsupervised audiovisual learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 9248\u20139257.",
      "doi": ""
    },
    {
      "text": "Haikun Huang, Michael Solah, Dingzeyu Li, and Lap-Fai Yu. 2019. Audible panorama: Automatic spatial audio generation for panorama imagery. In Proceedings of the 2019 CHI conference on human factors in computing systems. 1\u201311.",
      "doi": "10.1145/3290605.3300851"
    },
    {
      "text": "Lun Huang, Wenmin Wang, Jie Chen, and Xiao-Yong Wei. 2019. Attention on attention for image captioning. In Proceedings of the IEEE International Conference on Computer Vision. 4634\u20134643.",
      "doi": ""
    },
    {
      "text": "R Kingett. 2014. The Accessible Netflix Project Advocates Taking Steps to Ensure Netflix Accessibility for Everyone. The Accessible Netflix Project 26 (2014).",
      "doi": ""
    },
    {
      "text": "Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos\u00a0Niebles. 2017. Dense-captioning events in videos. In Proceedings of the IEEE international conference on computer vision. 706\u2013715.",
      "doi": ""
    },
    {
      "text": "Yehao Li, Ting Yao, Yingwei Pan, Hongyang Chao, and Tao Mei. 2018. Jointly localizing and describing events for dense video captioning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 7492\u20137500.",
      "doi": ""
    },
    {
      "text": "Lajanugen Logeswaran and Honglak Lee. 2018. An Efficient Framework for Learning Sentence Representations. In International Conference on Learning Representations. 1\u201316.",
      "doi": ""
    },
    {
      "text": "John Miers. 1995. Audio description\u2013seeing theater with your ears. Information Technology and Disabilities 2, 2 (1995).",
      "doi": ""
    },
    {
      "text": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg\u00a0S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems. 3111\u20133119.",
      "doi": ""
    },
    {
      "text": "Chris Mikul. 2010. Audio description background paper. Ultimo NSW: Media Access Australia(2010).",
      "doi": ""
    },
    {
      "text": "Tae-Hyun Oh, Tali Dekel, Changil Kim, Inbar Mosseri, William\u00a0T Freeman, Michael Rubinstein, and Wojciech Matusik. 2019. Speech2face: Learning the face behind a voice. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 7539\u20137548.",
      "doi": ""
    },
    {
      "text": "Jaclyn Packer, Katie Vizenor, and Joshua\u00a0A Miele. 2015. An overview of video description: history, benefits, and guidelines. Journal of Visual Impairment & Blindness 109, 2 (2015), 83\u201393.",
      "doi": ""
    },
    {
      "text": "Matteo Pagliardini, Prakhar Gupta, and Martin Jaggi. 2018. Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features. In NAACL 2018 - Conference of the North American Chapter of the Association for Computational Linguistics.",
      "doi": ""
    },
    {
      "text": "Yingwei Pan, Tao Mei, Ting Yao, Houqiang Li, and Yong Rui. 2016. Jointly modeling embedding and translation to bridge video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition. 4594\u20134602.",
      "doi": ""
    },
    {
      "text": "Yingwei Pan, Ting Yao, Houqiang Li, and Tao Mei. 2017. Video captioning with transferred semantic attributes. In Proceedings of the IEEE conference on computer vision and pattern recognition. 6504\u20136512.",
      "doi": ""
    },
    {
      "text": "Becky Parton. 2016. Video captions for online courses: Do YouTube\u2019s auto-generated captions meet deaf students\u2019 needs?Journal of Open, Flexible, and Distance Learning 20, 1 (2016), 8\u201318.",
      "doi": ""
    },
    {
      "text": "Elisa Perego. 2016. Gains and losses of watching audio described films for sighted viewers. Target. International Journal of Translation Studies 28, 3 (2016), 424\u2013444.",
      "doi": ""
    },
    {
      "text": "Zhe Quan, Zhi-Jie Wang, Yuquan Le, Bin Yao, Kenli Li, and Jian Yin. 2019. An efficient framework for sentence similarity modeling. IEEE/ACM Transactions on Audio, Speech, and Language Processing 27, 4(2019), 853\u2013865.",
      "doi": "10.1109/TASLP.2019.2899494"
    },
    {
      "text": "Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training.",
      "doi": ""
    },
    {
      "text": "Aline Remael and Gert Vercauteren. 2011. Basisprincipes voor audiobeschrijving voor televisie en film [Basics of audio description for television and film]. Antwerp: Departement Vertalers and Tolken, Artesis Hogeschool (2011).",
      "doi": ""
    },
    {
      "text": "Anna Rohrbach, Atousa Torabi, Marcus Rohrbach, Niket Tandon, Christopher Pal, Hugo Larochelle, Aaron Courville, and Bernt Schiele. 2017. Movie description. International Journal of Computer Vision 123, 1 (2017), 94\u2013120.",
      "doi": "10.1007/s11263-016-0987-1"
    },
    {
      "text": "Pablo Romero-Fresco and Louise Fryer. 2013. Could audio-described films benefit from audio introductions? An audience response study. Journal of Visual Impairment & Blindness 107, 4 (2013), 287\u2013295.",
      "doi": ""
    },
    {
      "text": "Elliot Salisbury, Ece Kamar, and Meredith\u00a0Ringel Morris. 2017. Toward Scalable Social Alt Text: Conversational Crowdsourcing as a Tool for Refining Vision-to-Language Technology for the Blind.. In HCOMP. 147\u2013156.",
      "doi": ""
    },
    {
      "text": "Jaime S\u00e1nchez, Mauricio Saenz, and Jose\u00a0Miguel Garrido. 2010. Usability of a multimodal video game to improve navigation skills for blind children. ACM Transactions on Accessible Computing (TACCESS) 3, 2 (2010), 1\u201329.",
      "doi": "10.1145/1857920.1857924"
    },
    {
      "text": "Emilie Schmeidler and Corinne Kirchner. 2001. Adding audio description: Does it make a difference?Journal of Visual Impairment & Blindness 95, 4 (2001), 197\u2013212.",
      "doi": ""
    },
    {
      "text": "Merrie Snell. 2015. Lipsynching: popular song recordings and the disembodied voice. Ph.D. Dissertation. Newcastle University.",
      "doi": ""
    },
    {
      "text": "Joel Snyder. 2014. The visual made verbal: A comprehensive training manual and guide to the history and applications of audio description. American Council of the Blind, Incorporated.",
      "doi": ""
    },
    {
      "text": "Abigale Stangl, Meredith\u00a0Ringel Morris, and Danna Gurari. 2020. \u201d Person, Shoes, Tree. Is the Person Naked?\u201d What People with Vision Impairments Want in Image Descriptions. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. 1\u201313.",
      "doi": "10.1145/3313831.3376404"
    },
    {
      "text": "Sandeep Subramanian, Adam Trischler, Yoshua Bengio, and Christopher\u00a0J Pal. 2018. Learning general purpose distributed sentence representations via large scale multi-task learning. (2018). arXiv:1804.00079",
      "doi": ""
    },
    {
      "text": "Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. 2015. Learning spatiotemporal features with 3d convolutional networks. In Proceedings of the IEEE international conference on computer vision. 4489\u20134497.",
      "doi": "10.1109/ICCV.2015.510"
    },
    {
      "text": "Subhashini Venugopalan, Marcus Rohrbach, Jeffrey Donahue, Raymond Mooney, Trevor Darrell, and Kate Saenko. 2015. Sequence to sequence-video to text. In Proceedings of the IEEE international conference on computer vision. 4534\u20134542.",
      "doi": "10.1109/ICCV.2015.515"
    },
    {
      "text": "Agnieszka Walczak and Louise Fryer. 2017. Creative description: The impact of audio description style on presence in visually impaired audiences. British Journal of Visual Impairment 35, 1 (2017), 6\u201317.",
      "doi": ""
    },
    {
      "text": "Jingwen Wang, Wenhao Jiang, Lin Ma, Wei Liu, and Yong Xu. 2018. Bidirectional attentive fusion with context gating for dense video captioning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 7190\u20137198.",
      "doi": ""
    },
    {
      "text": "Yujia Wang, Wenguan Wang, Wei Liang, and Lap-Fai Yu. 2019. Comic-guided speech synthesis. ACM Transactions on Graphics (TOG) 38, 6 (2019), 1\u201314.",
      "doi": "10.1145/3355089.3356520"
    },
    {
      "text": "Yujia Wang, Liang Wei, Li Wanwan, Li Dingzeyu, and Lap-Fai Yu. 2020. Scene-Aware Background Music Synthesis. In ACM Multimedia, Vol.\u00a038.",
      "doi": ""
    },
    {
      "text": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, 2019. Transformers: State-of-the-art natural language processing. (2019). arXiv:1910.03771",
      "doi": ""
    },
    {
      "text": "Luowei Zhou, Yingbo Zhou, Jason\u00a0J Corso, Richard Socher, and Caiming Xiong. 2018. End-to-end dense video captioning with masked transformer. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 8739\u20138748.",
      "doi": ""
    }
  ]
}