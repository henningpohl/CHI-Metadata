{
  "doi": "10.1145/1054972.1055005",
  "title": "Effects of task properties, partner actions, and message content on eye gaze patterns in a collaborative task",
  "published": "2005-04-02",
  "proctitle": "CHI '05: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems",
  "pages": "231-240",
  "year": 2005,
  "badges": [],
  "abstract": "Helpers providing guidance for collaborative physical tasks shift their gaze between the workspace, supply area, and instructions. Understanding when and why helpers gaze at each area is important both for a theoretical understanding of collaboration on physical tasks and for the design of automated video systems for remote collaboration. In a laboratory experiment using a collaborative puzzle task, we recorded helpers' gaze while manipulating task complexity and piece differentiability. Helpers gazed toward the pieces bay more frequently when pieces were difficult to differentiate and less frequently over repeated trials. Preliminary analyses of message content show that helpers tend to look at the pieces bay when describing the next piece and at the workspace when describing where it goes. The results are consistent with a grounding model of communication, in which helpers seek visual evidence of understanding unless they are confident that they have been understood. The results also suggest the feasibility of building automated video systems based on remote helpers' shifting visual requirements.",
  "tags": [
    "gesture",
    "video mediated communication",
    "eye-tracking",
    "computer-supported",
    "conversational analysis",
    "video conferencing",
    "empirical studies",
    "collaborative work"
  ],
  "authors": [
    {
      "name": "Jiazhi Ou",
      "institution": "Carnegie Mellon University, Pittsburgh, PA",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81100177074",
      "orcid": "missing"
    },
    {
      "name": "Lui Min Oh",
      "institution": "Carnegie Mellon University, Pittsburgh, PA",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "81100059735",
      "orcid": "missing"
    },
    {
      "name": "Jie Yang",
      "institution": "Carnegie Mellon University, Pittsburgh, PA",
      "img": "/pb-assets/icons/DOs/default-profile-1543932446943.svg",
      "acmid": "99660367560",
      "orcid": "missing"
    },
    {
      "name": "Susan R. Fussell",
      "institution": "Carnegie Mellon University, Pittsburgh, PA",
      "img": "/do/10.1145/contrib-81100583820/rel-imgonly/img_0320profile2.jpg",
      "acmid": "81100583820",
      "orcid": "missing"
    }
  ],
  "references": [
    {
      "text": "Argyle, M., & Cook, M. (1976). Gaze and Mutual Gaze. Cambridge: Cambridge University Press.]]",
      "doi": ""
    },
    {
      "text": "Brumitt B., Krumm J., Meyers B., & Shafer S. (2000). Let there be light: Comparing interfaces for homes of the future. IEEE Personal Communications, August 2000.]]",
      "doi": ""
    },
    {
      "text": "Campana, E., Baldridge, J., Dowding, J., Hockey, B. A., Remington, R. W., & Stone, L. S. (2001). Using eye movements to determine referents in a spoken dialogue system. In Proceedings of the 2001 workshop on Perceptive user interfaces (pp. 1--5).]]  ",
      "doi": "10.1145/971478.971489"
    },
    {
      "text": "Chambers, C. G., Tanenhaus, M. K., Eberhard, K. M., Filip, H., & Carlson, G. N. (2002). Circumscribing referential domains during real-time language comprehension. Journal of Memory and Language, 47, 30--49.]]",
      "doi": ""
    },
    {
      "text": "Clark, H. H. & Marshall, C. E. (1981). Definite reference and mutual knowledge. In A. K. Joshi, B. L. Webber & I. A. Sag (Eds.), Elements of discourse understanding (pp. 10--63). Cambridge: Cambridge University Press.]]",
      "doi": ""
    },
    {
      "text": "Clark, H. H. & Wilkes-Gibbs, D. (1986). Referring as a collaborative process. Cognition, 22, 1--39.]]",
      "doi": ""
    },
    {
      "text": "Clark, H. H. (1996). Using language. Cambridge, England: Cambridge University Press.]]",
      "doi": ""
    },
    {
      "text": "Dabbish, L. & Kraut R. (2004). Controlling interruptions: Awareness displays and social motivation for coordination. Proceedings of CSCW 2004 (pp. 182--191). NY: ACM Press.]]  ",
      "doi": "10.1145/1031607.1031638"
    },
    {
      "text": "Eberhard, K. M., Spivey-Knowlton, M. J., Sedivy, J. C., & Tanenhaus, M. K. (1995). Eye movements as a window into real-time spoken language processing in natural contexts. Journal of Psycholinguistic Research, 24, 409--436.]]",
      "doi": ""
    },
    {
      "text": "Endsley, M. (1995). Toward a theory of situation awareness in dynamic systems. Human Factors, 37, 32--64.]]",
      "doi": ""
    },
    {
      "text": "Farid. M,, Murtagh., F., and Starck., J.L. (2002) Computer display control and interaction using eye-gaze, Journal of the Society for Information Display, 10, 289--293.]]",
      "doi": ""
    },
    {
      "text": "Ford, C. E. (1999). Collaborative construction of task activity: Coordinating multiple resources in a high school physics lab. Research on Language and Social Interaction, 32, 369--408.]]",
      "doi": ""
    },
    {
      "text": "Frey L. A., White, K. P. Jr., & Hutchinson T. E. (1990). Eye-gaze word processing. IEEE Transactions on Systems, Man and Cybernetics, 20, 944--950.]]",
      "doi": ""
    },
    {
      "text": "Fussell, S. R., Kraut, R. E., & Siegel, J. (2000). Coordination of communication: Effects of shared visual context on collaborative work. Proceedings of CSCW 2000 (pp. 21--30). NY: ACM Press.]]  ",
      "doi": "10.1145/358916.358947"
    },
    {
      "text": "Fussell, S. R., Setlock, L. D., Yang, J., Ou, J., Mauer, E. M., & Kramer, A. (2004). Gestures over video streams to support remote collaboration on physical tasks. Human-Computer Interaction, 19, 273--309.]]  ",
      "doi": "10.1207/s15327051hci1903_3"
    },
    {
      "text": "Fussell, S. R., Setlock, L. D., & Kraut, R. E. (2003). Effects of head-mounted and scene-oriented video systems on remote collaboration on physical tasks. Proceedings of CHI 2003 (pp. 513--520). NY: ACM Press.]]  ",
      "doi": "10.1145/642611.642701"
    },
    {
      "text": "Fussell, S. R., Setlock, L. D., & Parker, E. M. (2003). Where do helpers look? Gaze targets during collaborative physical tasks. CHI 2003 Extended Abstracts (pp. 768--769). NY: ACM Press.]]  ",
      "doi": "10.1145/765891.765980"
    },
    {
      "text": "Gaver, W., Sellen, A., Heath, C., & Luff, P. (1993) One is not enough: Multiple views in a media space. Proceedings of Interchi '93 (pp. 335--341). NY: ACM Press.]] ",
      "doi": "10.5555/164632.164940"
    },
    {
      "text": "Gergle, D., Kraut, R.E., & Fussell, S.R. (2004). Action as language in a shared visual space. Proceedings of CSCW 2004 (pp. 487--496). NY: ACM Press.]]  ",
      "doi": "10.1145/1031607.1031687"
    },
    {
      "text": "Gergle, D., Millan, D. R., Kraut, R. E., & Fussell, S. R. (2004). Persistence matters: Making the most of chat in tightly-coupled work. CHI 2004 (pp. 431--438). NY: ACM Press.]]  ",
      "doi": "10.1145/985692.985747"
    },
    {
      "text": "Goodwin, C. (1996). Professional vision. American Anthropologist, 96, 606--633.]]",
      "doi": ""
    },
    {
      "text": "Gullberg, M. (2003). Eye movements and gestures in human face-to-face interaction. In J. Hyona, R. Radach, & H. Deubel, (Eds.) The Mind's Eyes: Cognitive and Applied Aspects of Eye Movements (pp. 685--703). Oxford: Elsevier Science.]]",
      "doi": ""
    },
    {
      "text": "Hutchinson T. E., White, K. P. Jr., Martin, W. N., Reichert, K. C., & Frey L. A. (1989). Human-computer interaction using eye-gaze input. IEEE Transaction on Systems, Man, and Cybernetics, 19, 1527--1534.]]",
      "doi": ""
    },
    {
      "text": "Jacob, R. J. K. (1993). Eye-movement-based human-computer interaction techniques. In H. R. Hartson & D. Hix (Eds.), Advances in Human-Computer Interaction, Vol. 4 (pp. 151--190). Norwood, NJ: Ablex.]]",
      "doi": ""
    },
    {
      "text": "Jefferson, G. (1972). Side sequences. In D. Sudnow (Ed.) Studies in social interaction (pp. 294--338). NY: Free Press.]]",
      "doi": ""
    },
    {
      "text": "Keysar, B., Barr, D. J., Balin, J. A., & Brauner, J. S. (2000). Taking perspective in conversation: The role of mutual knowledge in comprehension. Psychological Science, 11, 32--38.]]",
      "doi": ""
    },
    {
      "text": "Kraut, R. E., Fussell, S. R., & Siegel, J. (2003). Visual information as a conversational resource in collaborative physical tasks. Human Computer Interaction, 18, 13--49.]]  ",
      "doi": "10.1207/S15327051HCI1812_2"
    },
    {
      "text": "Kraut, R.E., Gergle, D., & Fussell, S.R. (2002). The Use of visual information in shared visual spaces: Informing the development of virtual co-presence. In Proceedings of CSCW 2002 (pp. 31--40). NY: ACM Press.]]  ",
      "doi": "10.1145/587078.587084"
    },
    {
      "text": "Kraut, R. E., Miller, M. D., & Siegel, J. (1996) Collaboration in performance of physical tasks: Effects on outcomes and communication. Proceedings of CSCW 1996 (pp. 57--66). NY ACM.]]  ",
      "doi": "10.1145/240080.240190"
    },
    {
      "text": "Kuzuoka, H., Kosuge, T., & Tanaka, K.. (1994) GestureCam: A video communication system for sympathetic remote collaboration. Proceedings of CSCW 1994 (pp. 35--43). NY: ACM.]]  ",
      "doi": "10.1145/192844.192866"
    },
    {
      "text": "Kuzuoka, H., Oyama, S., Yamazaki, K., Suzuki, K., & Mitsuishi, M. (2000). GestureMan: A mobile robot that embodies a remote instructor's actions. Proceedings of CSCW 2000 (pp. 155--162). NY: ACM Press.]]  ",
      "doi": "10.1145/358916.358986"
    },
    {
      "text": "Land, M., & Hayhoe, M. (2001). In what ways do eye movements contribute to everyday activities. Vision Research, 41, 3559--3565.]]",
      "doi": ""
    },
    {
      "text": "Land, M., Mennie, N., & Rusted, J. (1999). The roles of vision and eye movements in the control of activities of daily living. Perception, 28, 1307--1432.]]",
      "doi": ""
    },
    {
      "text": "Maglio P., Matlock T., Campbell C. S., Zhai S., & Smith, B. A. (2000). Gaze and speech in attentive user interfaces. Proceedings of the International Conference on Multimodal Interfaces. Springer.]] ",
      "doi": "10.5555/645524.656806"
    },
    {
      "text": "Oh, K., Kramer, A. D. I., & Fussell, S. R. (in preparation). Comparison of scene and laser pointing video systems for remote collaboration on physical tasks.]]",
      "doi": ""
    },
    {
      "text": "Ou, J., Fussell, S. R., Chen, X., Setlock, L. D., & Yang, J. (2003). Gestural communication over video stream: Supporting multimodal interaction for remote collaborative physical tasks. In Proceedings of International Conference on Multimodal Interfaces, Nov. 5-7, 2003, Vancouver, Canada.]]  ",
      "doi": "10.1145/958432.958477"
    },
    {
      "text": "Ou, J. (unpublished). DOVE-2: Combining gesture with remote camera control.]]",
      "doi": ""
    },
    {
      "text": "Oudejans, R. R. D., Michaels, C. F., Bakker, F. C., & Davids, K. (1999). Shedding some light on catching in the dark: Perceptual mechanisms for catching fly balls. Journal of Experimental Psychology: Human Perception and Performance, 25, 531--542.]]",
      "doi": ""
    },
    {
      "text": "Daly-Jones, O., Monk, A. & Watts, L. (1998). Some advantages of video conferencing over high-quality audio conferencing: fluency and awareness of attentional focus. International Journal of Human-Computer Studies, 49, 21--58.]]  ",
      "doi": "10.1006/ijhc.1998.0195"
    },
    {
      "text": "Pelz, J. B., & Canosa, R. (2001). Oculomotor behavior and perceptual strategies in complex tasks. Vision Research, 41, 3587--3596.]]",
      "doi": ""
    },
    {
      "text": "Salvucci, D. (1999). Inferring intent in eye-based interfaces: Tracing eye movements with process models. Proceedings of CHI 1999 (pp. 254--261). NY: ACM Press.]]  ",
      "doi": "10.1145/302979.303055"
    },
    {
      "text": "Stiefelhagen, R., Yang, J., & Waibel, A. (2002). Modeling focus of attention for meeting indexing based on multiple cues. IEEE Transactions on Neural Networks, 13, 928--938.]]  ",
      "doi": "10.1109/TNN.2002.1021893"
    },
    {
      "text": "Tang, J. C. (1991). Findings from observational studies of collaborative work. International Journal of Man-Machine Studies, 34, 143--160.]]  ",
      "doi": "10.1016/0020-7373%2891%2990039-A"
    },
    {
      "text": "Veinott, E., Olson, J., Olson, G., & Fu, X. (1999). Video helps remote work: Speakers who need to negotiate common ground benefit from seeing each other. Proceedings of CHI 1999 (pp. 302--309). NY: ACM Press.]]  ",
      "doi": "10.1145/302979.303067"
    },
    {
      "text": "Vertegaal, R., Slagter, R., van der Veer, G., & Nijholt, A. (2001). Eye gaze patterns in conversations: There is more to conversational agents than meets the eyes. Proceedings of CHI 2001 (pp. 301--308). NY: ACM Press.]]  ",
      "doi": "10.1145/365024.365119"
    },
    {
      "text": "Vickers, J. N. (1996). Visual control when aiming at a far target. Journal of Experimental Psychology: Human Perception and Performance, 22, 342--354.]]",
      "doi": ""
    },
    {
      "text": "Whittaker, S., & O'Conaill, B. (1997). The role of vision in face-to-face and mediated communication. In K. Finn, A.Sellen & S. Wilbur (Eds.) Video-Mediated Communication (pp. 23--49). Mahwah, NJ: Erlbaum.]]",
      "doi": ""
    }
  ]
}